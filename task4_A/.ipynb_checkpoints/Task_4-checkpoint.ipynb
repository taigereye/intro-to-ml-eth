{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to ML Project\n",
    "## Task 4\n",
    "### Jan Bauer, Alaisha Sharma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils, to_categorical\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# same seed for consistency\n",
    "seed = 1\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_labeled = pd.read_hdf(\"data/train_labeled.h5\", \"train\")\n",
    "# train_data_labeled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_labeled = train_data_labeled.iloc[:,1:]\n",
    "# X_train_labeled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_data_labeled.iloc[:,0:1]\n",
    "# y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_unlabeled = pd.read_hdf(\"data/train_unlabeled.h5\", \"train\")\n",
    "# train_data_unlabeled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_unlabeled = train_data_unlabeled.iloc[:,:]\n",
    "# X_train_unlabeled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_hdf(\"data/test.h5\", \"test\")\n",
    "# X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()  \n",
    "scaler.fit(X_train_labeled)  \n",
    "X_train_labeled = scaler.transform(X_train_labeled)  \n",
    "X_train_unlabeled = scaler.transform(X_train_unlabeled)\n",
    "X_test = scaler.transform(X_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Cumulative explained variance')"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(n_components=139)\n",
    "pca.fit(X_train_labeled)\n",
    "\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Cumulative explained variance')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 40\n",
    "pca = PCA(n_components=k)\n",
    "X_train_labeled = pca.fit_transform(X_train_labeled)\n",
    "\n",
    "X_train_unlabeled = pca.transform(X_train_unlabeled)\n",
    "X_test = pca.transform(X_test)\n",
    "pca_std = np.std(X_train_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:95: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:128: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_train)\n",
    "encoded_y = encoder.transform(y_train)\n",
    "onehot_y = np_utils.to_categorical(encoded_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "9000/9000 [==============================] - 1s 124us/step - loss: 2.1726 - acc: 0.2154\n",
      "Epoch 2/100\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 1.4463 - acc: 0.4484\n",
      "Epoch 3/100\n",
      "9000/9000 [==============================] - 0s 34us/step - loss: 1.0784 - acc: 0.6366\n",
      "Epoch 4/100\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.9037 - acc: 0.7117\n",
      "Epoch 5/100\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.7944 - acc: 0.7504\n",
      "Epoch 6/100\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.7197 - acc: 0.7802\n",
      "Epoch 7/100\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.6629 - acc: 0.7962\n",
      "Epoch 8/100\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 0.6170 - acc: 0.8132\n",
      "Epoch 9/100\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 0.5802 - acc: 0.8221\n",
      "Epoch 10/100\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 0.5544 - acc: 0.8322\n",
      "Epoch 11/100\n",
      "9000/9000 [==============================] - 0s 34us/step - loss: 0.5362 - acc: 0.8361\n",
      "Epoch 12/100\n",
      "9000/9000 [==============================] - 0s 46us/step - loss: 0.5092 - acc: 0.8448\n",
      "Epoch 13/100\n",
      "9000/9000 [==============================] - 0s 44us/step - loss: 0.4942 - acc: 0.8498\n",
      "Epoch 14/100\n",
      "9000/9000 [==============================] - 0s 53us/step - loss: 0.4809 - acc: 0.8556\n",
      "Epoch 15/100\n",
      "9000/9000 [==============================] - 0s 44us/step - loss: 0.4622 - acc: 0.8608\n",
      "Epoch 16/100\n",
      "9000/9000 [==============================] - 0s 40us/step - loss: 0.4503 - acc: 0.8646\n",
      "Epoch 17/100\n",
      "9000/9000 [==============================] - 0s 36us/step - loss: 0.4393 - acc: 0.8679\n",
      "Epoch 18/100\n",
      "9000/9000 [==============================] - 0s 34us/step - loss: 0.4252 - acc: 0.8713\n",
      "Epoch 19/100\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 0.4198 - acc: 0.8740\n",
      "Epoch 20/100\n",
      "9000/9000 [==============================] - 0s 38us/step - loss: 0.4045 - acc: 0.8754\n",
      "Epoch 21/100\n",
      "9000/9000 [==============================] - 0s 35us/step - loss: 0.3982 - acc: 0.8820\n",
      "Epoch 22/100\n",
      "9000/9000 [==============================] - 0s 36us/step - loss: 0.3847 - acc: 0.8811\n",
      "Epoch 23/100\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 0.3788 - acc: 0.8862\n",
      "Epoch 24/100\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 0.3728 - acc: 0.8879\n",
      "Epoch 25/100\n",
      "9000/9000 [==============================] - 0s 36us/step - loss: 0.3652 - acc: 0.8886\n",
      "Epoch 26/100\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 0.3556 - acc: 0.8939\n",
      "Epoch 27/100\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.3490 - acc: 0.8951\n",
      "Epoch 28/100\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 0.3411 - acc: 0.8963\n",
      "Epoch 29/100\n",
      "9000/9000 [==============================] - 0s 34us/step - loss: 0.3336 - acc: 0.8974\n",
      "Epoch 30/100\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 0.3342 - acc: 0.8956\n",
      "Epoch 31/100\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 0.3228 - acc: 0.9028\n",
      "Epoch 32/100\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.3161 - acc: 0.9040\n",
      "Epoch 33/100\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.3142 - acc: 0.9064\n",
      "Epoch 34/100\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.3061 - acc: 0.9069\n",
      "Epoch 35/100\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 0.2983 - acc: 0.9089\n",
      "Epoch 36/100\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.2976 - acc: 0.9068\n",
      "Epoch 37/100\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 0.2954 - acc: 0.9103\n",
      "Epoch 38/100\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.2839 - acc: 0.9134\n",
      "Epoch 39/100\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.2779 - acc: 0.9156\n",
      "Epoch 40/100\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.2775 - acc: 0.9144\n",
      "Epoch 41/100\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.2728 - acc: 0.9177\n",
      "Epoch 42/100\n",
      "9000/9000 [==============================] - 0s 34us/step - loss: 0.2689 - acc: 0.9192\n",
      "Epoch 43/100\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 0.2580 - acc: 0.9230\n",
      "Epoch 44/100\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 0.2568 - acc: 0.9232\n",
      "Epoch 45/100\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 0.2513 - acc: 0.9240\n",
      "Epoch 46/100\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.2421 - acc: 0.9297\n",
      "Epoch 47/100\n",
      "9000/9000 [==============================] - 0s 34us/step - loss: 0.2394 - acc: 0.9293\n",
      "Epoch 48/100\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 0.2344 - acc: 0.9297\n",
      "Epoch 49/100\n",
      "9000/9000 [==============================] - 0s 34us/step - loss: 0.2309 - acc: 0.9288\n",
      "Epoch 50/100\n",
      "9000/9000 [==============================] - 0s 34us/step - loss: 0.2250 - acc: 0.9317\n",
      "Epoch 51/100\n",
      "9000/9000 [==============================] - 0s 34us/step - loss: 0.2260 - acc: 0.9314\n",
      "Epoch 52/100\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 0.2165 - acc: 0.9363\n",
      "Epoch 53/100\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 0.2121 - acc: 0.9361\n",
      "Epoch 54/100\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.2120 - acc: 0.9342\n",
      "Epoch 55/100\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 0.2044 - acc: 0.9370\n",
      "Epoch 56/100\n",
      "9000/9000 [==============================] - 0s 50us/step - loss: 0.2029 - acc: 0.9388: 0s - loss: 0.1886 - acc\n",
      "Epoch 57/100\n",
      "9000/9000 [==============================] - 0s 50us/step - loss: 0.1979 - acc: 0.9383\n",
      "Epoch 58/100\n",
      "9000/9000 [==============================] - 0s 50us/step - loss: 0.1992 - acc: 0.9389\n",
      "Epoch 59/100\n",
      "9000/9000 [==============================] - 0s 37us/step - loss: 0.1931 - acc: 0.9400\n",
      "Epoch 60/100\n",
      "9000/9000 [==============================] - 0s 38us/step - loss: 0.1883 - acc: 0.9439\n",
      "Epoch 61/100\n",
      "9000/9000 [==============================] - 0s 36us/step - loss: 0.1837 - acc: 0.9446\n",
      "Epoch 62/100\n",
      "9000/9000 [==============================] - 0s 46us/step - loss: 0.1861 - acc: 0.9412\n",
      "Epoch 63/100\n",
      "9000/9000 [==============================] - 0s 41us/step - loss: 0.1808 - acc: 0.9430\n",
      "Epoch 64/100\n",
      "9000/9000 [==============================] - 0s 36us/step - loss: 0.1802 - acc: 0.9444\n",
      "Epoch 65/100\n",
      "9000/9000 [==============================] - 0s 50us/step - loss: 0.1787 - acc: 0.9458\n",
      "Epoch 66/100\n",
      "9000/9000 [==============================] - 0s 36us/step - loss: 0.1722 - acc: 0.9467\n",
      "Epoch 67/100\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 0.1730 - acc: 0.9452\n",
      "Epoch 68/100\n",
      "9000/9000 [==============================] - 0s 36us/step - loss: 0.1669 - acc: 0.9463: 0s - loss: 0.1465 - acc: 0.95 - ETA: 0s - loss: 0.1662 - acc: 0.\n",
      "Epoch 69/100\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 0.1641 - acc: 0.9517\n",
      "Epoch 70/100\n",
      "9000/9000 [==============================] - 0s 39us/step - loss: 0.1592 - acc: 0.9504\n",
      "Epoch 71/100\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.1571 - acc: 0.9537\n",
      "Epoch 72/100\n",
      "9000/9000 [==============================] - 0s 35us/step - loss: 0.1568 - acc: 0.9522\n",
      "Epoch 73/100\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 0.1562 - acc: 0.9513\n",
      "Epoch 74/100\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.1555 - acc: 0.9533\n",
      "Epoch 75/100\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.1508 - acc: 0.9533\n",
      "Epoch 76/100\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.1476 - acc: 0.9554\n",
      "Epoch 77/100\n",
      "9000/9000 [==============================] - 0s 34us/step - loss: 0.1451 - acc: 0.9554\n",
      "Epoch 78/100\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 0.1486 - acc: 0.9530\n",
      "Epoch 79/100\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.1419 - acc: 0.9563\n",
      "Epoch 80/100\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.1404 - acc: 0.9563\n",
      "Epoch 81/100\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.1419 - acc: 0.9562\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000/9000 [==============================] - 0s 33us/step - loss: 0.1349 - acc: 0.9573\n",
      "Epoch 83/100\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 0.1354 - acc: 0.9581: 0s - loss: 0.1274 - acc: 0.9\n",
      "Epoch 84/100\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.1312 - acc: 0.9607\n",
      "Epoch 85/100\n",
      "9000/9000 [==============================] - 0s 35us/step - loss: 0.1288 - acc: 0.9614\n",
      "Epoch 86/100\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 0.1308 - acc: 0.9589\n",
      "Epoch 87/100\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.1268 - acc: 0.9602\n",
      "Epoch 88/100\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.1234 - acc: 0.9636\n",
      "Epoch 89/100\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.1209 - acc: 0.9637\n",
      "Epoch 90/100\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.1214 - acc: 0.9638\n",
      "Epoch 91/100\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.1202 - acc: 0.9630\n",
      "Epoch 92/100\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.1147 - acc: 0.9659\n",
      "Epoch 93/100\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.1155 - acc: 0.9659\n",
      "Epoch 94/100\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.1178 - acc: 0.9630\n",
      "Epoch 95/100\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.1146 - acc: 0.9656\n",
      "Epoch 96/100\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 0.1098 - acc: 0.9673\n",
      "Epoch 97/100\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.1108 - acc: 0.9673\n",
      "Epoch 98/100\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.1111 - acc: 0.9653\n",
      "Epoch 99/100\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.1093 - acc: 0.9679\n",
      "Epoch 100/100\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.1080 - acc: 0.9670\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14f1f73c8>"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_labeled = Sequential()\n",
    "in_dim = X_train_labeled.shape[1]\n",
    "out_dim = onehot_y.shape[1]\n",
    "\n",
    "# first hidden layer\n",
    "classifier_labeled.add(Dense(40, activation='relu', kernel_initializer='random_normal', input_dim=in_dim))\n",
    "# second hidden layer\n",
    "classifier_labeled.add(Dense(30, activation='relu', kernel_initializer='random_normal'))\n",
    "# third hidden layer\n",
    "classifier_labeled.add(Dense(20, activation='relu', kernel_initializer='random_normal'))\n",
    "# fourth hidden layer\n",
    "classifier_labeled.add(Dense(15, activation='relu', kernel_initializer='random_normal'))\n",
    "# output layer\n",
    "classifier_labeled.add(Dense(out_dim, activation='softmax', kernel_initializer='random_normal'))\n",
    "\n",
    "classifier_labeled.compile(optimizer ='adam',loss='categorical_crossentropy', metrics =['accuracy'])\n",
    "classifier_labeled.fit(X_train_labeled, onehot_y, batch_size=100, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_y = classifier_labeled.predict(X_train_unlabeled)\n",
    "# find most likely category from soft max\n",
    "pseudo_y = np.argmax(pseudo_y, axis=-1)\n",
    "\n",
    "train_data_pseudo = pd.DataFrame(X_train_unlabeled.copy())\n",
    "train_data_pseudo['y'] = pseudo_y\n",
    "# train_data_pseudo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 41)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_augmented = pd.DataFrame(X_train_labeled.copy())\n",
    "train_data_augmented['y'] = y_train\n",
    "train_data_augmented = pd.concat([train_data_augmented, train_data_pseudo])\n",
    "# train_data_augmented = train_data_augmented.sample(frac=1)\n",
    "train_data_augmented.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_augmented = train_data_augmented.iloc[:,:train_data_augmented.shape[1]-1]\n",
    "# X_train_augmented.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_augmented = train_data_augmented.iloc[:,train_data_augmented.shape[1]-1:]\n",
    "# y_train_augmented.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:95: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:128: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "encoder.fit(y_train_augmented)\n",
    "encoded_y_augmented = encoder.transform(y_train_augmented)\n",
    "onehot_y_augmented = np_utils.to_categorical(encoded_y_augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "30000/30000 [==============================] - 2s 52us/step - loss: 1.5037 - acc: 0.4417\n",
      "Epoch 2/100\n",
      "30000/30000 [==============================] - 1s 32us/step - loss: 0.7668 - acc: 0.7653\n",
      "Epoch 3/100\n",
      "30000/30000 [==============================] - 1s 32us/step - loss: 0.6336 - acc: 0.8084\n",
      "Epoch 4/100\n",
      "30000/30000 [==============================] - 1s 32us/step - loss: 0.5636 - acc: 0.8319\n",
      "Epoch 5/100\n",
      "30000/30000 [==============================] - 1s 34us/step - loss: 0.5103 - acc: 0.8475\n",
      "Epoch 6/100\n",
      "30000/30000 [==============================] - 1s 34us/step - loss: 0.4725 - acc: 0.8571\n",
      "Epoch 7/100\n",
      "30000/30000 [==============================] - 1s 34us/step - loss: 0.4448 - acc: 0.8640\n",
      "Epoch 8/100\n",
      "30000/30000 [==============================] - 1s 34us/step - loss: 0.4245 - acc: 0.8690: 1s - loss: 0\n",
      "Epoch 9/100\n",
      "30000/30000 [==============================] - 1s 34us/step - loss: 0.4082 - acc: 0.8738\n",
      "Epoch 10/100\n",
      "30000/30000 [==============================] - 1s 34us/step - loss: 0.3938 - acc: 0.8749\n",
      "Epoch 11/100\n",
      "30000/30000 [==============================] - 1s 32us/step - loss: 0.3857 - acc: 0.8787\n",
      "Epoch 12/100\n",
      "30000/30000 [==============================] - 1s 33us/step - loss: 0.3771 - acc: 0.8814\n",
      "Epoch 13/100\n",
      "30000/30000 [==============================] - 1s 34us/step - loss: 0.3675 - acc: 0.8839\n",
      "Epoch 14/100\n",
      "30000/30000 [==============================] - 1s 32us/step - loss: 0.3592 - acc: 0.8860\n",
      "Epoch 15/100\n",
      "30000/30000 [==============================] - 1s 33us/step - loss: 0.3516 - acc: 0.8862\n",
      "Epoch 16/100\n",
      "30000/30000 [==============================] - 1s 34us/step - loss: 0.3442 - acc: 0.8885\n",
      "Epoch 17/100\n",
      "30000/30000 [==============================] - 1s 32us/step - loss: 0.3359 - acc: 0.8913\n",
      "Epoch 18/100\n",
      "30000/30000 [==============================] - 1s 34us/step - loss: 0.3281 - acc: 0.8933\n",
      "Epoch 19/100\n",
      "30000/30000 [==============================] - 1s 33us/step - loss: 0.3245 - acc: 0.8931\n",
      "Epoch 20/100\n",
      "30000/30000 [==============================] - 1s 34us/step - loss: 0.3204 - acc: 0.8953\n",
      "Epoch 21/100\n",
      "30000/30000 [==============================] - 1s 35us/step - loss: 0.3135 - acc: 0.8975\n",
      "Epoch 22/100\n",
      "30000/30000 [==============================] - 1s 33us/step - loss: 0.3089 - acc: 0.8981\n",
      "Epoch 23/100\n",
      "30000/30000 [==============================] - 1s 35us/step - loss: 0.3037 - acc: 0.8987\n",
      "Epoch 24/100\n",
      "30000/30000 [==============================] - 1s 33us/step - loss: 0.2989 - acc: 0.9007\n",
      "Epoch 25/100\n",
      "30000/30000 [==============================] - 1s 33us/step - loss: 0.2966 - acc: 0.9007\n",
      "Epoch 26/100\n",
      "30000/30000 [==============================] - 1s 34us/step - loss: 0.2932 - acc: 0.9018\n",
      "Epoch 27/100\n",
      "30000/30000 [==============================] - 1s 33us/step - loss: 0.2900 - acc: 0.9019\n",
      "Epoch 28/100\n",
      "30000/30000 [==============================] - 1s 34us/step - loss: 0.2857 - acc: 0.9029\n",
      "Epoch 29/100\n",
      "30000/30000 [==============================] - 1s 34us/step - loss: 0.2850 - acc: 0.9049\n",
      "Epoch 30/100\n",
      "30000/30000 [==============================] - 1s 34us/step - loss: 0.2809 - acc: 0.9036\n",
      "Epoch 31/100\n",
      "30000/30000 [==============================] - 1s 33us/step - loss: 0.2777 - acc: 0.9066\n",
      "Epoch 32/100\n",
      "30000/30000 [==============================] - 1s 34us/step - loss: 0.2763 - acc: 0.9066\n",
      "Epoch 33/100\n",
      "30000/30000 [==============================] - 1s 34us/step - loss: 0.2725 - acc: 0.9071\n",
      "Epoch 34/100\n",
      "30000/30000 [==============================] - 1s 33us/step - loss: 0.2703 - acc: 0.9084\n",
      "Epoch 35/100\n",
      "30000/30000 [==============================] - 1s 32us/step - loss: 0.2708 - acc: 0.9080\n",
      "Epoch 36/100\n",
      "30000/30000 [==============================] - 1s 34us/step - loss: 0.2678 - acc: 0.9084\n",
      "Epoch 37/100\n",
      "30000/30000 [==============================] - 1s 34us/step - loss: 0.2656 - acc: 0.9091\n",
      "Epoch 38/100\n",
      "30000/30000 [==============================] - 1s 33us/step - loss: 0.2624 - acc: 0.9110\n",
      "Epoch 39/100\n",
      "30000/30000 [==============================] - 1s 34us/step - loss: 0.2606 - acc: 0.9113\n",
      "Epoch 40/100\n",
      "30000/30000 [==============================] - 1s 34us/step - loss: 0.2616 - acc: 0.9099\n",
      "Epoch 41/100\n",
      "30000/30000 [==============================] - 1s 33us/step - loss: 0.2556 - acc: 0.9119\n",
      "Epoch 42/100\n",
      "30000/30000 [==============================] - 1s 33us/step - loss: 0.2569 - acc: 0.9116\n",
      "Epoch 43/100\n",
      "30000/30000 [==============================] - 1s 34us/step - loss: 0.2544 - acc: 0.9114\n",
      "Epoch 44/100\n",
      "30000/30000 [==============================] - 1s 33us/step - loss: 0.2520 - acc: 0.9129\n",
      "Epoch 45/100\n",
      "30000/30000 [==============================] - 1s 33us/step - loss: 0.2500 - acc: 0.9141\n",
      "Epoch 46/100\n",
      "30000/30000 [==============================] - 1s 33us/step - loss: 0.2489 - acc: 0.9140\n",
      "Epoch 47/100\n",
      "30000/30000 [==============================] - 1s 33us/step - loss: 0.2476 - acc: 0.9143\n",
      "Epoch 48/100\n",
      "30000/30000 [==============================] - 1s 33us/step - loss: 0.2472 - acc: 0.9148\n",
      "Epoch 49/100\n",
      "30000/30000 [==============================] - 1s 33us/step - loss: 0.2457 - acc: 0.9151\n",
      "Epoch 50/100\n",
      "30000/30000 [==============================] - 1s 33us/step - loss: 0.2431 - acc: 0.9157\n",
      "Epoch 51/100\n",
      "30000/30000 [==============================] - 1s 33us/step - loss: 0.2419 - acc: 0.9152\n",
      "Epoch 52/100\n",
      "30000/30000 [==============================] - 1s 34us/step - loss: 0.2401 - acc: 0.9169\n",
      "Epoch 53/100\n",
      "30000/30000 [==============================] - 1s 38us/step - loss: 0.2397 - acc: 0.9170\n",
      "Epoch 54/100\n",
      "30000/30000 [==============================] - 1s 37us/step - loss: 0.2371 - acc: 0.9166\n",
      "Epoch 55/100\n",
      "30000/30000 [==============================] - 1s 39us/step - loss: 0.2365 - acc: 0.9185\n",
      "Epoch 56/100\n",
      "30000/30000 [==============================] - 1s 39us/step - loss: 0.2338 - acc: 0.9192\n",
      "Epoch 57/100\n",
      "30000/30000 [==============================] - 1s 38us/step - loss: 0.2344 - acc: 0.9203\n",
      "Epoch 58/100\n",
      "30000/30000 [==============================] - 1s 37us/step - loss: 0.2313 - acc: 0.9195\n",
      "Epoch 59/100\n",
      "30000/30000 [==============================] - 1s 39us/step - loss: 0.2310 - acc: 0.9197\n",
      "Epoch 60/100\n",
      "30000/30000 [==============================] - 1s 38us/step - loss: 0.2293 - acc: 0.9204\n",
      "Epoch 61/100\n",
      "30000/30000 [==============================] - 1s 39us/step - loss: 0.2281 - acc: 0.9203\n",
      "Epoch 62/100\n",
      "30000/30000 [==============================] - 1s 38us/step - loss: 0.2281 - acc: 0.9199\n",
      "Epoch 63/100\n",
      "30000/30000 [==============================] - 1s 39us/step - loss: 0.2285 - acc: 0.9201\n",
      "Epoch 64/100\n",
      "30000/30000 [==============================] - 1s 38us/step - loss: 0.2262 - acc: 0.9213\n",
      "Epoch 65/100\n",
      "30000/30000 [==============================] - 1s 39us/step - loss: 0.2229 - acc: 0.9232\n",
      "Epoch 66/100\n",
      "30000/30000 [==============================] - 1s 38us/step - loss: 0.2223 - acc: 0.9236\n",
      "Epoch 67/100\n",
      "30000/30000 [==============================] - 1s 39us/step - loss: 0.2223 - acc: 0.9234\n",
      "Epoch 68/100\n",
      "30000/30000 [==============================] - 1s 38us/step - loss: 0.2215 - acc: 0.9229\n",
      "Epoch 69/100\n",
      "30000/30000 [==============================] - 1s 38us/step - loss: 0.2205 - acc: 0.9237\n",
      "Epoch 70/100\n",
      "30000/30000 [==============================] - 1s 39us/step - loss: 0.2185 - acc: 0.9231\n",
      "Epoch 71/100\n",
      "30000/30000 [==============================] - 1s 38us/step - loss: 0.2174 - acc: 0.9252\n",
      "Epoch 72/100\n",
      "30000/30000 [==============================] - 2s 61us/step - loss: 0.2168 - acc: 0.9243\n",
      "Epoch 73/100\n",
      "30000/30000 [==============================] - 1s 41us/step - loss: 0.2167 - acc: 0.9236\n",
      "Epoch 74/100\n",
      "30000/30000 [==============================] - 1s 45us/step - loss: 0.2144 - acc: 0.9251\n",
      "Epoch 75/100\n",
      "30000/30000 [==============================] - 1s 41us/step - loss: 0.2130 - acc: 0.9252\n",
      "Epoch 76/100\n",
      "30000/30000 [==============================] - 1s 41us/step - loss: 0.2150 - acc: 0.9247\n",
      "Epoch 77/100\n",
      "30000/30000 [==============================] - 1s 39us/step - loss: 0.2107 - acc: 0.9259\n",
      "Epoch 78/100\n",
      "30000/30000 [==============================] - 1s 44us/step - loss: 0.2124 - acc: 0.9252\n",
      "Epoch 79/100\n",
      "30000/30000 [==============================] - 1s 39us/step - loss: 0.2106 - acc: 0.9267\n",
      "Epoch 80/100\n",
      "30000/30000 [==============================] - 1s 38us/step - loss: 0.2108 - acc: 0.9251\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000/30000 [==============================] - 1s 32us/step - loss: 0.2091 - acc: 0.9259\n",
      "Epoch 82/100\n",
      "30000/30000 [==============================] - 1s 36us/step - loss: 0.2096 - acc: 0.9269\n",
      "Epoch 83/100\n",
      "30000/30000 [==============================] - 1s 31us/step - loss: 0.2067 - acc: 0.9278\n",
      "Epoch 84/100\n",
      "30000/30000 [==============================] - 1s 31us/step - loss: 0.2075 - acc: 0.9259\n",
      "Epoch 85/100\n",
      "30000/30000 [==============================] - 1s 31us/step - loss: 0.2078 - acc: 0.9262\n",
      "Epoch 86/100\n",
      "30000/30000 [==============================] - 1s 32us/step - loss: 0.2053 - acc: 0.9276\n",
      "Epoch 87/100\n",
      "30000/30000 [==============================] - 1s 31us/step - loss: 0.2058 - acc: 0.9271\n",
      "Epoch 88/100\n",
      "30000/30000 [==============================] - 1s 31us/step - loss: 0.2050 - acc: 0.9270\n",
      "Epoch 89/100\n",
      "30000/30000 [==============================] - 1s 31us/step - loss: 0.2040 - acc: 0.9273\n",
      "Epoch 90/100\n",
      "30000/30000 [==============================] - 1s 31us/step - loss: 0.2044 - acc: 0.9278\n",
      "Epoch 91/100\n",
      "30000/30000 [==============================] - 1s 31us/step - loss: 0.2016 - acc: 0.9293\n",
      "Epoch 92/100\n",
      "30000/30000 [==============================] - 1s 32us/step - loss: 0.2013 - acc: 0.9278\n",
      "Epoch 93/100\n",
      "30000/30000 [==============================] - 1s 32us/step - loss: 0.1994 - acc: 0.9296\n",
      "Epoch 94/100\n",
      "30000/30000 [==============================] - 1s 32us/step - loss: 0.1995 - acc: 0.9294\n",
      "Epoch 95/100\n",
      "30000/30000 [==============================] - 1s 31us/step - loss: 0.1985 - acc: 0.9302\n",
      "Epoch 96/100\n",
      "30000/30000 [==============================] - 1s 31us/step - loss: 0.1972 - acc: 0.9313\n",
      "Epoch 97/100\n",
      "30000/30000 [==============================] - 1s 31us/step - loss: 0.1971 - acc: 0.9302\n",
      "Epoch 98/100\n",
      "30000/30000 [==============================] - 1s 32us/step - loss: 0.1969 - acc: 0.9319\n",
      "Epoch 99/100\n",
      "30000/30000 [==============================] - 1s 31us/step - loss: 0.1971 - acc: 0.9313\n",
      "Epoch 100/100\n",
      "30000/30000 [==============================] - 1s 31us/step - loss: 0.1941 - acc: 0.9323\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x128a2d160>"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_pseudo = Sequential()\n",
    "in_dim = X_train_augmented.shape[1]\n",
    "out_dim = onehot_y_augmented.shape[1]\n",
    "\n",
    "# first hidden Layer\n",
    "classifier_pseudo.add(Dense(40, activation='relu', kernel_initializer='random_normal', input_dim=in_dim))\n",
    "# second hidden Layer\n",
    "classifier_pseudo.add(Dense(30, activation='relu', kernel_initializer='random_normal'))\n",
    "# third hidden Layer\n",
    "classifier_pseudo.add(Dense(20, activation='relu', kernel_initializer='random_normal'))\n",
    "# fourth hidden Layer\n",
    "classifier_pseudo.add(Dense(15, activation='relu', kernel_initializer='random_normal'))\n",
    "# output layer\n",
    "classifier_pseudo.add(Dense(out_dim, activation='softmax', kernel_initializer='random_normal'))\n",
    "\n",
    "classifier_pseudo.compile(optimizer ='adam',loss='categorical_crossentropy', metrics =['accuracy'])\n",
    "classifier_pseudo.fit(X_train_augmented, onehot_y_augmented, batch_size=100, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_semi = classifier_pseudo.predict(X_test)\n",
    "# find most likely category from soft max\n",
    "pred_semi = np.argmax(pred_semi, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert back to pandas dataframe\n",
    "X_test = pd.DataFrame(X_test)\n",
    "pred_submit = pd.DataFrame(list(zip(X_test.index.values + 30000, pred_semi)), columns=['Id', 'y'])\n",
    "final_submit = pred_submit.to_csv(\"submit.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def augment_labeled_data(n_batches, X_labeled, y, X_unlabeled, classifier):\n",
    "    # initially labeled data\n",
    "    labeled_data = X_labeled.copy()\n",
    "    labeled_data['y'] = y.copy()\n",
    "    labels = labeled_data['y']\n",
    "    # initial one-hot encoding for labels\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(labels)\n",
    "    encoded_y = encoder.transform(labels)\n",
    "    onehot_y = np_utils.to_categorical(encoded_y)\n",
    "    # split unlabeled data into chunks\n",
    "    batches = np.array_split(X_unlabeled, n_batches)\n",
    "    # augment labeled data by chunks\n",
    "    for batch in batches:\n",
    "        # pseudo label current batch\n",
    "        pseudo_y = classifier.predict(batch)\n",
    "        pseudo_y = np.argmax(pseudo_y, axis=-1)\n",
    "        # append newly labeled batch to already labeled data\n",
    "        pseudo_data = pd.DataFrame(batch.copy())\n",
    "        pseudo_data['y'] = pseudo_y\n",
    "        labeled_data = pd.concat([labeled_data, pseudo_data])\n",
    "        labels = labeled_data['y']\n",
    "        # redo one-hot encoding for labels\n",
    "        encoder.fit(labels)\n",
    "        encoded_y = encoder.transform(labels)\n",
    "        onehot_y = np_utils.to_categorical(encoded_y)\n",
    "        classifier.fit()\n",
    "        # train classifier with augmented data\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=1000,\n",
       "    n_clusters=10, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# know from labeled data that 0 <= y <= 9\n",
    "k = 10\n",
    "kmean = KMeans(n_clusters=k, init=\"k-means++\", max_iter=1000)\n",
    "kmean.fit(X_train_unlabeled)\n",
    "pred = kmean.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
