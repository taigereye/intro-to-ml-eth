{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to ML Project\n",
    "## Task 4\n",
    "### Jan Bauer, Alaisha Sharma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils, to_categorical\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# same seed for consistency\n",
    "seed = 1\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_labeled = pd.read_hdf(\"data/train_labeled.h5\", \"train\")\n",
    "# train_data_labeled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_labeled = train_data_labeled.iloc[:,1:]\n",
    "# X_train_labeled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = train_data_labeled.iloc[:,0:1]\n",
    "# y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_unlabeled = pd.read_hdf(\"data/train_unlabeled.h5\", \"train\")\n",
    "# train_data_unlabeled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_unlabeled = train_data_unlabeled.iloc[:,:]\n",
    "# X_train_unlabeled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = pd.read_hdf(\"data/test.h5\", \"test\")\n",
    "# X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()  \n",
    "scaler.fit(X_train_labeled)  \n",
    "X_train_labeled = scaler.transform(X_train_labeled)  \n",
    "X_train_unlabeled = scaler.transform(X_train_unlabeled)\n",
    "X_test = scaler.transform(X_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pca = PCA(n_components=139)\n",
    "pca.fit(X_train_labeled)\n",
    "\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Cumulative explained variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9000, 52) (21000, 52) (8000, 52)\n"
     ]
    }
   ],
   "source": [
    "k = 52\n",
    "pca = PCA(n_components=k)\n",
    "X_train_labeled = pca.fit_transform(X_train_labeled)\n",
    "\n",
    "X_train_unlabeled = pca.transform(X_train_unlabeled)\n",
    "X_test = pca.transform(X_test)\n",
    "pca_std = np.std(X_train_labeled)\n",
    "\n",
    "print(X_train_labeled.shape, X_train_unlabeled.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:95: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:128: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_train)\n",
    "encoded_y = encoder.transform(y_train)\n",
    "onehot_y = np_utils.to_categorical(encoded_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "9000/9000 [==============================] - 4s 435us/step - loss: 1.3273 - acc: 0.6119\n",
      "Epoch 2/100\n",
      "9000/9000 [==============================] - 1s 71us/step - loss: 0.4751 - acc: 0.8497\n",
      "Epoch 3/100\n",
      "9000/9000 [==============================] - 1s 74us/step - loss: 0.3655 - acc: 0.8870\n",
      "Epoch 4/100\n",
      "9000/9000 [==============================] - 1s 75us/step - loss: 0.3027 - acc: 0.9057\n",
      "Epoch 5/100\n",
      "9000/9000 [==============================] - 1s 68us/step - loss: 0.2593 - acc: 0.9166\n",
      "Epoch 6/100\n",
      "9000/9000 [==============================] - 1s 80us/step - loss: 0.2264 - acc: 0.9267\n",
      "Epoch 7/100\n",
      "9000/9000 [==============================] - 1s 76us/step - loss: 0.1937 - acc: 0.9399\n",
      "Epoch 8/100\n",
      "9000/9000 [==============================] - 1s 94us/step - loss: 0.1656 - acc: 0.9483\n",
      "Epoch 9/100\n",
      "9000/9000 [==============================] - 1s 99us/step - loss: 0.1482 - acc: 0.9536\n",
      "Epoch 10/100\n",
      "9000/9000 [==============================] - 1s 77us/step - loss: 0.1227 - acc: 0.9610\n",
      "Epoch 11/100\n",
      "9000/9000 [==============================] - 1s 90us/step - loss: 0.1105 - acc: 0.9657\n",
      "Epoch 12/100\n",
      "9000/9000 [==============================] - 1s 69us/step - loss: 0.0895 - acc: 0.9739\n",
      "Epoch 13/100\n",
      "9000/9000 [==============================] - 1s 68us/step - loss: 0.0797 - acc: 0.9772\n",
      "Epoch 14/100\n",
      "9000/9000 [==============================] - 1s 70us/step - loss: 0.0687 - acc: 0.9803: 0s - loss: 0.0517 - \n",
      "Epoch 15/100\n",
      "9000/9000 [==============================] - 1s 69us/step - loss: 0.0506 - acc: 0.9874\n",
      "Epoch 16/100\n",
      "9000/9000 [==============================] - 1s 68us/step - loss: 0.0439 - acc: 0.9886\n",
      "Epoch 17/100\n",
      "9000/9000 [==============================] - 1s 68us/step - loss: 0.0383 - acc: 0.9908\n",
      "Epoch 18/100\n",
      "9000/9000 [==============================] - 1s 68us/step - loss: 0.0329 - acc: 0.9920\n",
      "Epoch 19/100\n",
      "9000/9000 [==============================] - 1s 68us/step - loss: 0.0257 - acc: 0.9943\n",
      "Epoch 20/100\n",
      "9000/9000 [==============================] - 1s 67us/step - loss: 0.0192 - acc: 0.9968\n",
      "Epoch 21/100\n",
      "9000/9000 [==============================] - 1s 68us/step - loss: 0.0146 - acc: 0.9984\n",
      "Epoch 22/100\n",
      "9000/9000 [==============================] - 1s 68us/step - loss: 0.0103 - acc: 0.9989\n",
      "Epoch 23/100\n",
      "9000/9000 [==============================] - 1s 67us/step - loss: 0.0085 - acc: 0.9997\n",
      "Epoch 24/100\n",
      "9000/9000 [==============================] - 1s 70us/step - loss: 0.0060 - acc: 0.9999\n",
      "Epoch 25/100\n",
      "9000/9000 [==============================] - 1s 68us/step - loss: 0.0047 - acc: 0.9999\n",
      "Epoch 26/100\n",
      "9000/9000 [==============================] - 1s 67us/step - loss: 0.0037 - acc: 1.0000\n",
      "Epoch 27/100\n",
      "9000/9000 [==============================] - 1s 68us/step - loss: 0.0029 - acc: 1.0000\n",
      "Epoch 28/100\n",
      "9000/9000 [==============================] - 1s 68us/step - loss: 0.0027 - acc: 1.0000\n",
      "Epoch 29/100\n",
      "9000/9000 [==============================] - 1s 67us/step - loss: 0.0023 - acc: 1.0000\n",
      "Epoch 30/100\n",
      "9000/9000 [==============================] - 1s 69us/step - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 31/100\n",
      "9000/9000 [==============================] - 1s 70us/step - loss: 0.0018 - acc: 1.0000: 0s - loss: 0.0016 - ac\n",
      "Epoch 32/100\n",
      "9000/9000 [==============================] - 1s 69us/step - loss: 0.0015 - acc: 1.0000\n",
      "Epoch 33/100\n",
      "9000/9000 [==============================] - 1s 68us/step - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 34/100\n",
      "9000/9000 [==============================] - 1s 69us/step - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 35/100\n",
      "9000/9000 [==============================] - 1s 71us/step - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 36/100\n",
      "9000/9000 [==============================] - 1s 68us/step - loss: 0.0010 - acc: 1.0000\n",
      "Epoch 37/100\n",
      "9000/9000 [==============================] - 1s 68us/step - loss: 9.4671e-04 - acc: 1.0000\n",
      "Epoch 38/100\n",
      "9000/9000 [==============================] - 1s 68us/step - loss: 8.6707e-04 - acc: 1.0000\n",
      "Epoch 39/100\n",
      "9000/9000 [==============================] - 1s 67us/step - loss: 7.7346e-04 - acc: 1.0000\n",
      "Epoch 40/100\n",
      "9000/9000 [==============================] - 1s 68us/step - loss: 7.1289e-04 - acc: 1.0000\n",
      "Epoch 41/100\n",
      "9000/9000 [==============================] - 1s 68us/step - loss: 6.4517e-04 - acc: 1.0000\n",
      "Epoch 42/100\n",
      "9000/9000 [==============================] - 1s 68us/step - loss: 6.0907e-04 - acc: 1.0000\n",
      "Epoch 43/100\n",
      "9000/9000 [==============================] - 1s 67us/step - loss: 5.5175e-04 - acc: 1.0000\n",
      "Epoch 44/100\n",
      "9000/9000 [==============================] - 1s 68us/step - loss: 5.0973e-04 - acc: 1.0000\n",
      "Epoch 45/100\n",
      "9000/9000 [==============================] - 1s 67us/step - loss: 4.7466e-04 - acc: 1.0000\n",
      "Epoch 46/100\n",
      "9000/9000 [==============================] - 1s 68us/step - loss: 4.3424e-04 - acc: 1.0000\n",
      "Epoch 47/100\n",
      "9000/9000 [==============================] - 1s 67us/step - loss: 4.0079e-04 - acc: 1.0000\n",
      "Epoch 48/100\n",
      "9000/9000 [==============================] - 1s 68us/step - loss: 3.7480e-04 - acc: 1.0000\n",
      "Epoch 49/100\n",
      "9000/9000 [==============================] - 1s 68us/step - loss: 3.5219e-04 - acc: 1.0000\n",
      "Epoch 50/100\n",
      "9000/9000 [==============================] - 1s 69us/step - loss: 3.2611e-04 - acc: 1.0000\n",
      "Epoch 51/100\n",
      "9000/9000 [==============================] - 1s 70us/step - loss: 2.9789e-04 - acc: 1.0000\n",
      "Epoch 52/100\n",
      "9000/9000 [==============================] - 1s 69us/step - loss: 2.8094e-04 - acc: 1.0000\n",
      "Epoch 53/100\n",
      "9000/9000 [==============================] - 1s 69us/step - loss: 2.6223e-04 - acc: 1.0000\n",
      "Epoch 54/100\n",
      "9000/9000 [==============================] - 1s 68us/step - loss: 2.4430e-04 - acc: 1.0000\n",
      "Epoch 55/100\n",
      "9000/9000 [==============================] - 1s 67us/step - loss: 2.2948e-04 - acc: 1.0000\n",
      "Epoch 56/100\n",
      "9000/9000 [==============================] - 1s 67us/step - loss: 2.1603e-04 - acc: 1.0000\n",
      "Epoch 57/100\n",
      "9000/9000 [==============================] - 1s 67us/step - loss: 2.0055e-04 - acc: 1.0000\n",
      "Epoch 58/100\n",
      "9000/9000 [==============================] - 1s 67us/step - loss: 1.8576e-04 - acc: 1.0000\n",
      "Epoch 59/100\n",
      "9000/9000 [==============================] - 1s 67us/step - loss: 1.7319e-04 - acc: 1.0000\n",
      "Epoch 60/100\n",
      "9000/9000 [==============================] - 1s 68us/step - loss: 1.6557e-04 - acc: 1.0000\n",
      "Epoch 61/100\n",
      "9000/9000 [==============================] - 1s 67us/step - loss: 1.5226e-04 - acc: 1.0000\n",
      "Epoch 62/100\n",
      "9000/9000 [==============================] - 1s 67us/step - loss: 1.4450e-04 - acc: 1.0000\n",
      "Epoch 63/100\n",
      "9000/9000 [==============================] - 1s 68us/step - loss: 1.3288e-04 - acc: 1.0000\n",
      "Epoch 64/100\n",
      "9000/9000 [==============================] - 1s 69us/step - loss: 1.2677e-04 - acc: 1.0000\n",
      "Epoch 65/100\n",
      "9000/9000 [==============================] - 1s 71us/step - loss: 1.1912e-04 - acc: 1.0000\n",
      "Epoch 66/100\n",
      "9000/9000 [==============================] - 1s 68us/step - loss: 1.1316e-04 - acc: 1.0000\n",
      "Epoch 67/100\n",
      "9000/9000 [==============================] - 1s 67us/step - loss: 1.0476e-04 - acc: 1.0000\n",
      "Epoch 68/100\n",
      "9000/9000 [==============================] - 1s 67us/step - loss: 9.7307e-05 - acc: 1.0000\n",
      "Epoch 69/100\n",
      "9000/9000 [==============================] - 1s 69us/step - loss: 9.2760e-05 - acc: 1.0000\n",
      "Epoch 70/100\n",
      "9000/9000 [==============================] - 1s 68us/step - loss: 8.7794e-05 - acc: 1.0000\n",
      "Epoch 71/100\n",
      "9000/9000 [==============================] - 1s 68us/step - loss: 8.1955e-05 - acc: 1.0000\n",
      "Epoch 72/100\n",
      "9000/9000 [==============================] - 1s 67us/step - loss: 7.6992e-05 - acc: 1.0000\n",
      "Epoch 73/100\n",
      "9000/9000 [==============================] - 1s 71us/step - loss: 7.2897e-05 - acc: 1.0000\n",
      "Epoch 74/100\n",
      "9000/9000 [==============================] - 1s 68us/step - loss: 6.8472e-05 - acc: 1.0000\n",
      "Epoch 75/100\n",
      "9000/9000 [==============================] - 1s 67us/step - loss: 6.4354e-05 - acc: 1.0000\n",
      "Epoch 76/100\n",
      "9000/9000 [==============================] - 1s 67us/step - loss: 6.0565e-05 - acc: 1.0000\n",
      "Epoch 77/100\n",
      "9000/9000 [==============================] - 1s 68us/step - loss: 5.7171e-05 - acc: 1.0000\n",
      "Epoch 78/100\n",
      "9000/9000 [==============================] - 1s 67us/step - loss: 5.4017e-05 - acc: 1.0000\n",
      "Epoch 79/100\n",
      "9000/9000 [==============================] - 1s 69us/step - loss: 5.0847e-05 - acc: 1.0000\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000/9000 [==============================] - 1s 67us/step - loss: 4.8030e-05 - acc: 1.0000\n",
      "Epoch 81/100\n",
      "9000/9000 [==============================] - 1s 65us/step - loss: 4.5092e-05 - acc: 1.0000\n",
      "Epoch 82/100\n",
      "9000/9000 [==============================] - 1s 64us/step - loss: 4.2699e-05 - acc: 1.0000\n",
      "Epoch 83/100\n",
      "9000/9000 [==============================] - 1s 64us/step - loss: 4.0188e-05 - acc: 1.0000\n",
      "Epoch 84/100\n",
      "9000/9000 [==============================] - 1s 68us/step - loss: 3.8251e-05 - acc: 1.0000\n",
      "Epoch 85/100\n",
      "9000/9000 [==============================] - 1s 66us/step - loss: 3.5972e-05 - acc: 1.0000\n",
      "Epoch 86/100\n",
      "9000/9000 [==============================] - 1s 99us/step - loss: 3.3517e-05 - acc: 1.0000\n",
      "Epoch 87/100\n",
      "9000/9000 [==============================] - 1s 97us/step - loss: 3.2059e-05 - acc: 1.0000\n",
      "Epoch 88/100\n",
      "9000/9000 [==============================] - 1s 67us/step - loss: 3.0185e-05 - acc: 1.0000\n",
      "Epoch 89/100\n",
      "9000/9000 [==============================] - 1s 67us/step - loss: 2.8285e-05 - acc: 1.0000\n",
      "Epoch 90/100\n",
      "9000/9000 [==============================] - 1s 68us/step - loss: 2.7176e-05 - acc: 1.0000\n",
      "Epoch 91/100\n",
      "9000/9000 [==============================] - 1s 66us/step - loss: 2.5553e-05 - acc: 1.0000\n",
      "Epoch 92/100\n",
      "9000/9000 [==============================] - 1s 65us/step - loss: 2.4068e-05 - acc: 1.0000\n",
      "Epoch 93/100\n",
      "9000/9000 [==============================] - 1s 66us/step - loss: 2.2977e-05 - acc: 1.0000\n",
      "Epoch 94/100\n",
      "9000/9000 [==============================] - 1s 65us/step - loss: 2.1523e-05 - acc: 1.0000\n",
      "Epoch 95/100\n",
      "9000/9000 [==============================] - 1s 66us/step - loss: 2.0566e-05 - acc: 1.0000\n",
      "Epoch 96/100\n",
      "9000/9000 [==============================] - 1s 66us/step - loss: 1.9388e-05 - acc: 1.0000\n",
      "Epoch 97/100\n",
      "9000/9000 [==============================] - 1s 65us/step - loss: 1.8144e-05 - acc: 1.0000\n",
      "Epoch 98/100\n",
      "9000/9000 [==============================] - 1s 66us/step - loss: 1.7428e-05 - acc: 1.0000\n",
      "Epoch 99/100\n",
      "9000/9000 [==============================] - 1s 67us/step - loss: 1.6408e-05 - acc: 1.0000\n",
      "Epoch 100/100\n",
      "9000/9000 [==============================] - 1s 66us/step - loss: 1.5722e-05 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1618f60f0>"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_labeled = Sequential()\n",
    "in_dim = X_train_labeled.shape[1]\n",
    "out_dim = onehot_y.shape[1]\n",
    "\n",
    "# first hidden layer\n",
    "classifier_labeled.add(Dense(100, activation='relu', kernel_initializer='random_normal', input_dim=in_dim))\n",
    "# second hidden layer\n",
    "classifier_labeled.add(Dense(100, activation='relu', kernel_initializer='random_normal'))\n",
    "# third hidden layer\n",
    "classifier_labeled.add(Dense(100, activation='relu', kernel_initializer='random_normal'))\n",
    "# output layer\n",
    "classifier_labeled.add(Dense(out_dim, activation='softmax', kernel_initializer='random_normal'))\n",
    "\n",
    "classifier_labeled.compile(optimizer='adam', loss='categorical_crossentropy', metrics =['accuracy'])\n",
    "classifier_labeled.fit(X_train_labeled, onehot_y, batch_size=100, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_classifier(X_train, onehot_y, n_input_neurons, N_hidden_neurons):\n",
    "    # define basic classifier model\n",
    "    classifier = Sequential()\n",
    "    in_dim = X_train.shape[1]\n",
    "    out_dim = onehot_y.shape[1]\n",
    "    # input layer\n",
    "    classifier.add(Dense(n_input_neurons, activation='relu', kernel_initializer='random_normal', input_dim=in_dim))\n",
    "    # build hidden layers of classifier\n",
    "    for n in N_hidden_neurons:\n",
    "        # create hidden layer\n",
    "        classifier.add(Dense(n, activation='relu', kernel_initializer='random_normal'))\n",
    "    # output layer\n",
    "    classifier.add(Dense(out_dim, activation='softmax', kernel_initializer='random_normal'))\n",
    "    # add SGD\n",
    "    sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    classifier.compile(optimizer=sgd, loss='categorical_crossentropy', metrics =['accuracy'])\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def augment_labeled_data(n_batches, tau, X_labeled, y, X_unlabeled, classifier):\n",
    "    # initial model\n",
    "    model = classifier\n",
    "    # initially labeled data\n",
    "    labeled_data = X_labeled.copy()\n",
    "    labels = y.copy()\n",
    "    # initial one-hot encoding for labels\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(labels)\n",
    "    encoded_y = encoder.transform(labels)\n",
    "    onehot_y = np_utils.to_categorical(encoded_y)\n",
    "    # split unlabeled data into chunks\n",
    "    batches = np.array_split(X_unlabeled, n_batches)\n",
    "    # augment labeled data by chunks\n",
    "    for i, batch in zip(range(1, n_batches+1), batches):\n",
    "        print()\n",
    "        print(\"AUGMENTING DATASET WITH BATCH #\", i, \":\")\n",
    "        print()\n",
    "        # pseudo labels for current batch\n",
    "        pseudo_y = classifier.predict(batch)\n",
    "        pseudo_maxes = np.max(pseudo_y, axis=-1)\n",
    "        pseudo_classes = np.argmax(pseudo_y, axis=-1)\n",
    "        pseudo_labels = pd.DataFrame({'y': pseudo_classes, 'confidence': pseudo_maxes})\n",
    "        # only accept labels if confidence high enough\n",
    "        labeling = pd.concat([pd.DataFrame(batch), pseudo_labels], axis=1)\n",
    "        labeling = labeling[labeling.confidence >= tau]  \n",
    "        print(\"Labels accepted: \", len(labeling), \"/\", len(batch))\n",
    "        print()\n",
    "        new_labeled_data = labeling.drop(columns=['y', 'confidence'])\n",
    "        new_labels = labeling.drop(columns=['confidence']).iloc[:,labeling.shape[1]-2:]\n",
    "        # append newly labeled batch to already labeled data\n",
    "        labeled_data = pd.concat([pd.DataFrame(labeled_data), new_labeled_data])\n",
    "        labels = pd.concat([labels, new_labels])\n",
    "        # redo one-hot encoding for labels\n",
    "        encoder.fit(labels)\n",
    "        encoded_y = encoder.transform(labels)\n",
    "        onehot_y = np_utils.to_categorical(encoded_y)\n",
    "        # train new model with augmented data\n",
    "        classifier = create_classifier(labeled_data, onehot_y, 500, [500, 500])\n",
    "        classifier.fit(labeled_data, onehot_y, batch_size=1000, epochs=50)\n",
    "    print()\n",
    "    print(\"Size of final dataset: \", labeled_data.shape)\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:95: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:128: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AUGMENTING DATASET WITH BATCH # 1 :\n",
      "\n",
      "Labels accepted:  1984 / 2100\n",
      "\n",
      "Epoch 1/50\n",
      "10984/10984 [==============================] - 5s 418us/step - loss: 1.3958 - acc: 0.5865\n",
      "Epoch 2/50\n",
      "10984/10984 [==============================] - 2s 137us/step - loss: 0.6592 - acc: 0.8042\n",
      "Epoch 3/50\n",
      "10984/10984 [==============================] - 1s 121us/step - loss: 0.3484 - acc: 0.8897\n",
      "Epoch 4/50\n",
      "10984/10984 [==============================] - 2s 138us/step - loss: 0.2587 - acc: 0.9196\n",
      "Epoch 5/50\n",
      "10984/10984 [==============================] - 1s 116us/step - loss: 0.1992 - acc: 0.9387\n",
      "Epoch 6/50\n",
      "10984/10984 [==============================] - 1s 117us/step - loss: 0.1606 - acc: 0.9506\n",
      "Epoch 7/50\n",
      "10984/10984 [==============================] - 1s 117us/step - loss: 0.1262 - acc: 0.9634\n",
      "Epoch 8/50\n",
      "10984/10984 [==============================] - 1s 120us/step - loss: 0.1020 - acc: 0.9708\n",
      "Epoch 9/50\n",
      "10984/10984 [==============================] - 1s 116us/step - loss: 0.0840 - acc: 0.9770\n",
      "Epoch 10/50\n",
      "10984/10984 [==============================] - 1s 117us/step - loss: 0.0631 - acc: 0.9859\n",
      "Epoch 11/50\n",
      "10984/10984 [==============================] - 1s 117us/step - loss: 0.0519 - acc: 0.9899\n",
      "Epoch 12/50\n",
      "10984/10984 [==============================] - 1s 118us/step - loss: 0.0407 - acc: 0.9932\n",
      "Epoch 13/50\n",
      "10984/10984 [==============================] - 1s 117us/step - loss: 0.0320 - acc: 0.9956\n",
      "Epoch 14/50\n",
      "10984/10984 [==============================] - 1s 117us/step - loss: 0.0252 - acc: 0.9980\n",
      "Epoch 15/50\n",
      "10984/10984 [==============================] - 1s 118us/step - loss: 0.0204 - acc: 0.9988\n",
      "Epoch 16/50\n",
      "10984/10984 [==============================] - 1s 118us/step - loss: 0.0166 - acc: 0.9995\n",
      "Epoch 17/50\n",
      "10984/10984 [==============================] - 1s 119us/step - loss: 0.0137 - acc: 0.9997\n",
      "Epoch 18/50\n",
      "10984/10984 [==============================] - 1s 117us/step - loss: 0.0119 - acc: 0.9998\n",
      "Epoch 19/50\n",
      "10984/10984 [==============================] - 1s 118us/step - loss: 0.0102 - acc: 0.9997\n",
      "Epoch 20/50\n",
      "10984/10984 [==============================] - 1s 118us/step - loss: 0.0088 - acc: 0.9998\n",
      "Epoch 21/50\n",
      "10984/10984 [==============================] - 1s 116us/step - loss: 0.0081 - acc: 0.9999\n",
      "Epoch 22/50\n",
      "10984/10984 [==============================] - 1s 117us/step - loss: 0.0073 - acc: 0.9999\n",
      "Epoch 23/50\n",
      "10984/10984 [==============================] - 1s 117us/step - loss: 0.0067 - acc: 0.9999\n",
      "Epoch 24/50\n",
      "10984/10984 [==============================] - 1s 118us/step - loss: 0.0061 - acc: 0.9999\n",
      "Epoch 25/50\n",
      "10984/10984 [==============================] - 1s 116us/step - loss: 0.0057 - acc: 0.9999\n",
      "Epoch 26/50\n",
      "10984/10984 [==============================] - 1s 115us/step - loss: 0.0053 - acc: 0.9999\n",
      "Epoch 27/50\n",
      "10984/10984 [==============================] - 1s 117us/step - loss: 0.0050 - acc: 0.9999\n",
      "Epoch 28/50\n",
      "10984/10984 [==============================] - 1s 121us/step - loss: 0.0048 - acc: 0.9999\n",
      "Epoch 29/50\n",
      "10984/10984 [==============================] - 2s 148us/step - loss: 0.0045 - acc: 0.9999\n",
      "Epoch 30/50\n",
      "10984/10984 [==============================] - 1s 124us/step - loss: 0.0043 - acc: 0.9999\n",
      "Epoch 31/50\n",
      "10984/10984 [==============================] - 1s 119us/step - loss: 0.0041 - acc: 0.9999\n",
      "Epoch 32/50\n",
      "10984/10984 [==============================] - 1s 116us/step - loss: 0.0040 - acc: 0.9999\n",
      "Epoch 33/50\n",
      "10984/10984 [==============================] - 1s 116us/step - loss: 0.0038 - acc: 0.9999\n",
      "Epoch 34/50\n",
      "10984/10984 [==============================] - 1s 116us/step - loss: 0.0037 - acc: 0.9999\n",
      "Epoch 35/50\n",
      "10984/10984 [==============================] - 1s 117us/step - loss: 0.0036 - acc: 0.9999\n",
      "Epoch 36/50\n",
      "10984/10984 [==============================] - 1s 116us/step - loss: 0.0035 - acc: 0.9999\n",
      "Epoch 37/50\n",
      "10984/10984 [==============================] - 1s 116us/step - loss: 0.0034 - acc: 0.9999\n",
      "Epoch 38/50\n",
      "10984/10984 [==============================] - 1s 124us/step - loss: 0.0033 - acc: 0.9999\n",
      "Epoch 39/50\n",
      "10984/10984 [==============================] - 1s 124us/step - loss: 0.0032 - acc: 0.9999\n",
      "Epoch 40/50\n",
      "10984/10984 [==============================] - 1s 115us/step - loss: 0.0031 - acc: 0.9999\n",
      "Epoch 41/50\n",
      "10984/10984 [==============================] - 1s 115us/step - loss: 0.0031 - acc: 0.9999\n",
      "Epoch 42/50\n",
      "10984/10984 [==============================] - 1s 117us/step - loss: 0.0030 - acc: 0.9999\n",
      "Epoch 43/50\n",
      "10984/10984 [==============================] - 1s 117us/step - loss: 0.0029 - acc: 0.9999\n",
      "Epoch 44/50\n",
      "10984/10984 [==============================] - 1s 116us/step - loss: 0.0029 - acc: 0.9999\n",
      "Epoch 45/50\n",
      "10984/10984 [==============================] - 1s 116us/step - loss: 0.0028 - acc: 0.9999\n",
      "Epoch 46/50\n",
      "10984/10984 [==============================] - 1s 122us/step - loss: 0.0028 - acc: 0.9999\n",
      "Epoch 47/50\n",
      "10984/10984 [==============================] - 1s 126us/step - loss: 0.0027 - acc: 0.9999\n",
      "Epoch 48/50\n",
      "10984/10984 [==============================] - 1s 117us/step - loss: 0.0027 - acc: 0.9999\n",
      "Epoch 49/50\n",
      "10984/10984 [==============================] - 1s 119us/step - loss: 0.0026 - acc: 0.9999\n",
      "Epoch 50/50\n",
      "10984/10984 [==============================] - 1s 119us/step - loss: 0.0026 - acc: 0.9999\n",
      "\n",
      "AUGMENTING DATASET WITH BATCH # 2 :\n",
      "\n",
      "Labels accepted:  1892 / 2100\n",
      "\n",
      "Epoch 1/50\n",
      "12876/12876 [==============================] - 4s 347us/step - loss: 1.2609 - acc: 0.6216\n",
      "Epoch 2/50\n",
      "12876/12876 [==============================] - 1s 116us/step - loss: 0.5197 - acc: 0.8465\n",
      "Epoch 3/50\n",
      "12876/12876 [==============================] - 1s 116us/step - loss: 0.2734 - acc: 0.9156\n",
      "Epoch 4/50\n",
      "12876/12876 [==============================] - 2s 118us/step - loss: 0.1987 - acc: 0.9401\n",
      "Epoch 5/50\n",
      "12876/12876 [==============================] - 2s 117us/step - loss: 0.1548 - acc: 0.9532\n",
      "Epoch 6/50\n",
      "12876/12876 [==============================] - 2s 117us/step - loss: 0.1203 - acc: 0.9647\n",
      "Epoch 7/50\n",
      "12876/12876 [==============================] - 2s 117us/step - loss: 0.0974 - acc: 0.9733\n",
      "Epoch 8/50\n",
      "12876/12876 [==============================] - 2s 117us/step - loss: 0.0762 - acc: 0.9800\n",
      "Epoch 9/50\n",
      "12876/12876 [==============================] - 2s 117us/step - loss: 0.0601 - acc: 0.9859\n",
      "Epoch 10/50\n",
      "12876/12876 [==============================] - 2s 119us/step - loss: 0.0474 - acc: 0.9897\n",
      "Epoch 11/50\n",
      "12876/12876 [==============================] - 2s 117us/step - loss: 0.0370 - acc: 0.9937\n",
      "Epoch 12/50\n",
      "12876/12876 [==============================] - 2s 117us/step - loss: 0.0284 - acc: 0.9970\n",
      "Epoch 13/50\n",
      "12876/12876 [==============================] - 2s 117us/step - loss: 0.0225 - acc: 0.9978\n",
      "Epoch 14/50\n",
      "12876/12876 [==============================] - 2s 117us/step - loss: 0.0179 - acc: 0.9990\n",
      "Epoch 15/50\n",
      "12876/12876 [==============================] - 2s 117us/step - loss: 0.0143 - acc: 0.9996\n",
      "Epoch 16/50\n",
      "12876/12876 [==============================] - 2s 117us/step - loss: 0.0121 - acc: 0.9996\n",
      "Epoch 17/50\n",
      "12876/12876 [==============================] - 2s 117us/step - loss: 0.0101 - acc: 0.9998\n",
      "Epoch 18/50\n",
      "12876/12876 [==============================] - 2s 117us/step - loss: 0.0087 - acc: 0.9998\n",
      "Epoch 19/50\n",
      "12876/12876 [==============================] - 2s 117us/step - loss: 0.0076 - acc: 0.9999\n",
      "Epoch 20/50\n",
      "12876/12876 [==============================] - 2s 120us/step - loss: 0.0067 - acc: 0.9999\n",
      "Epoch 21/50\n",
      "12876/12876 [==============================] - 2s 117us/step - loss: 0.0061 - acc: 0.9999\n",
      "Epoch 22/50\n",
      "12876/12876 [==============================] - 2s 118us/step - loss: 0.0056 - acc: 0.9999\n",
      "Epoch 23/50\n",
      "12876/12876 [==============================] - 2s 118us/step - loss: 0.0052 - acc: 0.9999\n",
      "Epoch 24/50\n",
      "12876/12876 [==============================] - 2s 117us/step - loss: 0.0048 - acc: 0.9999\n",
      "Epoch 25/50\n",
      "12876/12876 [==============================] - 2s 122us/step - loss: 0.0045 - acc: 0.9999\n",
      "Epoch 26/50\n",
      "12876/12876 [==============================] - 2s 119us/step - loss: 0.0042 - acc: 0.9999\n",
      "Epoch 27/50\n",
      "12876/12876 [==============================] - 2s 127us/step - loss: 0.0040 - acc: 0.9999\n",
      "Epoch 28/50\n",
      "12876/12876 [==============================] - 2s 145us/step - loss: 0.0038 - acc: 0.9999\n",
      "Epoch 29/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12876/12876 [==============================] - 2s 142us/step - loss: 0.0036 - acc: 0.9999 0s - loss: 0.0040 - acc: 0.9\n",
      "Epoch 30/50\n",
      "12876/12876 [==============================] - 2s 119us/step - loss: 0.0035 - acc: 0.9999\n",
      "Epoch 31/50\n",
      "12876/12876 [==============================] - 2s 123us/step - loss: 0.0033 - acc: 0.9999\n",
      "Epoch 32/50\n",
      "12876/12876 [==============================] - 2s 142us/step - loss: 0.0032 - acc: 0.9999\n",
      "Epoch 33/50\n",
      "12876/12876 [==============================] - 2s 147us/step - loss: 0.0031 - acc: 0.9999\n",
      "Epoch 34/50\n",
      "12876/12876 [==============================] - 2s 119us/step - loss: 0.0030 - acc: 0.9999\n",
      "Epoch 35/50\n",
      "12876/12876 [==============================] - 2s 132us/step - loss: 0.0029 - acc: 0.9999\n",
      "Epoch 36/50\n",
      "12876/12876 [==============================] - 2s 145us/step - loss: 0.0028 - acc: 0.9999\n",
      "Epoch 37/50\n",
      "12876/12876 [==============================] - 2s 140us/step - loss: 0.0028 - acc: 0.9999\n",
      "Epoch 38/50\n",
      "12876/12876 [==============================] - 2s 132us/step - loss: 0.0027 - acc: 0.9999\n",
      "Epoch 39/50\n",
      "12876/12876 [==============================] - 2s 147us/step - loss: 0.0026 - acc: 0.9999\n",
      "Epoch 40/50\n",
      "12876/12876 [==============================] - 2s 144us/step - loss: 0.0026 - acc: 0.9999\n",
      "Epoch 41/50\n",
      "12876/12876 [==============================] - 2s 153us/step - loss: 0.0025 - acc: 0.9999\n",
      "Epoch 42/50\n",
      "12876/12876 [==============================] - 2s 149us/step - loss: 0.0025 - acc: 0.9999\n",
      "Epoch 43/50\n",
      "12876/12876 [==============================] - 2s 141us/step - loss: 0.0024 - acc: 0.9999\n",
      "Epoch 44/50\n",
      "12876/12876 [==============================] - 2s 131us/step - loss: 0.0024 - acc: 0.9999\n",
      "Epoch 45/50\n",
      "12876/12876 [==============================] - 2s 118us/step - loss: 0.0023 - acc: 0.9999\n",
      "Epoch 46/50\n",
      "12876/12876 [==============================] - 1s 115us/step - loss: 0.0023 - acc: 0.9999\n",
      "Epoch 47/50\n",
      "12876/12876 [==============================] - 2s 132us/step - loss: 0.0023 - acc: 0.9999\n",
      "Epoch 48/50\n",
      "12876/12876 [==============================] - 2s 120us/step - loss: 0.0022 - acc: 0.9999\n",
      "Epoch 49/50\n",
      "12876/12876 [==============================] - 1s 116us/step - loss: 0.0022 - acc: 0.9999\n",
      "Epoch 50/50\n",
      "12876/12876 [==============================] - 2s 117us/step - loss: 0.0022 - acc: 0.9999\n",
      "\n",
      "AUGMENTING DATASET WITH BATCH # 3 :\n",
      "\n",
      "Labels accepted:  1916 / 2100\n",
      "\n",
      "Epoch 1/50\n",
      "14792/14792 [==============================] - 5s 357us/step - loss: 1.1237 - acc: 0.6685\n",
      "Epoch 2/50\n",
      "14792/14792 [==============================] - 2s 150us/step - loss: 0.4498 - acc: 0.8607\n",
      "Epoch 3/50\n",
      "14792/14792 [==============================] - 2s 124us/step - loss: 0.2312 - acc: 0.9289\n",
      "Epoch 4/50\n",
      "14792/14792 [==============================] - 2s 115us/step - loss: 0.1636 - acc: 0.9521\n",
      "Epoch 5/50\n",
      "14792/14792 [==============================] - 2s 116us/step - loss: 0.1237 - acc: 0.9630\n",
      "Epoch 6/50\n",
      "14792/14792 [==============================] - 2s 115us/step - loss: 0.0966 - acc: 0.9721\n",
      "Epoch 7/50\n",
      "14792/14792 [==============================] - 2s 114us/step - loss: 0.0752 - acc: 0.9803\n",
      "Epoch 8/50\n",
      "14792/14792 [==============================] - 2s 114us/step - loss: 0.0561 - acc: 0.9876\n",
      "Epoch 9/50\n",
      "14792/14792 [==============================] - 2s 115us/step - loss: 0.0459 - acc: 0.9901\n",
      "Epoch 10/50\n",
      "14792/14792 [==============================] - 2s 114us/step - loss: 0.0340 - acc: 0.9945\n",
      "Epoch 11/50\n",
      "14792/14792 [==============================] - 2s 115us/step - loss: 0.0261 - acc: 0.9968\n",
      "Epoch 12/50\n",
      "14792/14792 [==============================] - 2s 115us/step - loss: 0.0204 - acc: 0.9983\n",
      "Epoch 13/50\n",
      "14792/14792 [==============================] - 2s 122us/step - loss: 0.0157 - acc: 0.9991\n",
      "Epoch 14/50\n",
      "14792/14792 [==============================] - 2s 131us/step - loss: 0.0129 - acc: 0.9995\n",
      "Epoch 15/50\n",
      "14792/14792 [==============================] - 2s 125us/step - loss: 0.0104 - acc: 0.9998\n",
      "Epoch 16/50\n",
      "14792/14792 [==============================] - 2s 115us/step - loss: 0.0089 - acc: 0.9998\n",
      "Epoch 17/50\n",
      "14792/14792 [==============================] - 2s 115us/step - loss: 0.0076 - acc: 0.9999\n",
      "Epoch 18/50\n",
      "14792/14792 [==============================] - 2s 115us/step - loss: 0.0067 - acc: 0.9999\n",
      "Epoch 19/50\n",
      "14792/14792 [==============================] - 2s 115us/step - loss: 0.0058 - acc: 0.9999\n",
      "Epoch 20/50\n",
      "14792/14792 [==============================] - 2s 117us/step - loss: 0.0052 - acc: 0.9999\n",
      "Epoch 21/50\n",
      "14792/14792 [==============================] - 2s 124us/step - loss: 0.0047 - acc: 0.9999\n",
      "Epoch 22/50\n",
      "14792/14792 [==============================] - 2s 116us/step - loss: 0.0044 - acc: 0.9999\n",
      "Epoch 23/50\n",
      "14792/14792 [==============================] - 2s 116us/step - loss: 0.0041 - acc: 0.9999\n",
      "Epoch 24/50\n",
      "14792/14792 [==============================] - 2s 116us/step - loss: 0.0038 - acc: 0.9999\n",
      "Epoch 25/50\n",
      "14792/14792 [==============================] - 2s 117us/step - loss: 0.0036 - acc: 0.9999\n",
      "Epoch 26/50\n",
      "14792/14792 [==============================] - 2s 137us/step - loss: 0.0034 - acc: 0.9999\n",
      "Epoch 27/50\n",
      "14792/14792 [==============================] - 2s 151us/step - loss: 0.0032 - acc: 0.9999\n",
      "Epoch 28/50\n",
      "14792/14792 [==============================] - 2s 130us/step - loss: 0.0031 - acc: 0.9999\n",
      "Epoch 29/50\n",
      "14792/14792 [==============================] - 2s 117us/step - loss: 0.0030 - acc: 0.9999\n",
      "Epoch 30/50\n",
      "14792/14792 [==============================] - 2s 115us/step - loss: 0.0029 - acc: 0.9999\n",
      "Epoch 31/50\n",
      "14792/14792 [==============================] - 2s 115us/step - loss: 0.0028 - acc: 0.9999\n",
      "Epoch 32/50\n",
      "14792/14792 [==============================] - 2s 121us/step - loss: 0.0027 - acc: 0.9999\n",
      "Epoch 33/50\n",
      "14792/14792 [==============================] - 2s 138us/step - loss: 0.0026 - acc: 0.9999\n",
      "Epoch 34/50\n",
      "14792/14792 [==============================] - 2s 138us/step - loss: 0.0025 - acc: 0.9999\n",
      "Epoch 35/50\n",
      "14792/14792 [==============================] - 2s 125us/step - loss: 0.0024 - acc: 0.9999\n",
      "Epoch 36/50\n",
      "14792/14792 [==============================] - 2s 117us/step - loss: 0.0024 - acc: 0.9999\n",
      "Epoch 37/50\n",
      "14792/14792 [==============================] - 2s 116us/step - loss: 0.0023 - acc: 0.9999\n",
      "Epoch 38/50\n",
      "14792/14792 [==============================] - 2s 116us/step - loss: 0.0023 - acc: 0.9999\n",
      "Epoch 39/50\n",
      "14792/14792 [==============================] - 2s 116us/step - loss: 0.0022 - acc: 0.9999\n",
      "Epoch 40/50\n",
      "14792/14792 [==============================] - 2s 118us/step - loss: 0.0022 - acc: 0.9999\n",
      "Epoch 41/50\n",
      "14792/14792 [==============================] - 2s 117us/step - loss: 0.0021 - acc: 0.9999\n",
      "Epoch 42/50\n",
      "14792/14792 [==============================] - 2s 115us/step - loss: 0.0021 - acc: 0.9999\n",
      "Epoch 43/50\n",
      "14792/14792 [==============================] - 2s 130us/step - loss: 0.0020 - acc: 0.9999\n",
      "Epoch 44/50\n",
      "14792/14792 [==============================] - 2s 122us/step - loss: 0.0020 - acc: 0.9999\n",
      "Epoch 45/50\n",
      "14792/14792 [==============================] - 2s 128us/step - loss: 0.0020 - acc: 0.9999\n",
      "Epoch 46/50\n",
      "14792/14792 [==============================] - 2s 119us/step - loss: 0.0019 - acc: 0.9999\n",
      "Epoch 47/50\n",
      "14792/14792 [==============================] - 2s 118us/step - loss: 0.0019 - acc: 0.9999\n",
      "Epoch 48/50\n",
      "14792/14792 [==============================] - 2s 118us/step - loss: 0.0019 - acc: 0.9999\n",
      "Epoch 49/50\n",
      "14792/14792 [==============================] - 2s 117us/step - loss: 0.0019 - acc: 0.9999\n",
      "Epoch 50/50\n",
      "14792/14792 [==============================] - 2s 118us/step - loss: 0.0018 - acc: 0.9999\n",
      "\n",
      "AUGMENTING DATASET WITH BATCH # 4 :\n",
      "\n",
      "Labels accepted:  1935 / 2100\n",
      "\n",
      "Epoch 1/50\n",
      "16727/16727 [==============================] - 6s 354us/step - loss: 1.1059 - acc: 0.6731\n",
      "Epoch 2/50\n",
      "16727/16727 [==============================] - 3s 152us/step - loss: 0.3838 - acc: 0.8825\n",
      "Epoch 3/50\n",
      "16727/16727 [==============================] - 2s 149us/step - loss: 0.2067 - acc: 0.9359\n",
      "Epoch 4/50\n",
      "16727/16727 [==============================] - 2s 119us/step - loss: 0.1439 - acc: 0.9568\n",
      "Epoch 5/50\n",
      "16727/16727 [==============================] - 2s 135us/step - loss: 0.1094 - acc: 0.9686\n",
      "Epoch 6/50\n",
      "16727/16727 [==============================] - 2s 131us/step - loss: 0.0826 - acc: 0.9764\n",
      "Epoch 7/50\n",
      "16727/16727 [==============================] - 2s 117us/step - loss: 0.0637 - acc: 0.9826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50\n",
      "16727/16727 [==============================] - 2s 134us/step - loss: 0.0498 - acc: 0.9876\n",
      "Epoch 9/50\n",
      "16727/16727 [==============================] - 2s 132us/step - loss: 0.0391 - acc: 0.9916\n",
      "Epoch 10/50\n",
      "16727/16727 [==============================] - 3s 175us/step - loss: 0.0291 - acc: 0.9950\n",
      "Epoch 11/50\n",
      "16727/16727 [==============================] - 2s 134us/step - loss: 0.0223 - acc: 0.9973\n",
      "Epoch 12/50\n",
      "16727/16727 [==============================] - 2s 118us/step - loss: 0.0169 - acc: 0.9986\n",
      "Epoch 13/50\n",
      "16727/16727 [==============================] - 3s 153us/step - loss: 0.0136 - acc: 0.9995\n",
      "Epoch 14/50\n",
      "16727/16727 [==============================] - 2s 128us/step - loss: 0.0107 - acc: 0.9997\n",
      "Epoch 15/50\n",
      "16727/16727 [==============================] - 2s 121us/step - loss: 0.0088 - acc: 0.9998\n",
      "Epoch 16/50\n",
      "16727/16727 [==============================] - 2s 121us/step - loss: 0.0073 - acc: 0.9999\n",
      "Epoch 17/50\n",
      "16727/16727 [==============================] - 2s 127us/step - loss: 0.0063 - acc: 0.9999\n",
      "Epoch 18/50\n",
      "16727/16727 [==============================] - 2s 118us/step - loss: 0.0056 - acc: 0.9998\n",
      "Epoch 19/50\n",
      "16727/16727 [==============================] - 2s 117us/step - loss: 0.0050 - acc: 0.9999\n",
      "Epoch 20/50\n",
      "16727/16727 [==============================] - 2s 118us/step - loss: 0.0045 - acc: 0.9999\n",
      "Epoch 21/50\n",
      "16727/16727 [==============================] - 2s 120us/step - loss: 0.0041 - acc: 0.9999\n",
      "Epoch 22/50\n",
      "16727/16727 [==============================] - 2s 117us/step - loss: 0.0039 - acc: 0.9999\n",
      "Epoch 23/50\n",
      "16727/16727 [==============================] - 2s 118us/step - loss: 0.0035 - acc: 0.9999\n",
      "Epoch 24/50\n",
      "16727/16727 [==============================] - 2s 117us/step - loss: 0.0033 - acc: 0.9999\n",
      "Epoch 25/50\n",
      "16727/16727 [==============================] - 2s 118us/step - loss: 0.0031 - acc: 0.9999\n",
      "Epoch 26/50\n",
      "16727/16727 [==============================] - 2s 118us/step - loss: 0.0030 - acc: 0.9999\n",
      "Epoch 27/50\n",
      "16727/16727 [==============================] - 2s 118us/step - loss: 0.0029 - acc: 0.9999\n",
      "Epoch 28/50\n",
      "16727/16727 [==============================] - 2s 119us/step - loss: 0.0027 - acc: 0.9999\n",
      "Epoch 29/50\n",
      "16727/16727 [==============================] - 2s 117us/step - loss: 0.0026 - acc: 0.9999\n",
      "Epoch 30/50\n",
      "16727/16727 [==============================] - 2s 117us/step - loss: 0.0025 - acc: 0.9999\n",
      "Epoch 31/50\n",
      "16727/16727 [==============================] - 2s 118us/step - loss: 0.0024 - acc: 0.9999\n",
      "Epoch 32/50\n",
      "16727/16727 [==============================] - 2s 117us/step - loss: 0.0023 - acc: 0.9999\n",
      "Epoch 33/50\n",
      "16727/16727 [==============================] - 2s 117us/step - loss: 0.0023 - acc: 0.9999\n",
      "Epoch 34/50\n",
      "16727/16727 [==============================] - 2s 118us/step - loss: 0.0022 - acc: 0.9999\n",
      "Epoch 35/50\n",
      "16727/16727 [==============================] - 2s 117us/step - loss: 0.0021 - acc: 0.9999\n",
      "Epoch 36/50\n",
      "16727/16727 [==============================] - 2s 118us/step - loss: 0.0021 - acc: 0.9999\n",
      "Epoch 37/50\n",
      "16727/16727 [==============================] - 2s 117us/step - loss: 0.0020 - acc: 0.9999\n",
      "Epoch 38/50\n",
      "16727/16727 [==============================] - 2s 117us/step - loss: 0.0020 - acc: 0.9999\n",
      "Epoch 39/50\n",
      "16727/16727 [==============================] - 2s 118us/step - loss: 0.0020 - acc: 0.9999\n",
      "Epoch 40/50\n",
      "16727/16727 [==============================] - 2s 117us/step - loss: 0.0019 - acc: 0.9999\n",
      "Epoch 41/50\n",
      "16727/16727 [==============================] - 2s 123us/step - loss: 0.0019 - acc: 0.9999\n",
      "Epoch 42/50\n",
      "16727/16727 [==============================] - 2s 122us/step - loss: 0.0018 - acc: 0.9999\n",
      "Epoch 43/50\n",
      "16727/16727 [==============================] - 2s 125us/step - loss: 0.0018 - acc: 0.9999\n",
      "Epoch 44/50\n",
      "16727/16727 [==============================] - 2s 119us/step - loss: 0.0018 - acc: 0.9999\n",
      "Epoch 45/50\n",
      "16727/16727 [==============================] - 2s 119us/step - loss: 0.0018 - acc: 0.9999\n",
      "Epoch 46/50\n",
      "16727/16727 [==============================] - 2s 148us/step - loss: 0.0017 - acc: 0.9999\n",
      "Epoch 47/50\n",
      "16727/16727 [==============================] - 2s 139us/step - loss: 0.0017 - acc: 0.9999\n",
      "Epoch 48/50\n",
      "16727/16727 [==============================] - 3s 195us/step - loss: 0.0017 - acc: 0.9999\n",
      "Epoch 49/50\n",
      "16727/16727 [==============================] - 2s 125us/step - loss: 0.0017 - acc: 0.9999\n",
      "Epoch 50/50\n",
      "16727/16727 [==============================] - 2s 149us/step - loss: 0.0016 - acc: 0.9999\n",
      "\n",
      "AUGMENTING DATASET WITH BATCH # 5 :\n",
      "\n",
      "Labels accepted:  1905 / 2100\n",
      "\n",
      "Epoch 1/50\n",
      "18632/18632 [==============================] - 6s 337us/step - loss: 1.0526 - acc: 0.6916\n",
      "Epoch 2/50\n",
      "18632/18632 [==============================] - 3s 144us/step - loss: 0.3014 - acc: 0.9058\n",
      "Epoch 3/50\n",
      "18632/18632 [==============================] - 2s 131us/step - loss: 0.1784 - acc: 0.9471\n",
      "Epoch 4/50\n",
      "18632/18632 [==============================] - 3s 149us/step - loss: 0.1247 - acc: 0.9638\n",
      "Epoch 5/50\n",
      "18632/18632 [==============================] - 3s 137us/step - loss: 0.0931 - acc: 0.9739\n",
      "Epoch 6/50\n",
      "18632/18632 [==============================] - 2s 132us/step - loss: 0.0715 - acc: 0.9808\n",
      "Epoch 7/50\n",
      "18632/18632 [==============================] - 2s 124us/step - loss: 0.0552 - acc: 0.9860\n",
      "Epoch 8/50\n",
      "18632/18632 [==============================] - 3s 146us/step - loss: 0.0426 - acc: 0.9904\n",
      "Epoch 9/50\n",
      "18632/18632 [==============================] - 3s 146us/step - loss: 0.0332 - acc: 0.9929\n",
      "Epoch 10/50\n",
      "18632/18632 [==============================] - 3s 155us/step - loss: 0.0250 - acc: 0.9961\n",
      "Epoch 11/50\n",
      "18632/18632 [==============================] - 3s 148us/step - loss: 0.0191 - acc: 0.9976\n",
      "Epoch 12/50\n",
      "18632/18632 [==============================] - 3s 151us/step - loss: 0.0150 - acc: 0.9988\n",
      "Epoch 13/50\n",
      "18632/18632 [==============================] - 3s 136us/step - loss: 0.0116 - acc: 0.9991\n",
      "Epoch 14/50\n",
      "18632/18632 [==============================] - 3s 147us/step - loss: 0.0091 - acc: 0.9997\n",
      "Epoch 15/50\n",
      "18632/18632 [==============================] - 2s 126us/step - loss: 0.0074 - acc: 0.9998\n",
      "Epoch 16/50\n",
      "18632/18632 [==============================] - 2s 116us/step - loss: 0.0063 - acc: 0.9998\n",
      "Epoch 17/50\n",
      "18632/18632 [==============================] - 2s 116us/step - loss: 0.0055 - acc: 0.9998\n",
      "Epoch 18/50\n",
      "18632/18632 [==============================] - 2s 117us/step - loss: 0.0048 - acc: 0.9999\n",
      "Epoch 19/50\n",
      "18632/18632 [==============================] - 2s 116us/step - loss: 0.0043 - acc: 0.9999\n",
      "Epoch 20/50\n",
      "18632/18632 [==============================] - 2s 117us/step - loss: 0.0039 - acc: 0.9999\n",
      "Epoch 21/50\n",
      "18632/18632 [==============================] - 2s 115us/step - loss: 0.0036 - acc: 0.9999\n",
      "Epoch 22/50\n",
      "18632/18632 [==============================] - 2s 116us/step - loss: 0.0033 - acc: 0.9999\n",
      "Epoch 23/50\n",
      "18632/18632 [==============================] - 2s 115us/step - loss: 0.0031 - acc: 0.9999\n",
      "Epoch 24/50\n",
      "18632/18632 [==============================] - 2s 115us/step - loss: 0.0029 - acc: 0.9999\n",
      "Epoch 25/50\n",
      "18632/18632 [==============================] - 2s 117us/step - loss: 0.0027 - acc: 0.9999\n",
      "Epoch 26/50\n",
      "18632/18632 [==============================] - 2s 116us/step - loss: 0.0026 - acc: 0.9999\n",
      "Epoch 27/50\n",
      "18632/18632 [==============================] - 3s 139us/step - loss: 0.0025 - acc: 0.9999\n",
      "Epoch 28/50\n",
      "18632/18632 [==============================] - 2s 131us/step - loss: 0.0024 - acc: 0.9999\n",
      "Epoch 29/50\n",
      "18632/18632 [==============================] - 3s 135us/step - loss: 0.0023 - acc: 0.9999\n",
      "Epoch 30/50\n",
      "18632/18632 [==============================] - 3s 138us/step - loss: 0.0022 - acc: 0.9999\n",
      "Epoch 31/50\n",
      "18632/18632 [==============================] - 2s 115us/step - loss: 0.0021 - acc: 0.9999\n",
      "Epoch 32/50\n",
      "18632/18632 [==============================] - 2s 119us/step - loss: 0.0020 - acc: 0.9999\n",
      "Epoch 33/50\n",
      "18632/18632 [==============================] - 3s 145us/step - loss: 0.0020 - acc: 0.9999\n",
      "Epoch 34/50\n",
      "18632/18632 [==============================] - 2s 125us/step - loss: 0.0019 - acc: 0.9999\n",
      "Epoch 35/50\n",
      "18632/18632 [==============================] - 2s 116us/step - loss: 0.0019 - acc: 0.9999\n",
      "Epoch 36/50\n",
      "18632/18632 [==============================] - 2s 115us/step - loss: 0.0018 - acc: 0.9999\n",
      "Epoch 37/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18632/18632 [==============================] - 2s 114us/step - loss: 0.0018 - acc: 0.9999\n",
      "Epoch 38/50\n",
      "18632/18632 [==============================] - 2s 113us/step - loss: 0.0018 - acc: 0.9999\n",
      "Epoch 39/50\n",
      "18632/18632 [==============================] - 2s 114us/step - loss: 0.0017 - acc: 0.9999\n",
      "Epoch 40/50\n",
      "18632/18632 [==============================] - 2s 114us/step - loss: 0.0017 - acc: 0.9999\n",
      "Epoch 41/50\n",
      "18632/18632 [==============================] - 2s 115us/step - loss: 0.0017 - acc: 0.9999\n",
      "Epoch 42/50\n",
      "18632/18632 [==============================] - 2s 114us/step - loss: 0.0016 - acc: 0.9999\n",
      "Epoch 43/50\n",
      "18632/18632 [==============================] - 2s 115us/step - loss: 0.0016 - acc: 0.9999\n",
      "Epoch 44/50\n",
      "18632/18632 [==============================] - 2s 118us/step - loss: 0.0016 - acc: 0.9999\n",
      "Epoch 45/50\n",
      "18632/18632 [==============================] - 2s 116us/step - loss: 0.0015 - acc: 0.9999\n",
      "Epoch 46/50\n",
      "18632/18632 [==============================] - 2s 114us/step - loss: 0.0015 - acc: 0.9999\n",
      "Epoch 47/50\n",
      "18632/18632 [==============================] - 2s 114us/step - loss: 0.0015 - acc: 0.9999\n",
      "Epoch 48/50\n",
      "18632/18632 [==============================] - 2s 114us/step - loss: 0.0015 - acc: 0.9999\n",
      "Epoch 49/50\n",
      "18632/18632 [==============================] - 2s 115us/step - loss: 0.0015 - acc: 0.9999\n",
      "Epoch 50/50\n",
      "18632/18632 [==============================] - 2s 114us/step - loss: 0.0014 - acc: 0.9999\n",
      "\n",
      "AUGMENTING DATASET WITH BATCH # 6 :\n",
      "\n",
      "Labels accepted:  1955 / 2100\n",
      "\n",
      "Epoch 1/50\n",
      "20587/20587 [==============================] - 5s 258us/step - loss: 0.9551 - acc: 0.7200\n",
      "Epoch 2/50\n",
      "20587/20587 [==============================] - 2s 113us/step - loss: 0.2549 - acc: 0.9209\n",
      "Epoch 3/50\n",
      "20587/20587 [==============================] - 2s 112us/step - loss: 0.1523 - acc: 0.9541\n",
      "Epoch 4/50\n",
      "20587/20587 [==============================] - 2s 112us/step - loss: 0.1057 - acc: 0.9687\n",
      "Epoch 5/50\n",
      "20587/20587 [==============================] - 2s 113us/step - loss: 0.0800 - acc: 0.9769\n",
      "Epoch 6/50\n",
      "20587/20587 [==============================] - 2s 113us/step - loss: 0.0602 - acc: 0.9834\n",
      "Epoch 7/50\n",
      "20587/20587 [==============================] - 2s 115us/step - loss: 0.0444 - acc: 0.9897\n",
      "Epoch 8/50\n",
      "20587/20587 [==============================] - 2s 113us/step - loss: 0.0344 - acc: 0.9925\n",
      "Epoch 9/50\n",
      "20587/20587 [==============================] - 2s 113us/step - loss: 0.0255 - acc: 0.9959\n",
      "Epoch 10/50\n",
      "20587/20587 [==============================] - 2s 114us/step - loss: 0.0195 - acc: 0.9975\n",
      "Epoch 11/50\n",
      "20587/20587 [==============================] - 2s 113us/step - loss: 0.0146 - acc: 0.9987\n",
      "Epoch 12/50\n",
      "20587/20587 [==============================] - 2s 113us/step - loss: 0.0118 - acc: 0.9991\n",
      "Epoch 13/50\n",
      "20587/20587 [==============================] - 2s 113us/step - loss: 0.0091 - acc: 0.9997\n",
      "Epoch 14/50\n",
      "20587/20587 [==============================] - 2s 113us/step - loss: 0.0073 - acc: 0.9998\n",
      "Epoch 15/50\n",
      "20587/20587 [==============================] - 2s 114us/step - loss: 0.0062 - acc: 0.9998\n",
      "Epoch 16/50\n",
      "20587/20587 [==============================] - 2s 113us/step - loss: 0.0051 - acc: 0.9999\n",
      "Epoch 17/50\n",
      "20587/20587 [==============================] - 2s 113us/step - loss: 0.0044 - acc: 0.9999\n",
      "Epoch 18/50\n",
      "20587/20587 [==============================] - 2s 114us/step - loss: 0.0040 - acc: 1.0000\n",
      "Epoch 19/50\n",
      "20587/20587 [==============================] - 2s 113us/step - loss: 0.0037 - acc: 0.9999\n",
      "Epoch 20/50\n",
      "20587/20587 [==============================] - 2s 113us/step - loss: 0.0033 - acc: 1.0000\n",
      "Epoch 21/50\n",
      "20587/20587 [==============================] - 2s 114us/step - loss: 0.0031 - acc: 1.0000\n",
      "Epoch 22/50\n",
      "20587/20587 [==============================] - 2s 113us/step - loss: 0.0028 - acc: 1.0000\n",
      "Epoch 23/50\n",
      "20587/20587 [==============================] - 2s 115us/step - loss: 0.0027 - acc: 1.0000\n",
      "Epoch 24/50\n",
      "20587/20587 [==============================] - 2s 114us/step - loss: 0.0025 - acc: 1.0000\n",
      "Epoch 25/50\n",
      "20587/20587 [==============================] - 2s 113us/step - loss: 0.0024 - acc: 1.0000\n",
      "Epoch 26/50\n",
      "20587/20587 [==============================] - 2s 113us/step - loss: 0.0023 - acc: 1.0000\n",
      "Epoch 27/50\n",
      "20587/20587 [==============================] - 2s 114us/step - loss: 0.0022 - acc: 1.0000\n",
      "Epoch 28/50\n",
      "20587/20587 [==============================] - 2s 116us/step - loss: 0.0021 - acc: 1.0000\n",
      "Epoch 29/50\n",
      "20587/20587 [==============================] - 2s 113us/step - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 30/50\n",
      "20587/20587 [==============================] - 2s 113us/step - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 31/50\n",
      "20587/20587 [==============================] - 2s 113us/step - loss: 0.0019 - acc: 1.0000\n",
      "Epoch 32/50\n",
      "20587/20587 [==============================] - 2s 113us/step - loss: 0.0018 - acc: 1.0000\n",
      "Epoch 33/50\n",
      "20587/20587 [==============================] - 3s 134us/step - loss: 0.0018 - acc: 1.0000\n",
      "Epoch 34/50\n",
      "20587/20587 [==============================] - 3s 146us/step - loss: 0.0017 - acc: 1.0000\n",
      "Epoch 35/50\n",
      "20587/20587 [==============================] - 2s 121us/step - loss: 0.0017 - acc: 1.0000\n",
      "Epoch 36/50\n",
      "20587/20587 [==============================] - 2s 115us/step - loss: 0.0016 - acc: 1.0000\n",
      "Epoch 37/50\n",
      "20587/20587 [==============================] - 3s 136us/step - loss: 0.0016 - acc: 1.0000\n",
      "Epoch 38/50\n",
      "20587/20587 [==============================] - 3s 132us/step - loss: 0.0016 - acc: 1.0000ETA: 3s - loss: 8.126\n",
      "Epoch 39/50\n",
      "20587/20587 [==============================] - 3s 141us/step - loss: 0.0015 - acc: 1.0000\n",
      "Epoch 40/50\n",
      "20587/20587 [==============================] - 3s 134us/step - loss: 0.0015 - acc: 1.0000\n",
      "Epoch 41/50\n",
      "20587/20587 [==============================] - 3s 131us/step - loss: 0.0015 - acc: 1.0000\n",
      "Epoch 42/50\n",
      "20587/20587 [==============================] - 3s 131us/step - loss: 0.0015 - acc: 1.0000\n",
      "Epoch 43/50\n",
      "20587/20587 [==============================] - 3s 166us/step - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 44/50\n",
      "20587/20587 [==============================] - 3s 165us/step - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 45/50\n",
      "20587/20587 [==============================] - 3s 146us/step - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 46/50\n",
      "20587/20587 [==============================] - 3s 140us/step - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 47/50\n",
      "20587/20587 [==============================] - 3s 126us/step - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 48/50\n",
      "20587/20587 [==============================] - 3s 136us/step - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 49/50\n",
      "20587/20587 [==============================] - 3s 131us/step - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 50/50\n",
      "20587/20587 [==============================] - 2s 114us/step - loss: 0.0013 - acc: 1.0000\n",
      "\n",
      "AUGMENTING DATASET WITH BATCH # 7 :\n",
      "\n",
      "Labels accepted:  1941 / 2100\n",
      "\n",
      "Epoch 1/50\n",
      "22528/22528 [==============================] - 7s 315us/step - loss: 0.9314 - acc: 0.7334\n",
      "Epoch 2/50\n",
      "22528/22528 [==============================] - 4s 192us/step - loss: 0.2370 - acc: 0.9264\n",
      "Epoch 3/50\n",
      "22528/22528 [==============================] - 3s 153us/step - loss: 0.1414 - acc: 0.9569\n",
      "Epoch 4/50\n",
      "22528/22528 [==============================] - 3s 118us/step - loss: 0.0987 - acc: 0.9711\n",
      "Epoch 5/50\n",
      "22528/22528 [==============================] - 3s 120us/step - loss: 0.0757 - acc: 0.9794\n",
      "Epoch 6/50\n",
      "22528/22528 [==============================] - 3s 129us/step - loss: 0.0558 - acc: 0.9864\n",
      "Epoch 7/50\n",
      "22528/22528 [==============================] - 3s 121us/step - loss: 0.0427 - acc: 0.9895\n",
      "Epoch 8/50\n",
      "22528/22528 [==============================] - 3s 118us/step - loss: 0.0326 - acc: 0.9930\n",
      "Epoch 9/50\n",
      "22528/22528 [==============================] - 3s 115us/step - loss: 0.0252 - acc: 0.9953\n",
      "Epoch 10/50\n",
      "22528/22528 [==============================] - 3s 125us/step - loss: 0.0185 - acc: 0.9973\n",
      "Epoch 11/50\n",
      "22528/22528 [==============================] - 3s 117us/step - loss: 0.0139 - acc: 0.9986\n",
      "Epoch 12/50\n",
      "22528/22528 [==============================] - 3s 129us/step - loss: 0.0104 - acc: 0.9992\n",
      "Epoch 13/50\n",
      "22528/22528 [==============================] - 3s 121us/step - loss: 0.0085 - acc: 0.9995\n",
      "Epoch 14/50\n",
      "22528/22528 [==============================] - 3s 116us/step - loss: 0.0067 - acc: 0.9998\n",
      "Epoch 15/50\n",
      "22528/22528 [==============================] - 3s 141us/step - loss: 0.0055 - acc: 0.9999\n",
      "Epoch 16/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22528/22528 [==============================] - 3s 136us/step - loss: 0.0047 - acc: 1.0000\n",
      "Epoch 17/50\n",
      "22528/22528 [==============================] - 3s 116us/step - loss: 0.0041 - acc: 0.9999\n",
      "Epoch 18/50\n",
      "22528/22528 [==============================] - 3s 130us/step - loss: 0.0036 - acc: 1.0000\n",
      "Epoch 19/50\n",
      "22528/22528 [==============================] - 3s 148us/step - loss: 0.0033 - acc: 1.0000\n",
      "Epoch 20/50\n",
      "22528/22528 [==============================] - 3s 131us/step - loss: 0.0030 - acc: 1.0000\n",
      "Epoch 21/50\n",
      "22528/22528 [==============================] - ETA: 0s - loss: 0.0028 - acc: 1.000 - 3s 118us/step - loss: 0.0028 - acc: 1.0000\n",
      "Epoch 22/50\n",
      "22528/22528 [==============================] - 4s 159us/step - loss: 0.0026 - acc: 1.0000\n",
      "Epoch 23/50\n",
      "22528/22528 [==============================] - 4s 157us/step - loss: 0.0024 - acc: 1.0000 1s - loss: 0.0018 - acc:\n",
      "Epoch 24/50\n",
      "22528/22528 [==============================] - 4s 162us/step - loss: 0.0023 - acc: 1.0000\n",
      "Epoch 25/50\n",
      "22528/22528 [==============================] - 3s 144us/step - loss: 0.0022 - acc: 1.0000\n",
      "Epoch 26/50\n",
      "22528/22528 [==============================] - 3s 139us/step - loss: 0.0021 - acc: 1.0000\n",
      "Epoch 27/50\n",
      "22528/22528 [==============================] - 3s 134us/step - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 28/50\n",
      "22528/22528 [==============================] - 3s 140us/step - loss: 0.0019 - acc: 1.0000\n",
      "Epoch 29/50\n",
      "22528/22528 [==============================] - 3s 130us/step - loss: 0.0018 - acc: 1.0000\n",
      "Epoch 30/50\n",
      "22528/22528 [==============================] - 3s 127us/step - loss: 0.0018 - acc: 1.0000\n",
      "Epoch 31/50\n",
      "22528/22528 [==============================] - 3s 148us/step - loss: 0.0017 - acc: 1.0000\n",
      "Epoch 32/50\n",
      "22528/22528 [==============================] - 3s 119us/step - loss: 0.0017 - acc: 1.0000\n",
      "Epoch 33/50\n",
      "22528/22528 [==============================] - 3s 116us/step - loss: 0.0016 - acc: 1.0000\n",
      "Epoch 34/50\n",
      "22528/22528 [==============================] - 3s 116us/step - loss: 0.0016 - acc: 1.0000\n",
      "Epoch 35/50\n",
      "22528/22528 [==============================] - 3s 114us/step - loss: 0.0015 - acc: 1.0000\n",
      "Epoch 36/50\n",
      "22528/22528 [==============================] - 3s 115us/step - loss: 0.0015 - acc: 1.0000\n",
      "Epoch 37/50\n",
      "22528/22528 [==============================] - 3s 116us/step - loss: 0.0015 - acc: 1.0000\n",
      "Epoch 38/50\n",
      "22528/22528 [==============================] - 3s 141us/step - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 39/50\n",
      "22528/22528 [==============================] - 3s 144us/step - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 40/50\n",
      "22528/22528 [==============================] - 3s 126us/step - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 41/50\n",
      "22528/22528 [==============================] - 3s 144us/step - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 42/50\n",
      "22528/22528 [==============================] - 3s 134us/step - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 43/50\n",
      "22528/22528 [==============================] - 3s 135us/step - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 44/50\n",
      "22528/22528 [==============================] - 3s 118us/step - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 45/50\n",
      "22528/22528 [==============================] - 3s 124us/step - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 46/50\n",
      "22528/22528 [==============================] - 3s 118us/step - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 47/50\n",
      "22528/22528 [==============================] - 3s 116us/step - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 48/50\n",
      "22528/22528 [==============================] - 3s 116us/step - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 49/50\n",
      "22528/22528 [==============================] - 3s 118us/step - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 50/50\n",
      "22528/22528 [==============================] - 3s 116us/step - loss: 0.0012 - acc: 1.0000\n",
      "\n",
      "AUGMENTING DATASET WITH BATCH # 8 :\n",
      "\n",
      "Labels accepted:  1924 / 2100\n",
      "\n",
      "Epoch 1/50\n",
      "24452/24452 [==============================] - 6s 242us/step - loss: 0.8534 - acc: 0.7431\n",
      "Epoch 2/50\n",
      "24452/24452 [==============================] - 3s 114us/step - loss: 0.2106 - acc: 0.9352\n",
      "Epoch 3/50\n",
      "24452/24452 [==============================] - 3s 114us/step - loss: 0.1266 - acc: 0.9619\n",
      "Epoch 4/50\n",
      "24452/24452 [==============================] - 3s 114us/step - loss: 0.0894 - acc: 0.9736\n",
      "Epoch 5/50\n",
      "24452/24452 [==============================] - 3s 116us/step - loss: 0.0671 - acc: 0.9811\n",
      "Epoch 6/50\n",
      "24452/24452 [==============================] - 3s 114us/step - loss: 0.0502 - acc: 0.9872\n",
      "Epoch 7/50\n",
      "24452/24452 [==============================] - 3s 115us/step - loss: 0.0377 - acc: 0.9906\n",
      "Epoch 8/50\n",
      "24452/24452 [==============================] - 3s 117us/step - loss: 0.0278 - acc: 0.9947\n",
      "Epoch 9/50\n",
      "24452/24452 [==============================] - 3s 115us/step - loss: 0.0209 - acc: 0.9962\n",
      "Epoch 10/50\n",
      "24452/24452 [==============================] - 3s 115us/step - loss: 0.0159 - acc: 0.9980\n",
      "Epoch 11/50\n",
      "24452/24452 [==============================] - 3s 115us/step - loss: 0.0119 - acc: 0.9987\n",
      "Epoch 12/50\n",
      "24452/24452 [==============================] - 3s 116us/step - loss: 0.0090 - acc: 0.9996\n",
      "Epoch 13/50\n",
      "24452/24452 [==============================] - 3s 114us/step - loss: 0.0070 - acc: 0.9997\n",
      "Epoch 14/50\n",
      "24452/24452 [==============================] - 3s 114us/step - loss: 0.0056 - acc: 0.9999\n",
      "Epoch 15/50\n",
      "24452/24452 [==============================] - 3s 114us/step - loss: 0.0047 - acc: 0.9999\n",
      "Epoch 16/50\n",
      "24452/24452 [==============================] - 3s 115us/step - loss: 0.0041 - acc: 0.9999\n",
      "Epoch 17/50\n",
      "24452/24452 [==============================] - 3s 114us/step - loss: 0.0036 - acc: 0.9999\n",
      "Epoch 18/50\n",
      "24452/24452 [==============================] - 3s 116us/step - loss: 0.0032 - acc: 1.0000\n",
      "Epoch 19/50\n",
      "24452/24452 [==============================] - 3s 114us/step - loss: 0.0029 - acc: 1.0000\n",
      "Epoch 20/50\n",
      "24452/24452 [==============================] - 3s 114us/step - loss: 0.0027 - acc: 1.0000\n",
      "Epoch 21/50\n",
      "24452/24452 [==============================] - 3s 117us/step - loss: 0.0024 - acc: 1.0000\n",
      "Epoch 22/50\n",
      "24452/24452 [==============================] - 3s 114us/step - loss: 0.0023 - acc: 1.0000\n",
      "Epoch 23/50\n",
      "24452/24452 [==============================] - 3s 114us/step - loss: 0.0022 - acc: 1.0000\n",
      "Epoch 24/50\n",
      "24452/24452 [==============================] - 3s 114us/step - loss: 0.0021 - acc: 1.0000\n",
      "Epoch 25/50\n",
      "24452/24452 [==============================] - 3s 114us/step - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 26/50\n",
      "24452/24452 [==============================] - 3s 114us/step - loss: 0.0019 - acc: 1.0000\n",
      "Epoch 27/50\n",
      "24452/24452 [==============================] - 3s 115us/step - loss: 0.0018 - acc: 1.0000\n",
      "Epoch 28/50\n",
      "24452/24452 [==============================] - 3s 115us/step - loss: 0.0017 - acc: 1.0000\n",
      "Epoch 29/50\n",
      "24452/24452 [==============================] - 3s 115us/step - loss: 0.0016 - acc: 1.0000\n",
      "Epoch 30/50\n",
      "24452/24452 [==============================] - 3s 127us/step - loss: 0.0016 - acc: 1.0000\n",
      "Epoch 31/50\n",
      "24452/24452 [==============================] - 4s 143us/step - loss: 0.0015 - acc: 1.0000\n",
      "Epoch 32/50\n",
      "24452/24452 [==============================] - 3s 135us/step - loss: 0.0015 - acc: 1.0000\n",
      "Epoch 33/50\n",
      "24452/24452 [==============================] - 4s 146us/step - loss: 0.0015 - acc: 1.0000\n",
      "Epoch 34/50\n",
      "24452/24452 [==============================] - 4s 146us/step - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 35/50\n",
      "24452/24452 [==============================] - 3s 141us/step - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 36/50\n",
      "24452/24452 [==============================] - 3s 122us/step - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 37/50\n",
      "24452/24452 [==============================] - 4s 144us/step - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 38/50\n",
      "24452/24452 [==============================] - 3s 138us/step - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 39/50\n",
      "24452/24452 [==============================] - 3s 129us/step - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 40/50\n",
      "24452/24452 [==============================] - 3s 129us/step - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 41/50\n",
      "24452/24452 [==============================] - 3s 140us/step - loss: 0.0012 - acc: 1.0000ETA: 2s - loss: 5.2582e\n",
      "Epoch 42/50\n",
      "24452/24452 [==============================] - 3s 123us/step - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 43/50\n",
      "24452/24452 [==============================] - 4s 151us/step - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 44/50\n",
      "24452/24452 [==============================] - 3s 116us/step - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 45/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24452/24452 [==============================] - 3s 114us/step - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 46/50\n",
      "24452/24452 [==============================] - 3s 133us/step - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 47/50\n",
      "24452/24452 [==============================] - 4s 151us/step - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 48/50\n",
      "24452/24452 [==============================] - 3s 134us/step - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 49/50\n",
      "24452/24452 [==============================] - 3s 129us/step - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 50/50\n",
      "24452/24452 [==============================] - 4s 147us/step - loss: 0.0011 - acc: 1.0000\n",
      "\n",
      "AUGMENTING DATASET WITH BATCH # 9 :\n",
      "\n",
      "Labels accepted:  1915 / 2100\n",
      "\n",
      "Epoch 1/50\n",
      "26367/26367 [==============================] - 6s 243us/step - loss: 0.8561 - acc: 0.7466\n",
      "Epoch 2/50\n",
      "26367/26367 [==============================] - 3s 115us/step - loss: 0.2037 - acc: 0.9383\n",
      "Epoch 3/50\n",
      "26367/26367 [==============================] - 3s 115us/step - loss: 0.1193 - acc: 0.9644\n",
      "Epoch 4/50\n",
      "26367/26367 [==============================] - 3s 115us/step - loss: 0.0833 - acc: 0.9757\n",
      "Epoch 5/50\n",
      "26367/26367 [==============================] - 3s 116us/step - loss: 0.0615 - acc: 0.9834\n",
      "Epoch 6/50\n",
      "26367/26367 [==============================] - 3s 114us/step - loss: 0.0451 - acc: 0.9879\n",
      "Epoch 7/50\n",
      "26367/26367 [==============================] - 3s 116us/step - loss: 0.0338 - acc: 0.9916\n",
      "Epoch 8/50\n",
      "26367/26367 [==============================] - 3s 117us/step - loss: 0.0255 - acc: 0.9948\n",
      "Epoch 9/50\n",
      "26367/26367 [==============================] - 3s 115us/step - loss: 0.0197 - acc: 0.9964\n",
      "Epoch 10/50\n",
      "26367/26367 [==============================] - 3s 115us/step - loss: 0.0145 - acc: 0.9982\n",
      "Epoch 11/50\n",
      "26367/26367 [==============================] - 3s 115us/step - loss: 0.0108 - acc: 0.9990\n",
      "Epoch 12/50\n",
      "26367/26367 [==============================] - 3s 114us/step - loss: 0.0086 - acc: 0.9994\n",
      "Epoch 13/50\n",
      "26367/26367 [==============================] - 3s 115us/step - loss: 0.0066 - acc: 0.9997\n",
      "Epoch 14/50\n",
      "26367/26367 [==============================] - 3s 116us/step - loss: 0.0055 - acc: 0.9998\n",
      "Epoch 15/50\n",
      "26367/26367 [==============================] - 3s 117us/step - loss: 0.0044 - acc: 0.9998\n",
      "Epoch 16/50\n",
      "26367/26367 [==============================] - 3s 115us/step - loss: 0.0038 - acc: 0.9999\n",
      "Epoch 17/50\n",
      "26367/26367 [==============================] - 3s 114us/step - loss: 0.0034 - acc: 0.9999\n",
      "Epoch 18/50\n",
      "26367/26367 [==============================] - 3s 114us/step - loss: 0.0029 - acc: 1.0000\n",
      "Epoch 19/50\n",
      "26367/26367 [==============================] - 3s 113us/step - loss: 0.0026 - acc: 1.0000\n",
      "Epoch 20/50\n",
      "26367/26367 [==============================] - 3s 114us/step - loss: 0.0024 - acc: 0.9999\n",
      "Epoch 21/50\n",
      "26367/26367 [==============================] - 3s 114us/step - loss: 0.0023 - acc: 1.0000\n",
      "Epoch 22/50\n",
      "26367/26367 [==============================] - 3s 113us/step - loss: 0.0021 - acc: 1.0000\n",
      "Epoch 23/50\n",
      "26367/26367 [==============================] - 3s 113us/step - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 24/50\n",
      "26367/26367 [==============================] - 3s 114us/step - loss: 0.0019 - acc: 1.0000\n",
      "Epoch 25/50\n",
      "26367/26367 [==============================] - 4s 148us/step - loss: 0.0018 - acc: 1.0000\n",
      "Epoch 26/50\n",
      "26367/26367 [==============================] - 4s 143us/step - loss: 0.0017 - acc: 1.0000\n",
      "Epoch 27/50\n",
      "26367/26367 [==============================] - 4s 136us/step - loss: 0.0016 - acc: 1.0000\n",
      "Epoch 28/50\n",
      "26367/26367 [==============================] - 3s 131us/step - loss: 0.0016 - acc: 1.0000\n",
      "Epoch 29/50\n",
      "26367/26367 [==============================] - 4s 133us/step - loss: 0.0015 - acc: 1.0000\n",
      "Epoch 30/50\n",
      "26367/26367 [==============================] - 3s 115us/step - loss: 0.0015 - acc: 1.0000\n",
      "Epoch 31/50\n",
      "26367/26367 [==============================] - 3s 115us/step - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 32/50\n",
      "26367/26367 [==============================] - 3s 114us/step - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 33/50\n",
      "26367/26367 [==============================] - 3s 114us/step - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 34/50\n",
      "26367/26367 [==============================] - 3s 115us/step - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 35/50\n",
      "26367/26367 [==============================] - 3s 114us/step - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 36/50\n",
      "26367/26367 [==============================] - 3s 114us/step - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 37/50\n",
      "26367/26367 [==============================] - 3s 115us/step - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 38/50\n",
      "26367/26367 [==============================] - 3s 115us/step - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 39/50\n",
      "26367/26367 [==============================] - 3s 115us/step - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 40/50\n",
      "26367/26367 [==============================] - 3s 115us/step - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 41/50\n",
      "26367/26367 [==============================] - 3s 114us/step - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 42/50\n",
      "26367/26367 [==============================] - 3s 114us/step - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 43/50\n",
      "26367/26367 [==============================] - 3s 116us/step - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 44/50\n",
      "26367/26367 [==============================] - 3s 117us/step - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 45/50\n",
      "26367/26367 [==============================] - 3s 114us/step - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 46/50\n",
      "26367/26367 [==============================] - 3s 118us/step - loss: 0.0010 - acc: 1.0000\n",
      "Epoch 47/50\n",
      "26367/26367 [==============================] - 3s 117us/step - loss: 0.0010 - acc: 1.0000\n",
      "Epoch 48/50\n",
      "26367/26367 [==============================] - 3s 115us/step - loss: 0.0010 - acc: 1.0000\n",
      "Epoch 49/50\n",
      "26367/26367 [==============================] - 3s 115us/step - loss: 0.0010 - acc: 1.0000\n",
      "Epoch 50/50\n",
      "26367/26367 [==============================] - 4s 137us/step - loss: 9.9521e-04 - acc: 1.0000\n",
      "\n",
      "AUGMENTING DATASET WITH BATCH # 10 :\n",
      "\n",
      "Labels accepted:  1929 / 2100\n",
      "\n",
      "Epoch 1/50\n",
      "28296/28296 [==============================] - 8s 282us/step - loss: 0.7924 - acc: 0.7695\n",
      "Epoch 2/50\n",
      "28296/28296 [==============================] - 3s 115us/step - loss: 0.1815 - acc: 0.9461\n",
      "Epoch 3/50\n",
      "28296/28296 [==============================] - 3s 115us/step - loss: 0.1068 - acc: 0.9684\n",
      "Epoch 4/50\n",
      "28296/28296 [==============================] - 3s 117us/step - loss: 0.0763 - acc: 0.9795\n",
      "Epoch 5/50\n",
      "28296/28296 [==============================] - 3s 116us/step - loss: 0.0575 - acc: 0.9849\n",
      "Epoch 6/50\n",
      "28296/28296 [==============================] - 3s 119us/step - loss: 0.0407 - acc: 0.9902\n",
      "Epoch 7/50\n",
      "28296/28296 [==============================] - 4s 126us/step - loss: 0.0307 - acc: 0.9931\n",
      "Epoch 8/50\n",
      "11000/28296 [==========>...................] - ETA: 2s - loss: 0.0220 - acc: 0.9959"
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "tau = 0.95\n",
    "classifier_augmented = augment_labeled_data(n, tau, X_train_labeled, y_train, X_train_unlabeled, classifier_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_semi = classifier_augmented.predict(X_test)\n",
    "# find most likely category from soft max\n",
    "pred_semi = np.argmax(pred_semi, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert back to pandas dataframe\n",
    "X_test = pd.DataFrame(X_test)\n",
    "pred_submit = pd.DataFrame(list(zip(X_test.index.values + 30000, pred_semi)), columns=['Id', 'y'])\n",
    "final_submit = pred_submit.to_csv(\"submit.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
