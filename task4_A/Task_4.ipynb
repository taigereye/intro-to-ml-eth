{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to ML Project\n",
    "## Task 4\n",
    "### Jan Bauer, Alaisha Sharma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils, to_categorical\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# same seed for consistency\n",
    "seed = 1\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>...</th>\n",
       "      <th>x130</th>\n",
       "      <th>x131</th>\n",
       "      <th>x132</th>\n",
       "      <th>x133</th>\n",
       "      <th>x134</th>\n",
       "      <th>x135</th>\n",
       "      <th>x136</th>\n",
       "      <th>x137</th>\n",
       "      <th>x138</th>\n",
       "      <th>x139</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0.791590</td>\n",
       "      <td>0.753101</td>\n",
       "      <td>0.577893</td>\n",
       "      <td>0.726169</td>\n",
       "      <td>0.395992</td>\n",
       "      <td>0.542416</td>\n",
       "      <td>0.482571</td>\n",
       "      <td>0.409337</td>\n",
       "      <td>0.365332</td>\n",
       "      <td>...</td>\n",
       "      <td>0.662795</td>\n",
       "      <td>0.569457</td>\n",
       "      <td>0.328729</td>\n",
       "      <td>0.356595</td>\n",
       "      <td>0.642982</td>\n",
       "      <td>0.545763</td>\n",
       "      <td>0.673734</td>\n",
       "      <td>0.487183</td>\n",
       "      <td>0.504894</td>\n",
       "      <td>0.670266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>0.798071</td>\n",
       "      <td>0.757271</td>\n",
       "      <td>0.575022</td>\n",
       "      <td>0.729636</td>\n",
       "      <td>0.400243</td>\n",
       "      <td>0.536507</td>\n",
       "      <td>0.483917</td>\n",
       "      <td>0.415249</td>\n",
       "      <td>0.374363</td>\n",
       "      <td>...</td>\n",
       "      <td>0.660528</td>\n",
       "      <td>0.573125</td>\n",
       "      <td>0.334675</td>\n",
       "      <td>0.360123</td>\n",
       "      <td>0.641473</td>\n",
       "      <td>0.552560</td>\n",
       "      <td>0.672730</td>\n",
       "      <td>0.486268</td>\n",
       "      <td>0.509508</td>\n",
       "      <td>0.670837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>0.794205</td>\n",
       "      <td>0.752145</td>\n",
       "      <td>0.570502</td>\n",
       "      <td>0.728514</td>\n",
       "      <td>0.407141</td>\n",
       "      <td>0.545053</td>\n",
       "      <td>0.482661</td>\n",
       "      <td>0.414902</td>\n",
       "      <td>0.370317</td>\n",
       "      <td>...</td>\n",
       "      <td>0.667669</td>\n",
       "      <td>0.576651</td>\n",
       "      <td>0.325308</td>\n",
       "      <td>0.365656</td>\n",
       "      <td>0.640514</td>\n",
       "      <td>0.568385</td>\n",
       "      <td>0.672446</td>\n",
       "      <td>0.485341</td>\n",
       "      <td>0.500234</td>\n",
       "      <td>0.678961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.793530</td>\n",
       "      <td>0.752631</td>\n",
       "      <td>0.575777</td>\n",
       "      <td>0.723480</td>\n",
       "      <td>0.395984</td>\n",
       "      <td>0.548614</td>\n",
       "      <td>0.488048</td>\n",
       "      <td>0.412096</td>\n",
       "      <td>0.373032</td>\n",
       "      <td>...</td>\n",
       "      <td>0.663633</td>\n",
       "      <td>0.574640</td>\n",
       "      <td>0.332292</td>\n",
       "      <td>0.361036</td>\n",
       "      <td>0.642928</td>\n",
       "      <td>0.545435</td>\n",
       "      <td>0.671515</td>\n",
       "      <td>0.481010</td>\n",
       "      <td>0.509037</td>\n",
       "      <td>0.673317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.794775</td>\n",
       "      <td>0.745556</td>\n",
       "      <td>0.570395</td>\n",
       "      <td>0.724464</td>\n",
       "      <td>0.399826</td>\n",
       "      <td>0.546392</td>\n",
       "      <td>0.490428</td>\n",
       "      <td>0.412121</td>\n",
       "      <td>0.365301</td>\n",
       "      <td>...</td>\n",
       "      <td>0.666438</td>\n",
       "      <td>0.578923</td>\n",
       "      <td>0.327307</td>\n",
       "      <td>0.370091</td>\n",
       "      <td>0.648864</td>\n",
       "      <td>0.557257</td>\n",
       "      <td>0.673051</td>\n",
       "      <td>0.481574</td>\n",
       "      <td>0.503186</td>\n",
       "      <td>0.674417</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 140 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   y        x1        x2        x3        x4        x5        x6        x7  \\\n",
       "0  5  0.791590  0.753101  0.577893  0.726169  0.395992  0.542416  0.482571   \n",
       "1  8  0.798071  0.757271  0.575022  0.729636  0.400243  0.536507  0.483917   \n",
       "2  7  0.794205  0.752145  0.570502  0.728514  0.407141  0.545053  0.482661   \n",
       "3  2  0.793530  0.752631  0.575777  0.723480  0.395984  0.548614  0.488048   \n",
       "4  1  0.794775  0.745556  0.570395  0.724464  0.399826  0.546392  0.490428   \n",
       "\n",
       "         x8        x9  ...      x130      x131      x132      x133      x134  \\\n",
       "0  0.409337  0.365332  ...  0.662795  0.569457  0.328729  0.356595  0.642982   \n",
       "1  0.415249  0.374363  ...  0.660528  0.573125  0.334675  0.360123  0.641473   \n",
       "2  0.414902  0.370317  ...  0.667669  0.576651  0.325308  0.365656  0.640514   \n",
       "3  0.412096  0.373032  ...  0.663633  0.574640  0.332292  0.361036  0.642928   \n",
       "4  0.412121  0.365301  ...  0.666438  0.578923  0.327307  0.370091  0.648864   \n",
       "\n",
       "       x135      x136      x137      x138      x139  \n",
       "0  0.545763  0.673734  0.487183  0.504894  0.670266  \n",
       "1  0.552560  0.672730  0.486268  0.509508  0.670837  \n",
       "2  0.568385  0.672446  0.485341  0.500234  0.678961  \n",
       "3  0.545435  0.671515  0.481010  0.509037  0.673317  \n",
       "4  0.557257  0.673051  0.481574  0.503186  0.674417  \n",
       "\n",
       "[5 rows x 140 columns]"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_labeled = pd.read_hdf(\"data/train_labeled.h5\", \"train\")\n",
    "train_data_labeled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "      <th>...</th>\n",
       "      <th>x130</th>\n",
       "      <th>x131</th>\n",
       "      <th>x132</th>\n",
       "      <th>x133</th>\n",
       "      <th>x134</th>\n",
       "      <th>x135</th>\n",
       "      <th>x136</th>\n",
       "      <th>x137</th>\n",
       "      <th>x138</th>\n",
       "      <th>x139</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.791590</td>\n",
       "      <td>0.753101</td>\n",
       "      <td>0.577893</td>\n",
       "      <td>0.726169</td>\n",
       "      <td>0.395992</td>\n",
       "      <td>0.542416</td>\n",
       "      <td>0.482571</td>\n",
       "      <td>0.409337</td>\n",
       "      <td>0.365332</td>\n",
       "      <td>0.163388</td>\n",
       "      <td>...</td>\n",
       "      <td>0.662795</td>\n",
       "      <td>0.569457</td>\n",
       "      <td>0.328729</td>\n",
       "      <td>0.356595</td>\n",
       "      <td>0.642982</td>\n",
       "      <td>0.545763</td>\n",
       "      <td>0.673734</td>\n",
       "      <td>0.487183</td>\n",
       "      <td>0.504894</td>\n",
       "      <td>0.670266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.798071</td>\n",
       "      <td>0.757271</td>\n",
       "      <td>0.575022</td>\n",
       "      <td>0.729636</td>\n",
       "      <td>0.400243</td>\n",
       "      <td>0.536507</td>\n",
       "      <td>0.483917</td>\n",
       "      <td>0.415249</td>\n",
       "      <td>0.374363</td>\n",
       "      <td>0.159272</td>\n",
       "      <td>...</td>\n",
       "      <td>0.660528</td>\n",
       "      <td>0.573125</td>\n",
       "      <td>0.334675</td>\n",
       "      <td>0.360123</td>\n",
       "      <td>0.641473</td>\n",
       "      <td>0.552560</td>\n",
       "      <td>0.672730</td>\n",
       "      <td>0.486268</td>\n",
       "      <td>0.509508</td>\n",
       "      <td>0.670837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.794205</td>\n",
       "      <td>0.752145</td>\n",
       "      <td>0.570502</td>\n",
       "      <td>0.728514</td>\n",
       "      <td>0.407141</td>\n",
       "      <td>0.545053</td>\n",
       "      <td>0.482661</td>\n",
       "      <td>0.414902</td>\n",
       "      <td>0.370317</td>\n",
       "      <td>0.164221</td>\n",
       "      <td>...</td>\n",
       "      <td>0.667669</td>\n",
       "      <td>0.576651</td>\n",
       "      <td>0.325308</td>\n",
       "      <td>0.365656</td>\n",
       "      <td>0.640514</td>\n",
       "      <td>0.568385</td>\n",
       "      <td>0.672446</td>\n",
       "      <td>0.485341</td>\n",
       "      <td>0.500234</td>\n",
       "      <td>0.678961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.793530</td>\n",
       "      <td>0.752631</td>\n",
       "      <td>0.575777</td>\n",
       "      <td>0.723480</td>\n",
       "      <td>0.395984</td>\n",
       "      <td>0.548614</td>\n",
       "      <td>0.488048</td>\n",
       "      <td>0.412096</td>\n",
       "      <td>0.373032</td>\n",
       "      <td>0.160413</td>\n",
       "      <td>...</td>\n",
       "      <td>0.663633</td>\n",
       "      <td>0.574640</td>\n",
       "      <td>0.332292</td>\n",
       "      <td>0.361036</td>\n",
       "      <td>0.642928</td>\n",
       "      <td>0.545435</td>\n",
       "      <td>0.671515</td>\n",
       "      <td>0.481010</td>\n",
       "      <td>0.509037</td>\n",
       "      <td>0.673317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.794775</td>\n",
       "      <td>0.745556</td>\n",
       "      <td>0.570395</td>\n",
       "      <td>0.724464</td>\n",
       "      <td>0.399826</td>\n",
       "      <td>0.546392</td>\n",
       "      <td>0.490428</td>\n",
       "      <td>0.412121</td>\n",
       "      <td>0.365301</td>\n",
       "      <td>0.161424</td>\n",
       "      <td>...</td>\n",
       "      <td>0.666438</td>\n",
       "      <td>0.578923</td>\n",
       "      <td>0.327307</td>\n",
       "      <td>0.370091</td>\n",
       "      <td>0.648864</td>\n",
       "      <td>0.557257</td>\n",
       "      <td>0.673051</td>\n",
       "      <td>0.481574</td>\n",
       "      <td>0.503186</td>\n",
       "      <td>0.674417</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 139 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         x1        x2        x3        x4        x5        x6        x7  \\\n",
       "0  0.791590  0.753101  0.577893  0.726169  0.395992  0.542416  0.482571   \n",
       "1  0.798071  0.757271  0.575022  0.729636  0.400243  0.536507  0.483917   \n",
       "2  0.794205  0.752145  0.570502  0.728514  0.407141  0.545053  0.482661   \n",
       "3  0.793530  0.752631  0.575777  0.723480  0.395984  0.548614  0.488048   \n",
       "4  0.794775  0.745556  0.570395  0.724464  0.399826  0.546392  0.490428   \n",
       "\n",
       "         x8        x9       x10  ...      x130      x131      x132      x133  \\\n",
       "0  0.409337  0.365332  0.163388  ...  0.662795  0.569457  0.328729  0.356595   \n",
       "1  0.415249  0.374363  0.159272  ...  0.660528  0.573125  0.334675  0.360123   \n",
       "2  0.414902  0.370317  0.164221  ...  0.667669  0.576651  0.325308  0.365656   \n",
       "3  0.412096  0.373032  0.160413  ...  0.663633  0.574640  0.332292  0.361036   \n",
       "4  0.412121  0.365301  0.161424  ...  0.666438  0.578923  0.327307  0.370091   \n",
       "\n",
       "       x134      x135      x136      x137      x138      x139  \n",
       "0  0.642982  0.545763  0.673734  0.487183  0.504894  0.670266  \n",
       "1  0.641473  0.552560  0.672730  0.486268  0.509508  0.670837  \n",
       "2  0.640514  0.568385  0.672446  0.485341  0.500234  0.678961  \n",
       "3  0.642928  0.545435  0.671515  0.481010  0.509037  0.673317  \n",
       "4  0.648864  0.557257  0.673051  0.481574  0.503186  0.674417  \n",
       "\n",
       "[5 rows x 139 columns]"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_labeled = train_data_labeled.iloc[:,1:]\n",
    "X_train_labeled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   y\n",
       "0  5\n",
       "1  8\n",
       "2  7\n",
       "3  2\n",
       "4  1"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = train_data_labeled.iloc[:,0:1]\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "      <th>...</th>\n",
       "      <th>x130</th>\n",
       "      <th>x131</th>\n",
       "      <th>x132</th>\n",
       "      <th>x133</th>\n",
       "      <th>x134</th>\n",
       "      <th>x135</th>\n",
       "      <th>x136</th>\n",
       "      <th>x137</th>\n",
       "      <th>x138</th>\n",
       "      <th>x139</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9000</th>\n",
       "      <td>0.802073</td>\n",
       "      <td>0.751775</td>\n",
       "      <td>0.572571</td>\n",
       "      <td>0.729820</td>\n",
       "      <td>0.401511</td>\n",
       "      <td>0.546931</td>\n",
       "      <td>0.483566</td>\n",
       "      <td>0.409619</td>\n",
       "      <td>0.369915</td>\n",
       "      <td>0.162539</td>\n",
       "      <td>...</td>\n",
       "      <td>0.663145</td>\n",
       "      <td>0.578450</td>\n",
       "      <td>0.325368</td>\n",
       "      <td>0.363988</td>\n",
       "      <td>0.640859</td>\n",
       "      <td>0.563103</td>\n",
       "      <td>0.673581</td>\n",
       "      <td>0.480801</td>\n",
       "      <td>0.506701</td>\n",
       "      <td>0.670141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9001</th>\n",
       "      <td>0.796261</td>\n",
       "      <td>0.747018</td>\n",
       "      <td>0.576803</td>\n",
       "      <td>0.720181</td>\n",
       "      <td>0.397541</td>\n",
       "      <td>0.543298</td>\n",
       "      <td>0.490455</td>\n",
       "      <td>0.410719</td>\n",
       "      <td>0.365985</td>\n",
       "      <td>0.159075</td>\n",
       "      <td>...</td>\n",
       "      <td>0.667465</td>\n",
       "      <td>0.574655</td>\n",
       "      <td>0.331916</td>\n",
       "      <td>0.366610</td>\n",
       "      <td>0.644864</td>\n",
       "      <td>0.544353</td>\n",
       "      <td>0.670754</td>\n",
       "      <td>0.485595</td>\n",
       "      <td>0.507704</td>\n",
       "      <td>0.670917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9002</th>\n",
       "      <td>0.794176</td>\n",
       "      <td>0.751640</td>\n",
       "      <td>0.570850</td>\n",
       "      <td>0.722511</td>\n",
       "      <td>0.401524</td>\n",
       "      <td>0.538023</td>\n",
       "      <td>0.481063</td>\n",
       "      <td>0.413562</td>\n",
       "      <td>0.371465</td>\n",
       "      <td>0.158596</td>\n",
       "      <td>...</td>\n",
       "      <td>0.659293</td>\n",
       "      <td>0.568071</td>\n",
       "      <td>0.327978</td>\n",
       "      <td>0.357855</td>\n",
       "      <td>0.643347</td>\n",
       "      <td>0.558021</td>\n",
       "      <td>0.674759</td>\n",
       "      <td>0.486917</td>\n",
       "      <td>0.507812</td>\n",
       "      <td>0.681148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9003</th>\n",
       "      <td>0.795145</td>\n",
       "      <td>0.753813</td>\n",
       "      <td>0.577260</td>\n",
       "      <td>0.727300</td>\n",
       "      <td>0.399132</td>\n",
       "      <td>0.540990</td>\n",
       "      <td>0.483486</td>\n",
       "      <td>0.410847</td>\n",
       "      <td>0.367753</td>\n",
       "      <td>0.163416</td>\n",
       "      <td>...</td>\n",
       "      <td>0.663264</td>\n",
       "      <td>0.573702</td>\n",
       "      <td>0.326960</td>\n",
       "      <td>0.356632</td>\n",
       "      <td>0.648938</td>\n",
       "      <td>0.553826</td>\n",
       "      <td>0.674693</td>\n",
       "      <td>0.494589</td>\n",
       "      <td>0.503478</td>\n",
       "      <td>0.675546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9004</th>\n",
       "      <td>0.795453</td>\n",
       "      <td>0.749442</td>\n",
       "      <td>0.574719</td>\n",
       "      <td>0.717568</td>\n",
       "      <td>0.395512</td>\n",
       "      <td>0.548442</td>\n",
       "      <td>0.489007</td>\n",
       "      <td>0.413862</td>\n",
       "      <td>0.365941</td>\n",
       "      <td>0.161363</td>\n",
       "      <td>...</td>\n",
       "      <td>0.660846</td>\n",
       "      <td>0.577899</td>\n",
       "      <td>0.333309</td>\n",
       "      <td>0.363624</td>\n",
       "      <td>0.647993</td>\n",
       "      <td>0.541827</td>\n",
       "      <td>0.675881</td>\n",
       "      <td>0.486793</td>\n",
       "      <td>0.508442</td>\n",
       "      <td>0.664697</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 139 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            x1        x2        x3        x4        x5        x6        x7  \\\n",
       "9000  0.802073  0.751775  0.572571  0.729820  0.401511  0.546931  0.483566   \n",
       "9001  0.796261  0.747018  0.576803  0.720181  0.397541  0.543298  0.490455   \n",
       "9002  0.794176  0.751640  0.570850  0.722511  0.401524  0.538023  0.481063   \n",
       "9003  0.795145  0.753813  0.577260  0.727300  0.399132  0.540990  0.483486   \n",
       "9004  0.795453  0.749442  0.574719  0.717568  0.395512  0.548442  0.489007   \n",
       "\n",
       "            x8        x9       x10  ...      x130      x131      x132  \\\n",
       "9000  0.409619  0.369915  0.162539  ...  0.663145  0.578450  0.325368   \n",
       "9001  0.410719  0.365985  0.159075  ...  0.667465  0.574655  0.331916   \n",
       "9002  0.413562  0.371465  0.158596  ...  0.659293  0.568071  0.327978   \n",
       "9003  0.410847  0.367753  0.163416  ...  0.663264  0.573702  0.326960   \n",
       "9004  0.413862  0.365941  0.161363  ...  0.660846  0.577899  0.333309   \n",
       "\n",
       "          x133      x134      x135      x136      x137      x138      x139  \n",
       "9000  0.363988  0.640859  0.563103  0.673581  0.480801  0.506701  0.670141  \n",
       "9001  0.366610  0.644864  0.544353  0.670754  0.485595  0.507704  0.670917  \n",
       "9002  0.357855  0.643347  0.558021  0.674759  0.486917  0.507812  0.681148  \n",
       "9003  0.356632  0.648938  0.553826  0.674693  0.494589  0.503478  0.675546  \n",
       "9004  0.363624  0.647993  0.541827  0.675881  0.486793  0.508442  0.664697  \n",
       "\n",
       "[5 rows x 139 columns]"
      ]
     },
     "execution_count": 476,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_unlabeled = pd.read_hdf(\"data/train_unlabeled.h5\", \"train\")\n",
    "train_data_unlabeled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "      <th>...</th>\n",
       "      <th>x130</th>\n",
       "      <th>x131</th>\n",
       "      <th>x132</th>\n",
       "      <th>x133</th>\n",
       "      <th>x134</th>\n",
       "      <th>x135</th>\n",
       "      <th>x136</th>\n",
       "      <th>x137</th>\n",
       "      <th>x138</th>\n",
       "      <th>x139</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9000</th>\n",
       "      <td>0.802073</td>\n",
       "      <td>0.751775</td>\n",
       "      <td>0.572571</td>\n",
       "      <td>0.729820</td>\n",
       "      <td>0.401511</td>\n",
       "      <td>0.546931</td>\n",
       "      <td>0.483566</td>\n",
       "      <td>0.409619</td>\n",
       "      <td>0.369915</td>\n",
       "      <td>0.162539</td>\n",
       "      <td>...</td>\n",
       "      <td>0.663145</td>\n",
       "      <td>0.578450</td>\n",
       "      <td>0.325368</td>\n",
       "      <td>0.363988</td>\n",
       "      <td>0.640859</td>\n",
       "      <td>0.563103</td>\n",
       "      <td>0.673581</td>\n",
       "      <td>0.480801</td>\n",
       "      <td>0.506701</td>\n",
       "      <td>0.670141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9001</th>\n",
       "      <td>0.796261</td>\n",
       "      <td>0.747018</td>\n",
       "      <td>0.576803</td>\n",
       "      <td>0.720181</td>\n",
       "      <td>0.397541</td>\n",
       "      <td>0.543298</td>\n",
       "      <td>0.490455</td>\n",
       "      <td>0.410719</td>\n",
       "      <td>0.365985</td>\n",
       "      <td>0.159075</td>\n",
       "      <td>...</td>\n",
       "      <td>0.667465</td>\n",
       "      <td>0.574655</td>\n",
       "      <td>0.331916</td>\n",
       "      <td>0.366610</td>\n",
       "      <td>0.644864</td>\n",
       "      <td>0.544353</td>\n",
       "      <td>0.670754</td>\n",
       "      <td>0.485595</td>\n",
       "      <td>0.507704</td>\n",
       "      <td>0.670917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9002</th>\n",
       "      <td>0.794176</td>\n",
       "      <td>0.751640</td>\n",
       "      <td>0.570850</td>\n",
       "      <td>0.722511</td>\n",
       "      <td>0.401524</td>\n",
       "      <td>0.538023</td>\n",
       "      <td>0.481063</td>\n",
       "      <td>0.413562</td>\n",
       "      <td>0.371465</td>\n",
       "      <td>0.158596</td>\n",
       "      <td>...</td>\n",
       "      <td>0.659293</td>\n",
       "      <td>0.568071</td>\n",
       "      <td>0.327978</td>\n",
       "      <td>0.357855</td>\n",
       "      <td>0.643347</td>\n",
       "      <td>0.558021</td>\n",
       "      <td>0.674759</td>\n",
       "      <td>0.486917</td>\n",
       "      <td>0.507812</td>\n",
       "      <td>0.681148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9003</th>\n",
       "      <td>0.795145</td>\n",
       "      <td>0.753813</td>\n",
       "      <td>0.577260</td>\n",
       "      <td>0.727300</td>\n",
       "      <td>0.399132</td>\n",
       "      <td>0.540990</td>\n",
       "      <td>0.483486</td>\n",
       "      <td>0.410847</td>\n",
       "      <td>0.367753</td>\n",
       "      <td>0.163416</td>\n",
       "      <td>...</td>\n",
       "      <td>0.663264</td>\n",
       "      <td>0.573702</td>\n",
       "      <td>0.326960</td>\n",
       "      <td>0.356632</td>\n",
       "      <td>0.648938</td>\n",
       "      <td>0.553826</td>\n",
       "      <td>0.674693</td>\n",
       "      <td>0.494589</td>\n",
       "      <td>0.503478</td>\n",
       "      <td>0.675546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9004</th>\n",
       "      <td>0.795453</td>\n",
       "      <td>0.749442</td>\n",
       "      <td>0.574719</td>\n",
       "      <td>0.717568</td>\n",
       "      <td>0.395512</td>\n",
       "      <td>0.548442</td>\n",
       "      <td>0.489007</td>\n",
       "      <td>0.413862</td>\n",
       "      <td>0.365941</td>\n",
       "      <td>0.161363</td>\n",
       "      <td>...</td>\n",
       "      <td>0.660846</td>\n",
       "      <td>0.577899</td>\n",
       "      <td>0.333309</td>\n",
       "      <td>0.363624</td>\n",
       "      <td>0.647993</td>\n",
       "      <td>0.541827</td>\n",
       "      <td>0.675881</td>\n",
       "      <td>0.486793</td>\n",
       "      <td>0.508442</td>\n",
       "      <td>0.664697</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 139 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            x1        x2        x3        x4        x5        x6        x7  \\\n",
       "9000  0.802073  0.751775  0.572571  0.729820  0.401511  0.546931  0.483566   \n",
       "9001  0.796261  0.747018  0.576803  0.720181  0.397541  0.543298  0.490455   \n",
       "9002  0.794176  0.751640  0.570850  0.722511  0.401524  0.538023  0.481063   \n",
       "9003  0.795145  0.753813  0.577260  0.727300  0.399132  0.540990  0.483486   \n",
       "9004  0.795453  0.749442  0.574719  0.717568  0.395512  0.548442  0.489007   \n",
       "\n",
       "            x8        x9       x10  ...      x130      x131      x132  \\\n",
       "9000  0.409619  0.369915  0.162539  ...  0.663145  0.578450  0.325368   \n",
       "9001  0.410719  0.365985  0.159075  ...  0.667465  0.574655  0.331916   \n",
       "9002  0.413562  0.371465  0.158596  ...  0.659293  0.568071  0.327978   \n",
       "9003  0.410847  0.367753  0.163416  ...  0.663264  0.573702  0.326960   \n",
       "9004  0.413862  0.365941  0.161363  ...  0.660846  0.577899  0.333309   \n",
       "\n",
       "          x133      x134      x135      x136      x137      x138      x139  \n",
       "9000  0.363988  0.640859  0.563103  0.673581  0.480801  0.506701  0.670141  \n",
       "9001  0.366610  0.644864  0.544353  0.670754  0.485595  0.507704  0.670917  \n",
       "9002  0.357855  0.643347  0.558021  0.674759  0.486917  0.507812  0.681148  \n",
       "9003  0.356632  0.648938  0.553826  0.674693  0.494589  0.503478  0.675546  \n",
       "9004  0.363624  0.647993  0.541827  0.675881  0.486793  0.508442  0.664697  \n",
       "\n",
       "[5 rows x 139 columns]"
      ]
     },
     "execution_count": 477,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_unlabeled = train_data_unlabeled.iloc[:,:]\n",
    "X_train_unlabeled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "      <th>...</th>\n",
       "      <th>x130</th>\n",
       "      <th>x131</th>\n",
       "      <th>x132</th>\n",
       "      <th>x133</th>\n",
       "      <th>x134</th>\n",
       "      <th>x135</th>\n",
       "      <th>x136</th>\n",
       "      <th>x137</th>\n",
       "      <th>x138</th>\n",
       "      <th>x139</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30000</th>\n",
       "      <td>0.795621</td>\n",
       "      <td>0.753163</td>\n",
       "      <td>0.572225</td>\n",
       "      <td>0.727620</td>\n",
       "      <td>0.400279</td>\n",
       "      <td>0.541950</td>\n",
       "      <td>0.478941</td>\n",
       "      <td>0.414370</td>\n",
       "      <td>0.371107</td>\n",
       "      <td>0.164238</td>\n",
       "      <td>...</td>\n",
       "      <td>0.664335</td>\n",
       "      <td>0.571888</td>\n",
       "      <td>0.325982</td>\n",
       "      <td>0.359063</td>\n",
       "      <td>0.639611</td>\n",
       "      <td>0.553841</td>\n",
       "      <td>0.674134</td>\n",
       "      <td>0.484140</td>\n",
       "      <td>0.510139</td>\n",
       "      <td>0.668640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30001</th>\n",
       "      <td>0.793922</td>\n",
       "      <td>0.745725</td>\n",
       "      <td>0.571753</td>\n",
       "      <td>0.721803</td>\n",
       "      <td>0.399201</td>\n",
       "      <td>0.548342</td>\n",
       "      <td>0.483106</td>\n",
       "      <td>0.410964</td>\n",
       "      <td>0.369730</td>\n",
       "      <td>0.160543</td>\n",
       "      <td>...</td>\n",
       "      <td>0.668972</td>\n",
       "      <td>0.579854</td>\n",
       "      <td>0.336250</td>\n",
       "      <td>0.369700</td>\n",
       "      <td>0.647012</td>\n",
       "      <td>0.552805</td>\n",
       "      <td>0.673593</td>\n",
       "      <td>0.483753</td>\n",
       "      <td>0.509276</td>\n",
       "      <td>0.675991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30002</th>\n",
       "      <td>0.791894</td>\n",
       "      <td>0.757128</td>\n",
       "      <td>0.573741</td>\n",
       "      <td>0.724121</td>\n",
       "      <td>0.401164</td>\n",
       "      <td>0.547744</td>\n",
       "      <td>0.481351</td>\n",
       "      <td>0.415416</td>\n",
       "      <td>0.368478</td>\n",
       "      <td>0.161942</td>\n",
       "      <td>...</td>\n",
       "      <td>0.663038</td>\n",
       "      <td>0.568997</td>\n",
       "      <td>0.328963</td>\n",
       "      <td>0.355481</td>\n",
       "      <td>0.641702</td>\n",
       "      <td>0.551018</td>\n",
       "      <td>0.675904</td>\n",
       "      <td>0.488888</td>\n",
       "      <td>0.506695</td>\n",
       "      <td>0.675925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30003</th>\n",
       "      <td>0.794773</td>\n",
       "      <td>0.747188</td>\n",
       "      <td>0.571375</td>\n",
       "      <td>0.719419</td>\n",
       "      <td>0.398849</td>\n",
       "      <td>0.541140</td>\n",
       "      <td>0.486918</td>\n",
       "      <td>0.422196</td>\n",
       "      <td>0.371877</td>\n",
       "      <td>0.160025</td>\n",
       "      <td>...</td>\n",
       "      <td>0.665051</td>\n",
       "      <td>0.578397</td>\n",
       "      <td>0.332610</td>\n",
       "      <td>0.370070</td>\n",
       "      <td>0.643746</td>\n",
       "      <td>0.550737</td>\n",
       "      <td>0.674714</td>\n",
       "      <td>0.487900</td>\n",
       "      <td>0.509579</td>\n",
       "      <td>0.672635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30004</th>\n",
       "      <td>0.796351</td>\n",
       "      <td>0.751545</td>\n",
       "      <td>0.569544</td>\n",
       "      <td>0.718811</td>\n",
       "      <td>0.401796</td>\n",
       "      <td>0.543482</td>\n",
       "      <td>0.484521</td>\n",
       "      <td>0.420023</td>\n",
       "      <td>0.369082</td>\n",
       "      <td>0.158959</td>\n",
       "      <td>...</td>\n",
       "      <td>0.663495</td>\n",
       "      <td>0.577053</td>\n",
       "      <td>0.332620</td>\n",
       "      <td>0.366809</td>\n",
       "      <td>0.641575</td>\n",
       "      <td>0.545131</td>\n",
       "      <td>0.678834</td>\n",
       "      <td>0.488806</td>\n",
       "      <td>0.512001</td>\n",
       "      <td>0.672110</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 139 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             x1        x2        x3        x4        x5        x6        x7  \\\n",
       "30000  0.795621  0.753163  0.572225  0.727620  0.400279  0.541950  0.478941   \n",
       "30001  0.793922  0.745725  0.571753  0.721803  0.399201  0.548342  0.483106   \n",
       "30002  0.791894  0.757128  0.573741  0.724121  0.401164  0.547744  0.481351   \n",
       "30003  0.794773  0.747188  0.571375  0.719419  0.398849  0.541140  0.486918   \n",
       "30004  0.796351  0.751545  0.569544  0.718811  0.401796  0.543482  0.484521   \n",
       "\n",
       "             x8        x9       x10  ...      x130      x131      x132  \\\n",
       "30000  0.414370  0.371107  0.164238  ...  0.664335  0.571888  0.325982   \n",
       "30001  0.410964  0.369730  0.160543  ...  0.668972  0.579854  0.336250   \n",
       "30002  0.415416  0.368478  0.161942  ...  0.663038  0.568997  0.328963   \n",
       "30003  0.422196  0.371877  0.160025  ...  0.665051  0.578397  0.332610   \n",
       "30004  0.420023  0.369082  0.158959  ...  0.663495  0.577053  0.332620   \n",
       "\n",
       "           x133      x134      x135      x136      x137      x138      x139  \n",
       "30000  0.359063  0.639611  0.553841  0.674134  0.484140  0.510139  0.668640  \n",
       "30001  0.369700  0.647012  0.552805  0.673593  0.483753  0.509276  0.675991  \n",
       "30002  0.355481  0.641702  0.551018  0.675904  0.488888  0.506695  0.675925  \n",
       "30003  0.370070  0.643746  0.550737  0.674714  0.487900  0.509579  0.672635  \n",
       "30004  0.366809  0.641575  0.545131  0.678834  0.488806  0.512001  0.672110  \n",
       "\n",
       "[5 rows x 139 columns]"
      ]
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = pd.read_hdf(\"data/test.h5\", \"test\")\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()  \n",
    "X_all_data = pd.concat([pd.DataFrame(X_train_labeled), pd.DataFrame(X_train_unlabeled), pd.DataFrame(X_test)])\n",
    "scaler.fit(X_all_data)\n",
    "\n",
    "X_train_labeled = scaler.transform(X_train_labeled)  \n",
    "X_train_unlabeled = scaler.transform(X_train_unlabeled)\n",
    "X_test = scaler.transform(X_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pca = PCA(n_components=139)\n",
    "pca.fit(X_train_labeled)\n",
    "\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Cumulative explained variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9000, 52) (21000, 52) (8000, 52)\n"
     ]
    }
   ],
   "source": [
    "k = 52\n",
    "pca = PCA(n_components=k)\n",
    "X_train_labeled = pca.fit_transform(X_train_labeled)\n",
    "\n",
    "X_train_unlabeled = pca.transform(X_train_unlabeled)\n",
    "X_test = pca.transform(X_test)\n",
    "pca_std = np.std(X_train_labeled)\n",
    "\n",
    "print(X_train_labeled.shape, X_train_unlabeled.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:95: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:128: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_train)\n",
    "encoded_y = encoder.transform(y_train)\n",
    "onehot_y = np_utils.to_categorical(encoded_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "9000/9000 [==============================] - 5s 583us/step - loss: 1.7818 - acc: 0.4367\n",
      "Epoch 2/100\n",
      "9000/9000 [==============================] - 1s 87us/step - loss: 0.7142 - acc: 0.7796\n",
      "Epoch 3/100\n",
      "9000/9000 [==============================] - 1s 89us/step - loss: 0.5685 - acc: 0.8206\n",
      "Epoch 4/100\n",
      "9000/9000 [==============================] - 1s 89us/step - loss: 0.5107 - acc: 0.8398\n",
      "Epoch 5/100\n",
      "9000/9000 [==============================] - 1s 91us/step - loss: 0.4740 - acc: 0.8486\n",
      "Epoch 6/100\n",
      "9000/9000 [==============================] - 1s 126us/step - loss: 0.4311 - acc: 0.8672\n",
      "Epoch 7/100\n",
      "9000/9000 [==============================] - 1s 123us/step - loss: 0.4094 - acc: 0.8681\n",
      "Epoch 8/100\n",
      "9000/9000 [==============================] - 1s 127us/step - loss: 0.3842 - acc: 0.8802\n",
      "Epoch 9/100\n",
      "9000/9000 [==============================] - 1s 130us/step - loss: 0.3662 - acc: 0.8847\n",
      "Epoch 10/100\n",
      "9000/9000 [==============================] - 1s 115us/step - loss: 0.3464 - acc: 0.8892\n",
      "Epoch 11/100\n",
      "9000/9000 [==============================] - 1s 100us/step - loss: 0.3312 - acc: 0.8942\n",
      "Epoch 12/100\n",
      "9000/9000 [==============================] - 1s 94us/step - loss: 0.3094 - acc: 0.9011\n",
      "Epoch 13/100\n",
      "9000/9000 [==============================] - 1s 121us/step - loss: 0.3032 - acc: 0.9042\n",
      "Epoch 14/100\n",
      "9000/9000 [==============================] - 1s 131us/step - loss: 0.3024 - acc: 0.9030\n",
      "Epoch 15/100\n",
      "9000/9000 [==============================] - 1s 103us/step - loss: 0.2850 - acc: 0.9073\n",
      "Epoch 16/100\n",
      "9000/9000 [==============================] - 1s 106us/step - loss: 0.2689 - acc: 0.9152\n",
      "Epoch 17/100\n",
      "9000/9000 [==============================] - 1s 146us/step - loss: 0.2636 - acc: 0.9130\n",
      "Epoch 18/100\n",
      "9000/9000 [==============================] - 1s 92us/step - loss: 0.2516 - acc: 0.9180\n",
      "Epoch 19/100\n",
      "9000/9000 [==============================] - 1s 92us/step - loss: 0.2498 - acc: 0.9156\n",
      "Epoch 20/100\n",
      "9000/9000 [==============================] - 1s 93us/step - loss: 0.2428 - acc: 0.9233\n",
      "Epoch 21/100\n",
      "9000/9000 [==============================] - 1s 92us/step - loss: 0.2317 - acc: 0.9233\n",
      "Epoch 22/100\n",
      "9000/9000 [==============================] - 1s 91us/step - loss: 0.2201 - acc: 0.9280\n",
      "Epoch 23/100\n",
      "9000/9000 [==============================] - 1s 90us/step - loss: 0.2231 - acc: 0.9242\n",
      "Epoch 24/100\n",
      "9000/9000 [==============================] - 1s 90us/step - loss: 0.2191 - acc: 0.9291\n",
      "Epoch 25/100\n",
      "9000/9000 [==============================] - 1s 91us/step - loss: 0.2128 - acc: 0.9323\n",
      "Epoch 26/100\n",
      "9000/9000 [==============================] - 1s 92us/step - loss: 0.2030 - acc: 0.9320\n",
      "Epoch 27/100\n",
      "9000/9000 [==============================] - 1s 91us/step - loss: 0.2064 - acc: 0.9310\n",
      "Epoch 28/100\n",
      "9000/9000 [==============================] - 1s 92us/step - loss: 0.1861 - acc: 0.9378\n",
      "Epoch 29/100\n",
      "9000/9000 [==============================] - 1s 111us/step - loss: 0.1958 - acc: 0.9354\n",
      "Epoch 30/100\n",
      "9000/9000 [==============================] - 1s 105us/step - loss: 0.1908 - acc: 0.9361\n",
      "Epoch 31/100\n",
      "9000/9000 [==============================] - 1s 134us/step - loss: 0.1955 - acc: 0.9338\n",
      "Epoch 32/100\n",
      "9000/9000 [==============================] - 1s 94us/step - loss: 0.1907 - acc: 0.9372\n",
      "Epoch 33/100\n",
      "9000/9000 [==============================] - 1s 91us/step - loss: 0.1770 - acc: 0.9413\n",
      "Epoch 34/100\n",
      "9000/9000 [==============================] - 1s 92us/step - loss: 0.1861 - acc: 0.9379\n",
      "Epoch 35/100\n",
      "9000/9000 [==============================] - 1s 89us/step - loss: 0.1720 - acc: 0.9426\n",
      "Epoch 36/100\n",
      "9000/9000 [==============================] - 1s 89us/step - loss: 0.1667 - acc: 0.9462\n",
      "Epoch 37/100\n",
      "9000/9000 [==============================] - 1s 89us/step - loss: 0.1628 - acc: 0.9453\n",
      "Epoch 38/100\n",
      "9000/9000 [==============================] - 1s 88us/step - loss: 0.1780 - acc: 0.9394\n",
      "Epoch 39/100\n",
      "9000/9000 [==============================] - 1s 90us/step - loss: 0.1605 - acc: 0.9489\n",
      "Epoch 40/100\n",
      "9000/9000 [==============================] - 1s 89us/step - loss: 0.1646 - acc: 0.9449\n",
      "Epoch 41/100\n",
      "9000/9000 [==============================] - 1s 90us/step - loss: 0.1655 - acc: 0.9480\n",
      "Epoch 42/100\n",
      "9000/9000 [==============================] - 1s 89us/step - loss: 0.1531 - acc: 0.9497\n",
      "Epoch 43/100\n",
      "9000/9000 [==============================] - 1s 89us/step - loss: 0.1600 - acc: 0.9488\n",
      "Epoch 44/100\n",
      "9000/9000 [==============================] - 1s 90us/step - loss: 0.1634 - acc: 0.9466\n",
      "Epoch 45/100\n",
      "9000/9000 [==============================] - 1s 89us/step - loss: 0.1533 - acc: 0.9484\n",
      "Epoch 46/100\n",
      "9000/9000 [==============================] - 1s 89us/step - loss: 0.1544 - acc: 0.9490\n",
      "Epoch 47/100\n",
      "9000/9000 [==============================] - 1s 88us/step - loss: 0.1428 - acc: 0.9502\n",
      "Epoch 48/100\n",
      "9000/9000 [==============================] - 1s 90us/step - loss: 0.1404 - acc: 0.9511\n",
      "Epoch 49/100\n",
      "9000/9000 [==============================] - 1s 89us/step - loss: 0.1540 - acc: 0.9468\n",
      "Epoch 50/100\n",
      "9000/9000 [==============================] - 1s 88us/step - loss: 0.1499 - acc: 0.9502\n",
      "Epoch 51/100\n",
      "9000/9000 [==============================] - 1s 89us/step - loss: 0.1446 - acc: 0.9509\n",
      "Epoch 52/100\n",
      "9000/9000 [==============================] - 1s 89us/step - loss: 0.1438 - acc: 0.9524\n",
      "Epoch 53/100\n",
      "9000/9000 [==============================] - 1s 88us/step - loss: 0.1290 - acc: 0.9564\n",
      "Epoch 54/100\n",
      "9000/9000 [==============================] - 1s 88us/step - loss: 0.1435 - acc: 0.9542\n",
      "Epoch 55/100\n",
      "9000/9000 [==============================] - ETA: 0s - loss: 0.1254 - acc: 0.958 - 1s 87us/step - loss: 0.1265 - acc: 0.9579\n",
      "Epoch 56/100\n",
      "9000/9000 [==============================] - 1s 93us/step - loss: 0.1334 - acc: 0.9558\n",
      "Epoch 57/100\n",
      "9000/9000 [==============================] - 1s 88us/step - loss: 0.1455 - acc: 0.9501\n",
      "Epoch 58/100\n",
      "9000/9000 [==============================] - 1s 86us/step - loss: 0.1397 - acc: 0.9529\n",
      "Epoch 59/100\n",
      "9000/9000 [==============================] - 1s 90us/step - loss: 0.1278 - acc: 0.9570\n",
      "Epoch 60/100\n",
      "9000/9000 [==============================] - 1s 90us/step - loss: 0.1275 - acc: 0.9589\n",
      "Epoch 61/100\n",
      "9000/9000 [==============================] - 1s 89us/step - loss: 0.1237 - acc: 0.9586\n",
      "Epoch 62/100\n",
      "9000/9000 [==============================] - 1s 87us/step - loss: 0.1314 - acc: 0.9568\n",
      "Epoch 63/100\n",
      "9000/9000 [==============================] - 1s 89us/step - loss: 0.1303 - acc: 0.9557\n",
      "Epoch 64/100\n",
      "9000/9000 [==============================] - 1s 87us/step - loss: 0.1231 - acc: 0.9559\n",
      "Epoch 65/100\n",
      "9000/9000 [==============================] - 1s 86us/step - loss: 0.1221 - acc: 0.9582\n",
      "Epoch 66/100\n",
      "9000/9000 [==============================] - 1s 88us/step - loss: 0.1249 - acc: 0.9568\n",
      "Epoch 67/100\n",
      "9000/9000 [==============================] - 1s 91us/step - loss: 0.1202 - acc: 0.9577\n",
      "Epoch 68/100\n",
      "9000/9000 [==============================] - 1s 88us/step - loss: 0.1235 - acc: 0.9583\n",
      "Epoch 69/100\n",
      "9000/9000 [==============================] - 1s 88us/step - loss: 0.1198 - acc: 0.9604\n",
      "Epoch 70/100\n",
      "9000/9000 [==============================] - 1s 88us/step - loss: 0.1290 - acc: 0.9562\n",
      "Epoch 71/100\n",
      "9000/9000 [==============================] - 1s 88us/step - loss: 0.1206 - acc: 0.9587\n",
      "Epoch 72/100\n",
      "9000/9000 [==============================] - 1s 88us/step - loss: 0.1232 - acc: 0.9594\n",
      "Epoch 73/100\n",
      "9000/9000 [==============================] - 1s 88us/step - loss: 0.1184 - acc: 0.9592\n",
      "Epoch 74/100\n",
      "9000/9000 [==============================] - 1s 88us/step - loss: 0.1181 - acc: 0.9619\n",
      "Epoch 75/100\n",
      "9000/9000 [==============================] - 1s 88us/step - loss: 0.1234 - acc: 0.9591\n",
      "Epoch 76/100\n",
      "9000/9000 [==============================] - 1s 88us/step - loss: 0.1228 - acc: 0.9564\n",
      "Epoch 77/100\n",
      "9000/9000 [==============================] - 1s 88us/step - loss: 0.1210 - acc: 0.9574\n",
      "Epoch 78/100\n",
      "9000/9000 [==============================] - 1s 87us/step - loss: 0.1028 - acc: 0.9658\n",
      "Epoch 79/100\n",
      "9000/9000 [==============================] - 1s 87us/step - loss: 0.1107 - acc: 0.9613\n",
      "Epoch 80/100\n",
      "9000/9000 [==============================] - 1s 87us/step - loss: 0.1233 - acc: 0.9586\n",
      "Epoch 81/100\n",
      "9000/9000 [==============================] - 1s 90us/step - loss: 0.1117 - acc: 0.9648\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000/9000 [==============================] - 1s 88us/step - loss: 0.1158 - acc: 0.9619\n",
      "Epoch 83/100\n",
      "9000/9000 [==============================] - 1s 87us/step - loss: 0.1129 - acc: 0.9630\n",
      "Epoch 84/100\n",
      "9000/9000 [==============================] - 1s 88us/step - loss: 0.1066 - acc: 0.9636\n",
      "Epoch 85/100\n",
      "9000/9000 [==============================] - 1s 88us/step - loss: 0.1092 - acc: 0.9630\n",
      "Epoch 86/100\n",
      "9000/9000 [==============================] - 1s 88us/step - loss: 0.1058 - acc: 0.9644\n",
      "Epoch 87/100\n",
      "9000/9000 [==============================] - 1s 90us/step - loss: 0.1032 - acc: 0.9653\n",
      "Epoch 88/100\n",
      "9000/9000 [==============================] - 1s 88us/step - loss: 0.1064 - acc: 0.9649\n",
      "Epoch 89/100\n",
      "9000/9000 [==============================] - 1s 88us/step - loss: 0.1014 - acc: 0.9682\n",
      "Epoch 90/100\n",
      "9000/9000 [==============================] - 1s 87us/step - loss: 0.1058 - acc: 0.9654\n",
      "Epoch 91/100\n",
      "9000/9000 [==============================] - 1s 88us/step - loss: 0.1122 - acc: 0.9622\n",
      "Epoch 92/100\n",
      "9000/9000 [==============================] - 1s 87us/step - loss: 0.1025 - acc: 0.9652\n",
      "Epoch 93/100\n",
      "9000/9000 [==============================] - 1s 88us/step - loss: 0.1006 - acc: 0.9650\n",
      "Epoch 94/100\n",
      "9000/9000 [==============================] - 1s 88us/step - loss: 0.1025 - acc: 0.9663\n",
      "Epoch 95/100\n",
      "9000/9000 [==============================] - 1s 87us/step - loss: 0.1041 - acc: 0.9643\n",
      "Epoch 96/100\n",
      "9000/9000 [==============================] - 1s 87us/step - loss: 0.1003 - acc: 0.9662\n",
      "Epoch 97/100\n",
      "9000/9000 [==============================] - 1s 88us/step - loss: 0.1081 - acc: 0.9649\n",
      "Epoch 98/100\n",
      "9000/9000 [==============================] - 1s 88us/step - loss: 0.1046 - acc: 0.9652\n",
      "Epoch 99/100\n",
      "9000/9000 [==============================] - 1s 89us/step - loss: 0.0997 - acc: 0.9661\n",
      "Epoch 100/100\n",
      "9000/9000 [==============================] - 1s 88us/step - loss: 0.1042 - acc: 0.9650\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x165ddc978>"
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_labeled = Sequential()\n",
    "in_dim = X_train_labeled.shape[1]\n",
    "out_dim = onehot_y.shape[1]\n",
    "\n",
    "classifier_labeled.add(BatchNormalization())\n",
    "# first hidden layer\n",
    "classifier_labeled.add(Dense(100, activation='relu', kernel_initializer='random_normal', input_dim=in_dim))\n",
    "classifier_labeled.add(Dropout(0.2))\n",
    "# second hidden layer\n",
    "classifier_labeled.add(Dense(100, activation='relu', kernel_initializer='random_normal'))\n",
    "classifier_labeled.add(Dropout(0.2))\n",
    "# third hidden layer\n",
    "classifier_labeled.add(Dense(100, activation='relu', kernel_initializer='random_normal'))\n",
    "classifier_labeled.add(Dropout(0.2))\n",
    "# output layer\n",
    "classifier_labeled.add(Dense(out_dim, activation='softmax', kernel_initializer='random_normal'))\n",
    "\n",
    "classifier_labeled.compile(optimizer='adam', loss='categorical_crossentropy', metrics =['accuracy'])\n",
    "classifier_labeled.fit(X_train_labeled, onehot_y, batch_size=100, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_classifier(X_train, onehot_y, n_input_neurons, N_hidden_neurons):\n",
    "    # define basic classifier model\n",
    "    classifier = Sequential()\n",
    "    in_dim = X_train.shape[1]\n",
    "    out_dim = onehot_y.shape[1]\n",
    "    # input layer\n",
    "    classifier.add(Dense(n_input_neurons, activation='relu', kernel_initializer='random_normal', input_dim=in_dim))\n",
    "    classifier.add(BatchNormalization())\n",
    "    # build hidden layers of classifier\n",
    "    for n in N_hidden_neurons:\n",
    "        # create hidden layer\n",
    "        classifier.add(Dense(n, activation='relu', kernel_initializer='random_normal'))\n",
    "        classifier.add(Dropout(0.5))\n",
    "    # output layer\n",
    "    classifier.add(Dense(out_dim, activation='softmax', kernel_initializer='random_normal'))\n",
    "    # custom SGD optimizer\n",
    "    sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics =['accuracy'])\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def augment_labeled_data(n_batches, tau, X_labeled, y, X_unlabeled, classifier):\n",
    "    # initial model\n",
    "    model = classifier\n",
    "    # initially labeled data\n",
    "    labeled_data = X_labeled.copy()\n",
    "    labels = y.copy()\n",
    "    # initial one-hot encoding for labels\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(labels)\n",
    "    encoded_y = encoder.transform(labels)\n",
    "    onehot_y = np_utils.to_categorical(encoded_y)\n",
    "    # split unlabeled data into chunks\n",
    "    batches = np.array_split(X_unlabeled, n_batches)\n",
    "    # augment labeled data by chunks\n",
    "    for i, batch in zip(range(1, n_batches+1), batches):\n",
    "        print()\n",
    "        print(\"AUGMENTING DATASET WITH BATCH #\", i, \":\")\n",
    "        print()\n",
    "        # pseudo labels for current batch\n",
    "        pseudo_y = classifier.predict(batch)\n",
    "        pseudo_maxes = np.max(pseudo_y, axis=-1)\n",
    "        pseudo_classes = np.argmax(pseudo_y, axis=-1)\n",
    "        pseudo_labels = pd.DataFrame({'y': pseudo_classes, 'confidence': pseudo_maxes})\n",
    "        # only accept labels if confidence high enough\n",
    "        labeling = pd.concat([pd.DataFrame(batch), pseudo_labels], axis=1)\n",
    "        labeling = labeling[labeling.confidence >= tau]  \n",
    "        print(\"Labels accepted: \", len(labeling), \"/\", len(batch))\n",
    "        print()\n",
    "        new_labeled_data = labeling.drop(columns=['y', 'confidence'])\n",
    "        new_labels = labeling.drop(columns=['confidence']).iloc[:,labeling.shape[1]-2:]\n",
    "        # append newly labeled batch to already labeled data\n",
    "        labeled_data = pd.concat([pd.DataFrame(labeled_data), new_labeled_data])\n",
    "        labels = pd.concat([labels, new_labels])\n",
    "        # redo one-hot encoding for labels\n",
    "        encoder.fit(labels)\n",
    "        encoded_y = encoder.transform(labels)\n",
    "        onehot_y = np_utils.to_categorical(encoded_y)\n",
    "        # train new model with augmented data\n",
    "        classifier = create_classifier(labeled_data, onehot_y, 375, [375, 375])\n",
    "        classifier.fit(labeled_data, onehot_y, batch_size=1000, epochs=30)\n",
    "    print()\n",
    "    print(\"Size of final dataset: \", labeled_data.shape)\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:95: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:128: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AUGMENTING DATASET WITH BATCH # 1 :\n",
      "\n",
      "Labels accepted:  1642 / 2100\n",
      "\n",
      "Epoch 1/30\n",
      "10642/10642 [==============================] - 6s 541us/step - loss: 1.6221 - acc: 0.4694\n",
      "Epoch 2/30\n",
      "10642/10642 [==============================] - 1s 98us/step - loss: 0.7386 - acc: 0.7657\n",
      "Epoch 3/30\n",
      "10642/10642 [==============================] - 1s 98us/step - loss: 0.5327 - acc: 0.8335\n",
      "Epoch 4/30\n",
      "10642/10642 [==============================] - 1s 99us/step - loss: 0.4302 - acc: 0.8673\n",
      "Epoch 5/30\n",
      "10642/10642 [==============================] - 1s 98us/step - loss: 0.3672 - acc: 0.8828\n",
      "Epoch 6/30\n",
      "10642/10642 [==============================] - 1s 99us/step - loss: 0.3241 - acc: 0.8982\n",
      "Epoch 7/30\n",
      "10642/10642 [==============================] - 1s 99us/step - loss: 0.2847 - acc: 0.9110\n",
      "Epoch 8/30\n",
      "10642/10642 [==============================] - 1s 98us/step - loss: 0.2563 - acc: 0.9174\n",
      "Epoch 9/30\n",
      "10642/10642 [==============================] - 1s 98us/step - loss: 0.2336 - acc: 0.9241\n",
      "Epoch 10/30\n",
      "10642/10642 [==============================] - 1s 99us/step - loss: 0.2191 - acc: 0.9303\n",
      "Epoch 11/30\n",
      "10642/10642 [==============================] - 1s 100us/step - loss: 0.1918 - acc: 0.9388\n",
      "Epoch 12/30\n",
      "10642/10642 [==============================] - 1s 97us/step - loss: 0.1774 - acc: 0.9403\n",
      "Epoch 13/30\n",
      "10642/10642 [==============================] - 1s 99us/step - loss: 0.1594 - acc: 0.9483\n",
      "Epoch 14/30\n",
      "10642/10642 [==============================] - 1s 99us/step - loss: 0.1413 - acc: 0.9536\n",
      "Epoch 15/30\n",
      "10642/10642 [==============================] - 1s 100us/step - loss: 0.1281 - acc: 0.9585\n",
      "Epoch 16/30\n",
      "10642/10642 [==============================] - 1s 98us/step - loss: 0.1267 - acc: 0.9597\n",
      "Epoch 17/30\n",
      "10642/10642 [==============================] - 1s 97us/step - loss: 0.1092 - acc: 0.9665\n",
      "Epoch 18/30\n",
      "10642/10642 [==============================] - 1s 99us/step - loss: 0.1025 - acc: 0.9653\n",
      "Epoch 19/30\n",
      "10642/10642 [==============================] - 1s 98us/step - loss: 0.0978 - acc: 0.9677\n",
      "Epoch 20/30\n",
      "10642/10642 [==============================] - 1s 99us/step - loss: 0.0882 - acc: 0.9719\n",
      "Epoch 21/30\n",
      "10642/10642 [==============================] - 1s 98us/step - loss: 0.0838 - acc: 0.9718\n",
      "Epoch 22/30\n",
      "10642/10642 [==============================] - 1s 104us/step - loss: 0.0751 - acc: 0.9736\n",
      "Epoch 23/30\n",
      "10642/10642 [==============================] - 1s 105us/step - loss: 0.0675 - acc: 0.9775\n",
      "Epoch 24/30\n",
      "10642/10642 [==============================] - 1s 100us/step - loss: 0.0622 - acc: 0.9792\n",
      "Epoch 25/30\n",
      "10642/10642 [==============================] - 1s 101us/step - loss: 0.0596 - acc: 0.9808\n",
      "Epoch 26/30\n",
      "10642/10642 [==============================] - 1s 99us/step - loss: 0.0578 - acc: 0.9791\n",
      "Epoch 27/30\n",
      "10642/10642 [==============================] - 1s 99us/step - loss: 0.0514 - acc: 0.9821\n",
      "Epoch 28/30\n",
      "10642/10642 [==============================] - 1s 99us/step - loss: 0.0488 - acc: 0.9827: 0s - loss: 0.0531 - ac\n",
      "Epoch 29/30\n",
      "10642/10642 [==============================] - 1s 100us/step - loss: 0.0400 - acc: 0.9883\n",
      "Epoch 30/30\n",
      "10642/10642 [==============================] - 1s 99us/step - loss: 0.0442 - acc: 0.9852\n",
      "\n",
      "AUGMENTING DATASET WITH BATCH # 2 :\n",
      "\n",
      "Labels accepted:  1709 / 2100\n",
      "\n",
      "Epoch 1/30\n",
      "12351/12351 [==============================] - 6s 454us/step - loss: 1.4467 - acc: 0.5238: 7s - loss: 1.9922 - acc:  - ETA: 0s - loss: 1.5214 - acc: 0.50\n",
      "Epoch 2/30\n",
      "12351/12351 [==============================] - 1s 99us/step - loss: 0.6318 - acc: 0.7992\n",
      "Epoch 3/30\n",
      "12351/12351 [==============================] - 1s 99us/step - loss: 0.4571 - acc: 0.8588\n",
      "Epoch 4/30\n",
      "12351/12351 [==============================] - 1s 100us/step - loss: 0.3737 - acc: 0.8824\n",
      "Epoch 5/30\n",
      "12351/12351 [==============================] - 1s 99us/step - loss: 0.3194 - acc: 0.9003\n",
      "Epoch 6/30\n",
      "12351/12351 [==============================] - 1s 99us/step - loss: 0.2698 - acc: 0.9149\n",
      "Epoch 7/30\n",
      "12351/12351 [==============================] - 1s 101us/step - loss: 0.2414 - acc: 0.9231 0s - loss: 0.2459 - acc\n",
      "Epoch 8/30\n",
      "12351/12351 [==============================] - 1s 102us/step - loss: 0.2156 - acc: 0.9319\n",
      "Epoch 9/30\n",
      "12351/12351 [==============================] - 1s 98us/step - loss: 0.1863 - acc: 0.9412\n",
      "Epoch 10/30\n",
      "12351/12351 [==============================] - 1s 99us/step - loss: 0.1813 - acc: 0.9423\n",
      "Epoch 11/30\n",
      "12351/12351 [==============================] - 1s 100us/step - loss: 0.1652 - acc: 0.9473\n",
      "Epoch 12/30\n",
      "12351/12351 [==============================] - 1s 101us/step - loss: 0.1505 - acc: 0.9503\n",
      "Epoch 13/30\n",
      "12351/12351 [==============================] - 1s 98us/step - loss: 0.1350 - acc: 0.9553\n",
      "Epoch 14/30\n",
      "12351/12351 [==============================] - 1s 100us/step - loss: 0.1180 - acc: 0.9643\n",
      "Epoch 15/30\n",
      "12351/12351 [==============================] - 1s 100us/step - loss: 0.1096 - acc: 0.9652\n",
      "Epoch 16/30\n",
      "12351/12351 [==============================] - 1s 115us/step - loss: 0.1066 - acc: 0.9657\n",
      "Epoch 17/30\n",
      "12351/12351 [==============================] - 1s 120us/step - loss: 0.0921 - acc: 0.9688\n",
      "Epoch 18/30\n",
      "12351/12351 [==============================] - 1s 118us/step - loss: 0.0808 - acc: 0.9747\n",
      "Epoch 19/30\n",
      "12351/12351 [==============================] - 1s 103us/step - loss: 0.0754 - acc: 0.9747 0s - loss: 0.0746 - acc: 0.974\n",
      "Epoch 20/30\n",
      "12351/12351 [==============================] - 1s 104us/step - loss: 0.0749 - acc: 0.9754 0s - loss: 0.0757 - acc: 0.\n",
      "Epoch 21/30\n",
      "12351/12351 [==============================] - 1s 101us/step - loss: 0.0598 - acc: 0.9804\n",
      "Epoch 22/30\n",
      "12351/12351 [==============================] - 1s 101us/step - loss: 0.0608 - acc: 0.9791\n",
      "Epoch 23/30\n",
      "12351/12351 [==============================] - 1s 101us/step - loss: 0.0562 - acc: 0.9818\n",
      "Epoch 24/30\n",
      "12351/12351 [==============================] - 1s 102us/step - loss: 0.0487 - acc: 0.9832\n",
      "Epoch 25/30\n",
      " 8000/12351 [==================>...........] - ETA: 0s - loss: 0.0518 - acc: 0.9817- ETA: 0s - loss: 0.0562 - acc: 0.9"
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "tau = 0.99\n",
    "classifier_augmented = augment_labeled_data(n, tau, X_train_labeled, y_train, X_train_unlabeled, classifier_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_semi = classifier_augmented.predict(X_test)\n",
    "# find most likely category from soft max\n",
    "pred_semi = np.argmax(pred_semi, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert back to pandas dataframe\n",
    "X_test = pd.DataFrame(X_test)\n",
    "pred_submit = pd.DataFrame(list(zip(X_test.index.values + 30000, pred_semi)), columns=['Id', 'y'])\n",
    "final_submit = pred_submit.to_csv(\"submit.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
