{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to ML Project\n",
    "## Task 4\n",
    "### Jan Bauer, Alaisha Sharma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils, to_categorical\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# same seed for consistency\n",
    "seed = 1\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>...</th>\n",
       "      <th>x130</th>\n",
       "      <th>x131</th>\n",
       "      <th>x132</th>\n",
       "      <th>x133</th>\n",
       "      <th>x134</th>\n",
       "      <th>x135</th>\n",
       "      <th>x136</th>\n",
       "      <th>x137</th>\n",
       "      <th>x138</th>\n",
       "      <th>x139</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0.791590</td>\n",
       "      <td>0.753101</td>\n",
       "      <td>0.577893</td>\n",
       "      <td>0.726169</td>\n",
       "      <td>0.395992</td>\n",
       "      <td>0.542416</td>\n",
       "      <td>0.482571</td>\n",
       "      <td>0.409337</td>\n",
       "      <td>0.365332</td>\n",
       "      <td>...</td>\n",
       "      <td>0.662795</td>\n",
       "      <td>0.569457</td>\n",
       "      <td>0.328729</td>\n",
       "      <td>0.356595</td>\n",
       "      <td>0.642982</td>\n",
       "      <td>0.545763</td>\n",
       "      <td>0.673734</td>\n",
       "      <td>0.487183</td>\n",
       "      <td>0.504894</td>\n",
       "      <td>0.670266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>0.798071</td>\n",
       "      <td>0.757271</td>\n",
       "      <td>0.575022</td>\n",
       "      <td>0.729636</td>\n",
       "      <td>0.400243</td>\n",
       "      <td>0.536507</td>\n",
       "      <td>0.483917</td>\n",
       "      <td>0.415249</td>\n",
       "      <td>0.374363</td>\n",
       "      <td>...</td>\n",
       "      <td>0.660528</td>\n",
       "      <td>0.573125</td>\n",
       "      <td>0.334675</td>\n",
       "      <td>0.360123</td>\n",
       "      <td>0.641473</td>\n",
       "      <td>0.552560</td>\n",
       "      <td>0.672730</td>\n",
       "      <td>0.486268</td>\n",
       "      <td>0.509508</td>\n",
       "      <td>0.670837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>0.794205</td>\n",
       "      <td>0.752145</td>\n",
       "      <td>0.570502</td>\n",
       "      <td>0.728514</td>\n",
       "      <td>0.407141</td>\n",
       "      <td>0.545053</td>\n",
       "      <td>0.482661</td>\n",
       "      <td>0.414902</td>\n",
       "      <td>0.370317</td>\n",
       "      <td>...</td>\n",
       "      <td>0.667669</td>\n",
       "      <td>0.576651</td>\n",
       "      <td>0.325308</td>\n",
       "      <td>0.365656</td>\n",
       "      <td>0.640514</td>\n",
       "      <td>0.568385</td>\n",
       "      <td>0.672446</td>\n",
       "      <td>0.485341</td>\n",
       "      <td>0.500234</td>\n",
       "      <td>0.678961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.793530</td>\n",
       "      <td>0.752631</td>\n",
       "      <td>0.575777</td>\n",
       "      <td>0.723480</td>\n",
       "      <td>0.395984</td>\n",
       "      <td>0.548614</td>\n",
       "      <td>0.488048</td>\n",
       "      <td>0.412096</td>\n",
       "      <td>0.373032</td>\n",
       "      <td>...</td>\n",
       "      <td>0.663633</td>\n",
       "      <td>0.574640</td>\n",
       "      <td>0.332292</td>\n",
       "      <td>0.361036</td>\n",
       "      <td>0.642928</td>\n",
       "      <td>0.545435</td>\n",
       "      <td>0.671515</td>\n",
       "      <td>0.481010</td>\n",
       "      <td>0.509037</td>\n",
       "      <td>0.673317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.794775</td>\n",
       "      <td>0.745556</td>\n",
       "      <td>0.570395</td>\n",
       "      <td>0.724464</td>\n",
       "      <td>0.399826</td>\n",
       "      <td>0.546392</td>\n",
       "      <td>0.490428</td>\n",
       "      <td>0.412121</td>\n",
       "      <td>0.365301</td>\n",
       "      <td>...</td>\n",
       "      <td>0.666438</td>\n",
       "      <td>0.578923</td>\n",
       "      <td>0.327307</td>\n",
       "      <td>0.370091</td>\n",
       "      <td>0.648864</td>\n",
       "      <td>0.557257</td>\n",
       "      <td>0.673051</td>\n",
       "      <td>0.481574</td>\n",
       "      <td>0.503186</td>\n",
       "      <td>0.674417</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 140 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   y        x1        x2        x3        x4        x5        x6        x7  \\\n",
       "0  5  0.791590  0.753101  0.577893  0.726169  0.395992  0.542416  0.482571   \n",
       "1  8  0.798071  0.757271  0.575022  0.729636  0.400243  0.536507  0.483917   \n",
       "2  7  0.794205  0.752145  0.570502  0.728514  0.407141  0.545053  0.482661   \n",
       "3  2  0.793530  0.752631  0.575777  0.723480  0.395984  0.548614  0.488048   \n",
       "4  1  0.794775  0.745556  0.570395  0.724464  0.399826  0.546392  0.490428   \n",
       "\n",
       "         x8        x9  ...      x130      x131      x132      x133      x134  \\\n",
       "0  0.409337  0.365332  ...  0.662795  0.569457  0.328729  0.356595  0.642982   \n",
       "1  0.415249  0.374363  ...  0.660528  0.573125  0.334675  0.360123  0.641473   \n",
       "2  0.414902  0.370317  ...  0.667669  0.576651  0.325308  0.365656  0.640514   \n",
       "3  0.412096  0.373032  ...  0.663633  0.574640  0.332292  0.361036  0.642928   \n",
       "4  0.412121  0.365301  ...  0.666438  0.578923  0.327307  0.370091  0.648864   \n",
       "\n",
       "       x135      x136      x137      x138      x139  \n",
       "0  0.545763  0.673734  0.487183  0.504894  0.670266  \n",
       "1  0.552560  0.672730  0.486268  0.509508  0.670837  \n",
       "2  0.568385  0.672446  0.485341  0.500234  0.678961  \n",
       "3  0.545435  0.671515  0.481010  0.509037  0.673317  \n",
       "4  0.557257  0.673051  0.481574  0.503186  0.674417  \n",
       "\n",
       "[5 rows x 140 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_labeled = pd.read_hdf(\"data/train_labeled.h5\", \"train\")\n",
    "train_data_labeled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "      <th>...</th>\n",
       "      <th>x130</th>\n",
       "      <th>x131</th>\n",
       "      <th>x132</th>\n",
       "      <th>x133</th>\n",
       "      <th>x134</th>\n",
       "      <th>x135</th>\n",
       "      <th>x136</th>\n",
       "      <th>x137</th>\n",
       "      <th>x138</th>\n",
       "      <th>x139</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.791590</td>\n",
       "      <td>0.753101</td>\n",
       "      <td>0.577893</td>\n",
       "      <td>0.726169</td>\n",
       "      <td>0.395992</td>\n",
       "      <td>0.542416</td>\n",
       "      <td>0.482571</td>\n",
       "      <td>0.409337</td>\n",
       "      <td>0.365332</td>\n",
       "      <td>0.163388</td>\n",
       "      <td>...</td>\n",
       "      <td>0.662795</td>\n",
       "      <td>0.569457</td>\n",
       "      <td>0.328729</td>\n",
       "      <td>0.356595</td>\n",
       "      <td>0.642982</td>\n",
       "      <td>0.545763</td>\n",
       "      <td>0.673734</td>\n",
       "      <td>0.487183</td>\n",
       "      <td>0.504894</td>\n",
       "      <td>0.670266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.798071</td>\n",
       "      <td>0.757271</td>\n",
       "      <td>0.575022</td>\n",
       "      <td>0.729636</td>\n",
       "      <td>0.400243</td>\n",
       "      <td>0.536507</td>\n",
       "      <td>0.483917</td>\n",
       "      <td>0.415249</td>\n",
       "      <td>0.374363</td>\n",
       "      <td>0.159272</td>\n",
       "      <td>...</td>\n",
       "      <td>0.660528</td>\n",
       "      <td>0.573125</td>\n",
       "      <td>0.334675</td>\n",
       "      <td>0.360123</td>\n",
       "      <td>0.641473</td>\n",
       "      <td>0.552560</td>\n",
       "      <td>0.672730</td>\n",
       "      <td>0.486268</td>\n",
       "      <td>0.509508</td>\n",
       "      <td>0.670837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.794205</td>\n",
       "      <td>0.752145</td>\n",
       "      <td>0.570502</td>\n",
       "      <td>0.728514</td>\n",
       "      <td>0.407141</td>\n",
       "      <td>0.545053</td>\n",
       "      <td>0.482661</td>\n",
       "      <td>0.414902</td>\n",
       "      <td>0.370317</td>\n",
       "      <td>0.164221</td>\n",
       "      <td>...</td>\n",
       "      <td>0.667669</td>\n",
       "      <td>0.576651</td>\n",
       "      <td>0.325308</td>\n",
       "      <td>0.365656</td>\n",
       "      <td>0.640514</td>\n",
       "      <td>0.568385</td>\n",
       "      <td>0.672446</td>\n",
       "      <td>0.485341</td>\n",
       "      <td>0.500234</td>\n",
       "      <td>0.678961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.793530</td>\n",
       "      <td>0.752631</td>\n",
       "      <td>0.575777</td>\n",
       "      <td>0.723480</td>\n",
       "      <td>0.395984</td>\n",
       "      <td>0.548614</td>\n",
       "      <td>0.488048</td>\n",
       "      <td>0.412096</td>\n",
       "      <td>0.373032</td>\n",
       "      <td>0.160413</td>\n",
       "      <td>...</td>\n",
       "      <td>0.663633</td>\n",
       "      <td>0.574640</td>\n",
       "      <td>0.332292</td>\n",
       "      <td>0.361036</td>\n",
       "      <td>0.642928</td>\n",
       "      <td>0.545435</td>\n",
       "      <td>0.671515</td>\n",
       "      <td>0.481010</td>\n",
       "      <td>0.509037</td>\n",
       "      <td>0.673317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.794775</td>\n",
       "      <td>0.745556</td>\n",
       "      <td>0.570395</td>\n",
       "      <td>0.724464</td>\n",
       "      <td>0.399826</td>\n",
       "      <td>0.546392</td>\n",
       "      <td>0.490428</td>\n",
       "      <td>0.412121</td>\n",
       "      <td>0.365301</td>\n",
       "      <td>0.161424</td>\n",
       "      <td>...</td>\n",
       "      <td>0.666438</td>\n",
       "      <td>0.578923</td>\n",
       "      <td>0.327307</td>\n",
       "      <td>0.370091</td>\n",
       "      <td>0.648864</td>\n",
       "      <td>0.557257</td>\n",
       "      <td>0.673051</td>\n",
       "      <td>0.481574</td>\n",
       "      <td>0.503186</td>\n",
       "      <td>0.674417</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 139 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         x1        x2        x3        x4        x5        x6        x7  \\\n",
       "0  0.791590  0.753101  0.577893  0.726169  0.395992  0.542416  0.482571   \n",
       "1  0.798071  0.757271  0.575022  0.729636  0.400243  0.536507  0.483917   \n",
       "2  0.794205  0.752145  0.570502  0.728514  0.407141  0.545053  0.482661   \n",
       "3  0.793530  0.752631  0.575777  0.723480  0.395984  0.548614  0.488048   \n",
       "4  0.794775  0.745556  0.570395  0.724464  0.399826  0.546392  0.490428   \n",
       "\n",
       "         x8        x9       x10  ...      x130      x131      x132      x133  \\\n",
       "0  0.409337  0.365332  0.163388  ...  0.662795  0.569457  0.328729  0.356595   \n",
       "1  0.415249  0.374363  0.159272  ...  0.660528  0.573125  0.334675  0.360123   \n",
       "2  0.414902  0.370317  0.164221  ...  0.667669  0.576651  0.325308  0.365656   \n",
       "3  0.412096  0.373032  0.160413  ...  0.663633  0.574640  0.332292  0.361036   \n",
       "4  0.412121  0.365301  0.161424  ...  0.666438  0.578923  0.327307  0.370091   \n",
       "\n",
       "       x134      x135      x136      x137      x138      x139  \n",
       "0  0.642982  0.545763  0.673734  0.487183  0.504894  0.670266  \n",
       "1  0.641473  0.552560  0.672730  0.486268  0.509508  0.670837  \n",
       "2  0.640514  0.568385  0.672446  0.485341  0.500234  0.678961  \n",
       "3  0.642928  0.545435  0.671515  0.481010  0.509037  0.673317  \n",
       "4  0.648864  0.557257  0.673051  0.481574  0.503186  0.674417  \n",
       "\n",
       "[5 rows x 139 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_labeled = train_data_labeled.iloc[:,1:]\n",
    "X_train_labeled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   y\n",
       "0  5\n",
       "1  8\n",
       "2  7\n",
       "3  2\n",
       "4  1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = train_data_labeled.iloc[:,0:1]\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "      <th>...</th>\n",
       "      <th>x130</th>\n",
       "      <th>x131</th>\n",
       "      <th>x132</th>\n",
       "      <th>x133</th>\n",
       "      <th>x134</th>\n",
       "      <th>x135</th>\n",
       "      <th>x136</th>\n",
       "      <th>x137</th>\n",
       "      <th>x138</th>\n",
       "      <th>x139</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9000</th>\n",
       "      <td>0.802073</td>\n",
       "      <td>0.751775</td>\n",
       "      <td>0.572571</td>\n",
       "      <td>0.729820</td>\n",
       "      <td>0.401511</td>\n",
       "      <td>0.546931</td>\n",
       "      <td>0.483566</td>\n",
       "      <td>0.409619</td>\n",
       "      <td>0.369915</td>\n",
       "      <td>0.162539</td>\n",
       "      <td>...</td>\n",
       "      <td>0.663145</td>\n",
       "      <td>0.578450</td>\n",
       "      <td>0.325368</td>\n",
       "      <td>0.363988</td>\n",
       "      <td>0.640859</td>\n",
       "      <td>0.563103</td>\n",
       "      <td>0.673581</td>\n",
       "      <td>0.480801</td>\n",
       "      <td>0.506701</td>\n",
       "      <td>0.670141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9001</th>\n",
       "      <td>0.796261</td>\n",
       "      <td>0.747018</td>\n",
       "      <td>0.576803</td>\n",
       "      <td>0.720181</td>\n",
       "      <td>0.397541</td>\n",
       "      <td>0.543298</td>\n",
       "      <td>0.490455</td>\n",
       "      <td>0.410719</td>\n",
       "      <td>0.365985</td>\n",
       "      <td>0.159075</td>\n",
       "      <td>...</td>\n",
       "      <td>0.667465</td>\n",
       "      <td>0.574655</td>\n",
       "      <td>0.331916</td>\n",
       "      <td>0.366610</td>\n",
       "      <td>0.644864</td>\n",
       "      <td>0.544353</td>\n",
       "      <td>0.670754</td>\n",
       "      <td>0.485595</td>\n",
       "      <td>0.507704</td>\n",
       "      <td>0.670917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9002</th>\n",
       "      <td>0.794176</td>\n",
       "      <td>0.751640</td>\n",
       "      <td>0.570850</td>\n",
       "      <td>0.722511</td>\n",
       "      <td>0.401524</td>\n",
       "      <td>0.538023</td>\n",
       "      <td>0.481063</td>\n",
       "      <td>0.413562</td>\n",
       "      <td>0.371465</td>\n",
       "      <td>0.158596</td>\n",
       "      <td>...</td>\n",
       "      <td>0.659293</td>\n",
       "      <td>0.568071</td>\n",
       "      <td>0.327978</td>\n",
       "      <td>0.357855</td>\n",
       "      <td>0.643347</td>\n",
       "      <td>0.558021</td>\n",
       "      <td>0.674759</td>\n",
       "      <td>0.486917</td>\n",
       "      <td>0.507812</td>\n",
       "      <td>0.681148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9003</th>\n",
       "      <td>0.795145</td>\n",
       "      <td>0.753813</td>\n",
       "      <td>0.577260</td>\n",
       "      <td>0.727300</td>\n",
       "      <td>0.399132</td>\n",
       "      <td>0.540990</td>\n",
       "      <td>0.483486</td>\n",
       "      <td>0.410847</td>\n",
       "      <td>0.367753</td>\n",
       "      <td>0.163416</td>\n",
       "      <td>...</td>\n",
       "      <td>0.663264</td>\n",
       "      <td>0.573702</td>\n",
       "      <td>0.326960</td>\n",
       "      <td>0.356632</td>\n",
       "      <td>0.648938</td>\n",
       "      <td>0.553826</td>\n",
       "      <td>0.674693</td>\n",
       "      <td>0.494589</td>\n",
       "      <td>0.503478</td>\n",
       "      <td>0.675546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9004</th>\n",
       "      <td>0.795453</td>\n",
       "      <td>0.749442</td>\n",
       "      <td>0.574719</td>\n",
       "      <td>0.717568</td>\n",
       "      <td>0.395512</td>\n",
       "      <td>0.548442</td>\n",
       "      <td>0.489007</td>\n",
       "      <td>0.413862</td>\n",
       "      <td>0.365941</td>\n",
       "      <td>0.161363</td>\n",
       "      <td>...</td>\n",
       "      <td>0.660846</td>\n",
       "      <td>0.577899</td>\n",
       "      <td>0.333309</td>\n",
       "      <td>0.363624</td>\n",
       "      <td>0.647993</td>\n",
       "      <td>0.541827</td>\n",
       "      <td>0.675881</td>\n",
       "      <td>0.486793</td>\n",
       "      <td>0.508442</td>\n",
       "      <td>0.664697</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 139 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            x1        x2        x3        x4        x5        x6        x7  \\\n",
       "9000  0.802073  0.751775  0.572571  0.729820  0.401511  0.546931  0.483566   \n",
       "9001  0.796261  0.747018  0.576803  0.720181  0.397541  0.543298  0.490455   \n",
       "9002  0.794176  0.751640  0.570850  0.722511  0.401524  0.538023  0.481063   \n",
       "9003  0.795145  0.753813  0.577260  0.727300  0.399132  0.540990  0.483486   \n",
       "9004  0.795453  0.749442  0.574719  0.717568  0.395512  0.548442  0.489007   \n",
       "\n",
       "            x8        x9       x10  ...      x130      x131      x132  \\\n",
       "9000  0.409619  0.369915  0.162539  ...  0.663145  0.578450  0.325368   \n",
       "9001  0.410719  0.365985  0.159075  ...  0.667465  0.574655  0.331916   \n",
       "9002  0.413562  0.371465  0.158596  ...  0.659293  0.568071  0.327978   \n",
       "9003  0.410847  0.367753  0.163416  ...  0.663264  0.573702  0.326960   \n",
       "9004  0.413862  0.365941  0.161363  ...  0.660846  0.577899  0.333309   \n",
       "\n",
       "          x133      x134      x135      x136      x137      x138      x139  \n",
       "9000  0.363988  0.640859  0.563103  0.673581  0.480801  0.506701  0.670141  \n",
       "9001  0.366610  0.644864  0.544353  0.670754  0.485595  0.507704  0.670917  \n",
       "9002  0.357855  0.643347  0.558021  0.674759  0.486917  0.507812  0.681148  \n",
       "9003  0.356632  0.648938  0.553826  0.674693  0.494589  0.503478  0.675546  \n",
       "9004  0.363624  0.647993  0.541827  0.675881  0.486793  0.508442  0.664697  \n",
       "\n",
       "[5 rows x 139 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_unlabeled = pd.read_hdf(\"data/train_unlabeled.h5\", \"train\")\n",
    "train_data_unlabeled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "      <th>...</th>\n",
       "      <th>x130</th>\n",
       "      <th>x131</th>\n",
       "      <th>x132</th>\n",
       "      <th>x133</th>\n",
       "      <th>x134</th>\n",
       "      <th>x135</th>\n",
       "      <th>x136</th>\n",
       "      <th>x137</th>\n",
       "      <th>x138</th>\n",
       "      <th>x139</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9000</th>\n",
       "      <td>0.802073</td>\n",
       "      <td>0.751775</td>\n",
       "      <td>0.572571</td>\n",
       "      <td>0.729820</td>\n",
       "      <td>0.401511</td>\n",
       "      <td>0.546931</td>\n",
       "      <td>0.483566</td>\n",
       "      <td>0.409619</td>\n",
       "      <td>0.369915</td>\n",
       "      <td>0.162539</td>\n",
       "      <td>...</td>\n",
       "      <td>0.663145</td>\n",
       "      <td>0.578450</td>\n",
       "      <td>0.325368</td>\n",
       "      <td>0.363988</td>\n",
       "      <td>0.640859</td>\n",
       "      <td>0.563103</td>\n",
       "      <td>0.673581</td>\n",
       "      <td>0.480801</td>\n",
       "      <td>0.506701</td>\n",
       "      <td>0.670141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9001</th>\n",
       "      <td>0.796261</td>\n",
       "      <td>0.747018</td>\n",
       "      <td>0.576803</td>\n",
       "      <td>0.720181</td>\n",
       "      <td>0.397541</td>\n",
       "      <td>0.543298</td>\n",
       "      <td>0.490455</td>\n",
       "      <td>0.410719</td>\n",
       "      <td>0.365985</td>\n",
       "      <td>0.159075</td>\n",
       "      <td>...</td>\n",
       "      <td>0.667465</td>\n",
       "      <td>0.574655</td>\n",
       "      <td>0.331916</td>\n",
       "      <td>0.366610</td>\n",
       "      <td>0.644864</td>\n",
       "      <td>0.544353</td>\n",
       "      <td>0.670754</td>\n",
       "      <td>0.485595</td>\n",
       "      <td>0.507704</td>\n",
       "      <td>0.670917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9002</th>\n",
       "      <td>0.794176</td>\n",
       "      <td>0.751640</td>\n",
       "      <td>0.570850</td>\n",
       "      <td>0.722511</td>\n",
       "      <td>0.401524</td>\n",
       "      <td>0.538023</td>\n",
       "      <td>0.481063</td>\n",
       "      <td>0.413562</td>\n",
       "      <td>0.371465</td>\n",
       "      <td>0.158596</td>\n",
       "      <td>...</td>\n",
       "      <td>0.659293</td>\n",
       "      <td>0.568071</td>\n",
       "      <td>0.327978</td>\n",
       "      <td>0.357855</td>\n",
       "      <td>0.643347</td>\n",
       "      <td>0.558021</td>\n",
       "      <td>0.674759</td>\n",
       "      <td>0.486917</td>\n",
       "      <td>0.507812</td>\n",
       "      <td>0.681148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9003</th>\n",
       "      <td>0.795145</td>\n",
       "      <td>0.753813</td>\n",
       "      <td>0.577260</td>\n",
       "      <td>0.727300</td>\n",
       "      <td>0.399132</td>\n",
       "      <td>0.540990</td>\n",
       "      <td>0.483486</td>\n",
       "      <td>0.410847</td>\n",
       "      <td>0.367753</td>\n",
       "      <td>0.163416</td>\n",
       "      <td>...</td>\n",
       "      <td>0.663264</td>\n",
       "      <td>0.573702</td>\n",
       "      <td>0.326960</td>\n",
       "      <td>0.356632</td>\n",
       "      <td>0.648938</td>\n",
       "      <td>0.553826</td>\n",
       "      <td>0.674693</td>\n",
       "      <td>0.494589</td>\n",
       "      <td>0.503478</td>\n",
       "      <td>0.675546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9004</th>\n",
       "      <td>0.795453</td>\n",
       "      <td>0.749442</td>\n",
       "      <td>0.574719</td>\n",
       "      <td>0.717568</td>\n",
       "      <td>0.395512</td>\n",
       "      <td>0.548442</td>\n",
       "      <td>0.489007</td>\n",
       "      <td>0.413862</td>\n",
       "      <td>0.365941</td>\n",
       "      <td>0.161363</td>\n",
       "      <td>...</td>\n",
       "      <td>0.660846</td>\n",
       "      <td>0.577899</td>\n",
       "      <td>0.333309</td>\n",
       "      <td>0.363624</td>\n",
       "      <td>0.647993</td>\n",
       "      <td>0.541827</td>\n",
       "      <td>0.675881</td>\n",
       "      <td>0.486793</td>\n",
       "      <td>0.508442</td>\n",
       "      <td>0.664697</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 139 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            x1        x2        x3        x4        x5        x6        x7  \\\n",
       "9000  0.802073  0.751775  0.572571  0.729820  0.401511  0.546931  0.483566   \n",
       "9001  0.796261  0.747018  0.576803  0.720181  0.397541  0.543298  0.490455   \n",
       "9002  0.794176  0.751640  0.570850  0.722511  0.401524  0.538023  0.481063   \n",
       "9003  0.795145  0.753813  0.577260  0.727300  0.399132  0.540990  0.483486   \n",
       "9004  0.795453  0.749442  0.574719  0.717568  0.395512  0.548442  0.489007   \n",
       "\n",
       "            x8        x9       x10  ...      x130      x131      x132  \\\n",
       "9000  0.409619  0.369915  0.162539  ...  0.663145  0.578450  0.325368   \n",
       "9001  0.410719  0.365985  0.159075  ...  0.667465  0.574655  0.331916   \n",
       "9002  0.413562  0.371465  0.158596  ...  0.659293  0.568071  0.327978   \n",
       "9003  0.410847  0.367753  0.163416  ...  0.663264  0.573702  0.326960   \n",
       "9004  0.413862  0.365941  0.161363  ...  0.660846  0.577899  0.333309   \n",
       "\n",
       "          x133      x134      x135      x136      x137      x138      x139  \n",
       "9000  0.363988  0.640859  0.563103  0.673581  0.480801  0.506701  0.670141  \n",
       "9001  0.366610  0.644864  0.544353  0.670754  0.485595  0.507704  0.670917  \n",
       "9002  0.357855  0.643347  0.558021  0.674759  0.486917  0.507812  0.681148  \n",
       "9003  0.356632  0.648938  0.553826  0.674693  0.494589  0.503478  0.675546  \n",
       "9004  0.363624  0.647993  0.541827  0.675881  0.486793  0.508442  0.664697  \n",
       "\n",
       "[5 rows x 139 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_unlabeled = train_data_unlabeled.iloc[:,:]\n",
    "X_train_unlabeled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "      <th>...</th>\n",
       "      <th>x130</th>\n",
       "      <th>x131</th>\n",
       "      <th>x132</th>\n",
       "      <th>x133</th>\n",
       "      <th>x134</th>\n",
       "      <th>x135</th>\n",
       "      <th>x136</th>\n",
       "      <th>x137</th>\n",
       "      <th>x138</th>\n",
       "      <th>x139</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30000</th>\n",
       "      <td>0.795621</td>\n",
       "      <td>0.753163</td>\n",
       "      <td>0.572225</td>\n",
       "      <td>0.727620</td>\n",
       "      <td>0.400279</td>\n",
       "      <td>0.541950</td>\n",
       "      <td>0.478941</td>\n",
       "      <td>0.414370</td>\n",
       "      <td>0.371107</td>\n",
       "      <td>0.164238</td>\n",
       "      <td>...</td>\n",
       "      <td>0.664335</td>\n",
       "      <td>0.571888</td>\n",
       "      <td>0.325982</td>\n",
       "      <td>0.359063</td>\n",
       "      <td>0.639611</td>\n",
       "      <td>0.553841</td>\n",
       "      <td>0.674134</td>\n",
       "      <td>0.484140</td>\n",
       "      <td>0.510139</td>\n",
       "      <td>0.668640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30001</th>\n",
       "      <td>0.793922</td>\n",
       "      <td>0.745725</td>\n",
       "      <td>0.571753</td>\n",
       "      <td>0.721803</td>\n",
       "      <td>0.399201</td>\n",
       "      <td>0.548342</td>\n",
       "      <td>0.483106</td>\n",
       "      <td>0.410964</td>\n",
       "      <td>0.369730</td>\n",
       "      <td>0.160543</td>\n",
       "      <td>...</td>\n",
       "      <td>0.668972</td>\n",
       "      <td>0.579854</td>\n",
       "      <td>0.336250</td>\n",
       "      <td>0.369700</td>\n",
       "      <td>0.647012</td>\n",
       "      <td>0.552805</td>\n",
       "      <td>0.673593</td>\n",
       "      <td>0.483753</td>\n",
       "      <td>0.509276</td>\n",
       "      <td>0.675991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30002</th>\n",
       "      <td>0.791894</td>\n",
       "      <td>0.757128</td>\n",
       "      <td>0.573741</td>\n",
       "      <td>0.724121</td>\n",
       "      <td>0.401164</td>\n",
       "      <td>0.547744</td>\n",
       "      <td>0.481351</td>\n",
       "      <td>0.415416</td>\n",
       "      <td>0.368478</td>\n",
       "      <td>0.161942</td>\n",
       "      <td>...</td>\n",
       "      <td>0.663038</td>\n",
       "      <td>0.568997</td>\n",
       "      <td>0.328963</td>\n",
       "      <td>0.355481</td>\n",
       "      <td>0.641702</td>\n",
       "      <td>0.551018</td>\n",
       "      <td>0.675904</td>\n",
       "      <td>0.488888</td>\n",
       "      <td>0.506695</td>\n",
       "      <td>0.675925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30003</th>\n",
       "      <td>0.794773</td>\n",
       "      <td>0.747188</td>\n",
       "      <td>0.571375</td>\n",
       "      <td>0.719419</td>\n",
       "      <td>0.398849</td>\n",
       "      <td>0.541140</td>\n",
       "      <td>0.486918</td>\n",
       "      <td>0.422196</td>\n",
       "      <td>0.371877</td>\n",
       "      <td>0.160025</td>\n",
       "      <td>...</td>\n",
       "      <td>0.665051</td>\n",
       "      <td>0.578397</td>\n",
       "      <td>0.332610</td>\n",
       "      <td>0.370070</td>\n",
       "      <td>0.643746</td>\n",
       "      <td>0.550737</td>\n",
       "      <td>0.674714</td>\n",
       "      <td>0.487900</td>\n",
       "      <td>0.509579</td>\n",
       "      <td>0.672635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30004</th>\n",
       "      <td>0.796351</td>\n",
       "      <td>0.751545</td>\n",
       "      <td>0.569544</td>\n",
       "      <td>0.718811</td>\n",
       "      <td>0.401796</td>\n",
       "      <td>0.543482</td>\n",
       "      <td>0.484521</td>\n",
       "      <td>0.420023</td>\n",
       "      <td>0.369082</td>\n",
       "      <td>0.158959</td>\n",
       "      <td>...</td>\n",
       "      <td>0.663495</td>\n",
       "      <td>0.577053</td>\n",
       "      <td>0.332620</td>\n",
       "      <td>0.366809</td>\n",
       "      <td>0.641575</td>\n",
       "      <td>0.545131</td>\n",
       "      <td>0.678834</td>\n",
       "      <td>0.488806</td>\n",
       "      <td>0.512001</td>\n",
       "      <td>0.672110</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 139 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             x1        x2        x3        x4        x5        x6        x7  \\\n",
       "30000  0.795621  0.753163  0.572225  0.727620  0.400279  0.541950  0.478941   \n",
       "30001  0.793922  0.745725  0.571753  0.721803  0.399201  0.548342  0.483106   \n",
       "30002  0.791894  0.757128  0.573741  0.724121  0.401164  0.547744  0.481351   \n",
       "30003  0.794773  0.747188  0.571375  0.719419  0.398849  0.541140  0.486918   \n",
       "30004  0.796351  0.751545  0.569544  0.718811  0.401796  0.543482  0.484521   \n",
       "\n",
       "             x8        x9       x10  ...      x130      x131      x132  \\\n",
       "30000  0.414370  0.371107  0.164238  ...  0.664335  0.571888  0.325982   \n",
       "30001  0.410964  0.369730  0.160543  ...  0.668972  0.579854  0.336250   \n",
       "30002  0.415416  0.368478  0.161942  ...  0.663038  0.568997  0.328963   \n",
       "30003  0.422196  0.371877  0.160025  ...  0.665051  0.578397  0.332610   \n",
       "30004  0.420023  0.369082  0.158959  ...  0.663495  0.577053  0.332620   \n",
       "\n",
       "           x133      x134      x135      x136      x137      x138      x139  \n",
       "30000  0.359063  0.639611  0.553841  0.674134  0.484140  0.510139  0.668640  \n",
       "30001  0.369700  0.647012  0.552805  0.673593  0.483753  0.509276  0.675991  \n",
       "30002  0.355481  0.641702  0.551018  0.675904  0.488888  0.506695  0.675925  \n",
       "30003  0.370070  0.643746  0.550737  0.674714  0.487900  0.509579  0.672635  \n",
       "30004  0.366809  0.641575  0.545131  0.678834  0.488806  0.512001  0.672110  \n",
       "\n",
       "[5 rows x 139 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = pd.read_hdf(\"data/test.h5\", \"test\")\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()  \n",
    "scaler.fit(X_train_labeled)  \n",
    "X_train_labeled = scaler.transform(X_train_labeled)  \n",
    "X_train_unlabeled = scaler.transform(X_train_unlabeled)\n",
    "X_test = scaler.transform(X_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pca = PCA(n_components=139)\n",
    "pca.fit(X_train_labeled)\n",
    "\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Cumulative explained variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.07324126,  0.69622899, -0.78104536, ..., -0.15588816,\n",
       "         1.00031779, -1.15915928],\n",
       "       [-0.73751054, -1.89980251, -0.91036259, ..., -0.2728759 ,\n",
       "         0.76683616,  0.78237649],\n",
       "       [-1.53001137,  2.08040879, -0.36501964, ...,  1.27764129,\n",
       "         0.06909848,  0.76479385],\n",
       "       ...,\n",
       "       [ 0.17201557, -0.32908476, -2.41818607, ..., -1.32229216,\n",
       "        -0.86539205,  1.88498268],\n",
       "       [ 0.3435113 , -1.18117579, -1.39517547, ...,  0.01855413,\n",
       "        -1.24839926,  0.9765096 ],\n",
       "       [ 1.55952108,  0.64434123,  1.89377247, ..., -0.63889337,\n",
       "         0.06771264, -2.22619093]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 40\n",
    "pca = PCA(n_components=k)\n",
    "X_train_labeled_pca = pca.fit_transform(X_train_labeled)\n",
    "\n",
    "X_train_unlabeled_pca = pca.transform(X_train_unlabeled)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "pca_std = np.std(X_train_labeled_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:95: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:128: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_train)\n",
    "encoded_y = encoder.transform(y_train)\n",
    "onehot_y = np_utils.to_categorical(encoded_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "9000/9000 [==============================] - 1s 77us/step - loss: 2.1025 - acc: 0.1964\n",
      "Epoch 2/100\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 1.4556 - acc: 0.3993\n",
      "Epoch 3/100\n",
      "9000/9000 [==============================] - 0s 34us/step - loss: 1.0221 - acc: 0.6620\n",
      "Epoch 4/100\n",
      "9000/9000 [==============================] - 0s 45us/step - loss: 0.7260 - acc: 0.7738\n",
      "Epoch 5/100\n",
      "9000/9000 [==============================] - 0s 45us/step - loss: 0.6091 - acc: 0.8113\n",
      "Epoch 6/100\n",
      "9000/9000 [==============================] - 0s 34us/step - loss: 0.5522 - acc: 0.8321\n",
      "Epoch 7/100\n",
      "9000/9000 [==============================] - 0s 35us/step - loss: 0.5058 - acc: 0.8481\n",
      "Epoch 8/100\n",
      "9000/9000 [==============================] - 0s 37us/step - loss: 0.4717 - acc: 0.8600\n",
      "Epoch 9/100\n",
      "9000/9000 [==============================] - 0s 37us/step - loss: 0.4470 - acc: 0.8644\n",
      "Epoch 10/100\n",
      "9000/9000 [==============================] - 0s 35us/step - loss: 0.4203 - acc: 0.8742\n",
      "Epoch 11/100\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.4049 - acc: 0.8746\n",
      "Epoch 12/100\n",
      "9000/9000 [==============================] - 0s 29us/step - loss: 0.3830 - acc: 0.8811\n",
      "Epoch 13/100\n",
      "9000/9000 [==============================] - 0s 32us/step - loss: 0.3690 - acc: 0.8873\n",
      "Epoch 14/100\n",
      "9000/9000 [==============================] - 0s 45us/step - loss: 0.3548 - acc: 0.8911\n",
      "Epoch 15/100\n",
      "9000/9000 [==============================] - 0s 37us/step - loss: 0.3394 - acc: 0.8951\n",
      "Epoch 16/100\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.3322 - acc: 0.8980\n",
      "Epoch 17/100\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 0.3208 - acc: 0.9022\n",
      "Epoch 18/100\n",
      "9000/9000 [==============================] - 0s 52us/step - loss: 0.3097 - acc: 0.9058\n",
      "Epoch 19/100\n",
      "9000/9000 [==============================] - 0s 41us/step - loss: 0.3019 - acc: 0.9087\n",
      "Epoch 20/100\n",
      "9000/9000 [==============================] - 0s 38us/step - loss: 0.2957 - acc: 0.9087\n",
      "Epoch 21/100\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.2872 - acc: 0.9113\n",
      "Epoch 22/100\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.2801 - acc: 0.9136\n",
      "Epoch 23/100\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 0.2727 - acc: 0.9158\n",
      "Epoch 24/100\n",
      "9000/9000 [==============================] - 0s 37us/step - loss: 0.2652 - acc: 0.9176\n",
      "Epoch 25/100\n",
      "9000/9000 [==============================] - 0s 41us/step - loss: 0.2577 - acc: 0.9188\n",
      "Epoch 26/100\n",
      "9000/9000 [==============================] - 0s 37us/step - loss: 0.2528 - acc: 0.9190\n",
      "Epoch 27/100\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.2466 - acc: 0.9218\n",
      "Epoch 28/100\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.2375 - acc: 0.9247\n",
      "Epoch 29/100\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.2368 - acc: 0.9253\n",
      "Epoch 30/100\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.2304 - acc: 0.9273\n",
      "Epoch 31/100\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.2316 - acc: 0.9271\n",
      "Epoch 32/100\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.2200 - acc: 0.9307\n",
      "Epoch 33/100\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.2159 - acc: 0.9312\n",
      "Epoch 34/100\n",
      "9000/9000 [==============================] - 0s 45us/step - loss: 0.2115 - acc: 0.9324\n",
      "Epoch 35/100\n",
      "9000/9000 [==============================] - 0s 45us/step - loss: 0.2035 - acc: 0.9361\n",
      "Epoch 36/100\n",
      "9000/9000 [==============================] - 0s 37us/step - loss: 0.2019 - acc: 0.9352\n",
      "Epoch 37/100\n",
      "9000/9000 [==============================] - 0s 45us/step - loss: 0.1949 - acc: 0.9378\n",
      "Epoch 38/100\n",
      "9000/9000 [==============================] - 0s 42us/step - loss: 0.1930 - acc: 0.9403\n",
      "Epoch 39/100\n",
      "9000/9000 [==============================] - 0s 38us/step - loss: 0.1892 - acc: 0.9410\n",
      "Epoch 40/100\n",
      "9000/9000 [==============================] - 0s 35us/step - loss: 0.1851 - acc: 0.9424\n",
      "Epoch 41/100\n",
      "9000/9000 [==============================] - 0s 35us/step - loss: 0.1801 - acc: 0.9402\n",
      "Epoch 42/100\n",
      "9000/9000 [==============================] - 0s 34us/step - loss: 0.1782 - acc: 0.9439\n",
      "Epoch 43/100\n",
      "9000/9000 [==============================] - 0s 36us/step - loss: 0.1763 - acc: 0.9418\n",
      "Epoch 44/100\n",
      "9000/9000 [==============================] - 0s 36us/step - loss: 0.1736 - acc: 0.9456\n",
      "Epoch 45/100\n",
      "9000/9000 [==============================] - 0s 37us/step - loss: 0.1660 - acc: 0.9482\n",
      "Epoch 46/100\n",
      "9000/9000 [==============================] - 0s 46us/step - loss: 0.1617 - acc: 0.9477\n",
      "Epoch 47/100\n",
      "9000/9000 [==============================] - 0s 38us/step - loss: 0.1633 - acc: 0.9458\n",
      "Epoch 48/100\n",
      "9000/9000 [==============================] - 0s 42us/step - loss: 0.1582 - acc: 0.9487\n",
      "Epoch 49/100\n",
      "9000/9000 [==============================] - 0s 37us/step - loss: 0.1588 - acc: 0.9481\n",
      "Epoch 50/100\n",
      "9000/9000 [==============================] - 0s 50us/step - loss: 0.1549 - acc: 0.9507\n",
      "Epoch 51/100\n",
      "9000/9000 [==============================] - 0s 47us/step - loss: 0.1471 - acc: 0.9514\n",
      "Epoch 52/100\n",
      "9000/9000 [==============================] - 0s 46us/step - loss: 0.1473 - acc: 0.9527\n",
      "Epoch 53/100\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 0.1443 - acc: 0.9543\n",
      "Epoch 54/100\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.1421 - acc: 0.9534\n",
      "Epoch 55/100\n",
      "9000/9000 [==============================] - 0s 30us/step - loss: 0.1444 - acc: 0.9518\n",
      "Epoch 56/100\n",
      "9000/9000 [==============================] - 0s 39us/step - loss: 0.1325 - acc: 0.9587\n",
      "Epoch 57/100\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 0.1369 - acc: 0.9568\n",
      "Epoch 58/100\n",
      "9000/9000 [==============================] - 0s 48us/step - loss: 0.1346 - acc: 0.9578: 0s - loss: 0.1311 - acc\n",
      "Epoch 59/100\n",
      "9000/9000 [==============================] - 0s 40us/step - loss: 0.1322 - acc: 0.9569\n",
      "Epoch 60/100\n",
      "9000/9000 [==============================] - 0s 40us/step - loss: 0.1283 - acc: 0.9580\n",
      "Epoch 61/100\n",
      "9000/9000 [==============================] - 0s 37us/step - loss: 0.1213 - acc: 0.9607\n",
      "Epoch 62/100\n",
      "9000/9000 [==============================] - 0s 39us/step - loss: 0.1200 - acc: 0.9621\n",
      "Epoch 63/100\n",
      "9000/9000 [==============================] - 0s 39us/step - loss: 0.1180 - acc: 0.9612\n",
      "Epoch 64/100\n",
      "9000/9000 [==============================] - 0s 43us/step - loss: 0.1185 - acc: 0.9616: 0s - loss: 0.0869 - acc:\n",
      "Epoch 65/100\n",
      "9000/9000 [==============================] - 0s 36us/step - loss: 0.1206 - acc: 0.9608\n",
      "Epoch 66/100\n",
      "9000/9000 [==============================] - 0s 31us/step - loss: 0.1203 - acc: 0.9619\n",
      "Epoch 67/100\n",
      "9000/9000 [==============================] - 0s 36us/step - loss: 0.1101 - acc: 0.9653\n",
      "Epoch 68/100\n",
      "9000/9000 [==============================] - 0s 38us/step - loss: 0.1084 - acc: 0.9644\n",
      "Epoch 69/100\n",
      "9000/9000 [==============================] - 0s 34us/step - loss: 0.1065 - acc: 0.9648\n",
      "Epoch 70/100\n",
      "9000/9000 [==============================] - 0s 36us/step - loss: 0.1043 - acc: 0.9682\n",
      "Epoch 71/100\n",
      "9000/9000 [==============================] - 0s 36us/step - loss: 0.1077 - acc: 0.9656\n",
      "Epoch 72/100\n",
      "9000/9000 [==============================] - 0s 34us/step - loss: 0.1021 - acc: 0.9688\n",
      "Epoch 73/100\n",
      "9000/9000 [==============================] - 0s 37us/step - loss: 0.1009 - acc: 0.9679\n",
      "Epoch 74/100\n",
      "9000/9000 [==============================] - 0s 34us/step - loss: 0.1016 - acc: 0.9681\n",
      "Epoch 75/100\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 0.0978 - acc: 0.9699\n",
      "Epoch 76/100\n",
      "9000/9000 [==============================] - 0s 37us/step - loss: 0.0929 - acc: 0.9698\n",
      "Epoch 77/100\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 0.0972 - acc: 0.9673\n",
      "Epoch 78/100\n",
      "9000/9000 [==============================] - 0s 36us/step - loss: 0.0953 - acc: 0.9693\n",
      "Epoch 79/100\n",
      "9000/9000 [==============================] - 0s 34us/step - loss: 0.0843 - acc: 0.9737\n",
      "Epoch 80/100\n",
      "9000/9000 [==============================] - 0s 34us/step - loss: 0.0869 - acc: 0.9728\n",
      "Epoch 81/100\n",
      "9000/9000 [==============================] - 0s 37us/step - loss: 0.0848 - acc: 0.9734\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000/9000 [==============================] - 0s 34us/step - loss: 0.0853 - acc: 0.9727\n",
      "Epoch 83/100\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 0.0882 - acc: 0.9714\n",
      "Epoch 84/100\n",
      "9000/9000 [==============================] - 0s 35us/step - loss: 0.0786 - acc: 0.9751\n",
      "Epoch 85/100\n",
      "9000/9000 [==============================] - 0s 36us/step - loss: 0.0808 - acc: 0.9747\n",
      "Epoch 86/100\n",
      "9000/9000 [==============================] - 0s 34us/step - loss: 0.0828 - acc: 0.9721\n",
      "Epoch 87/100\n",
      "9000/9000 [==============================] - 0s 35us/step - loss: 0.0821 - acc: 0.9737\n",
      "Epoch 88/100\n",
      "9000/9000 [==============================] - 0s 36us/step - loss: 0.0792 - acc: 0.9766\n",
      "Epoch 89/100\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 0.0716 - acc: 0.9789\n",
      "Epoch 90/100\n",
      "9000/9000 [==============================] - 0s 38us/step - loss: 0.0690 - acc: 0.9792\n",
      "Epoch 91/100\n",
      "9000/9000 [==============================] - 0s 34us/step - loss: 0.0744 - acc: 0.9768\n",
      "Epoch 92/100\n",
      "9000/9000 [==============================] - 0s 37us/step - loss: 0.0731 - acc: 0.9763\n",
      "Epoch 93/100\n",
      "9000/9000 [==============================] - 0s 34us/step - loss: 0.0686 - acc: 0.9778\n",
      "Epoch 94/100\n",
      "9000/9000 [==============================] - 0s 37us/step - loss: 0.0645 - acc: 0.9817\n",
      "Epoch 95/100\n",
      "9000/9000 [==============================] - 0s 36us/step - loss: 0.0744 - acc: 0.9750\n",
      "Epoch 96/100\n",
      "9000/9000 [==============================] - 0s 33us/step - loss: 0.0749 - acc: 0.9749\n",
      "Epoch 97/100\n",
      "9000/9000 [==============================] - 0s 34us/step - loss: 0.0657 - acc: 0.9794\n",
      "Epoch 98/100\n",
      "9000/9000 [==============================] - 0s 38us/step - loss: 0.0607 - acc: 0.9812\n",
      "Epoch 99/100\n",
      "9000/9000 [==============================] - 0s 34us/step - loss: 0.0613 - acc: 0.9826\n",
      "Epoch 100/100\n",
      "9000/9000 [==============================] - 0s 39us/step - loss: 0.0597 - acc: 0.9820\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x128bf1828>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = Sequential()\n",
    "in_dim = X_train_labeled.shape[1]\n",
    "out_dim = onehot_y.shape[1]\n",
    "\n",
    "# first hidden Layer\n",
    "classifier.add(Dense(40, activation='relu', kernel_initializer='random_normal', input_dim=in_dim))\n",
    "# second hidden Layer\n",
    "classifier.add(Dense(30, activation='relu', kernel_initializer='random_normal'))\n",
    "# third hidden Layer\n",
    "classifier.add(Dense(20, activation='relu', kernel_initializer='random_normal'))\n",
    "# fourth hidden Layer\n",
    "classifier.add(Dense(15, activation='relu', kernel_initializer='random_normal'))\n",
    "# output layer\n",
    "classifier.add(Dense(out_dim, activation='softmax', kernel_initializer='random_normal'))\n",
    "\n",
    "classifier.compile(optimizer ='adam',loss='categorical_crossentropy', metrics =['accuracy'])\n",
    "classifier.fit(X_train_labeled, onehot_y, batch_size=100, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_ker = classifier.predict(X_test)\n",
    "# find most likely category from soft max\n",
    "pred_ker = np.argmax(pred_ker, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert back to pandas dataframe\n",
    "X_test = pd.DataFrame(X_test)\n",
    "pred_submit = pd.DataFrame(list(zip(X_test.index.values + 30000, pred_ker)), columns=['Id', 'y'])\n",
    "final_submit = pred_submit.to_csv(\"submit.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
